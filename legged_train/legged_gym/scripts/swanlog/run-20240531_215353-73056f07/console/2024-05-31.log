swanlab: \ Creating experiment...                                                                                                    swanlab:Tracking run with swanlab version 0.3.6
swanlab:Run data will be saved locally in /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog/run-20240531_215353-73056f07
swanlab:üëã Hi Aaron, welcome to swanlab!
swanlab:Syncing run yu_gym_May31_21-53-53 to the cloud
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/vsy9figgx3gqx0zq2s4ow
Setting seed: 1
********************************************************************************
Start creating ground...
Converting heightmap to trimesh...
Created 5913600 vertices
Created 11816962 triangles
Adding trimesh to simulation...
Trimesh added
Finished creating ground. Time taken 19.34 s
********************************************************************************
force sensors set at: ['body', 'left_roll_Link', 'left_yaw_Link', 'left_pitch_Link', 'left_knee_Link', 'left_foot_Link', 'right_roll_Link', 'right_yaw_Link', 'right_pitch_Link', 'right_knee_Link', 'right_foot_Link']
Creating env...
wow
Estimator Module: Estimator(
  (adaptor): Sequential(
    (0): Linear(in_features=410, out_features=128, bias=True)
    (1): ELU(alpha=1.0)
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ELU(alpha=1.0)
    (4): Linear(in_features=64, out_features=19, bias=True)
  )
  (fc1): Linear(in_features=19, out_features=128, bias=True)
  (fc21): Linear(in_features=128, out_features=64, bias=True)
  (fc22): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=41, bias=True)
)
Actor MLP: Sequential(
  (0): Linear(in_features=60, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=10, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=165, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                       Learning iteration 0/50000                       

                       Computation: 16792 steps/s (collection: 5.637s, learning 0.217s)
               Value function loss: 8.1636
                    Surrogate loss: 0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -7.32
                Mean reward (task): -7.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0023
       Mean episode rew_ang_vel_xy: -0.0178
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0310
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0502
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0008
Mean episode rew_power_distribution: -0.0450
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.1363
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0015
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 1.6671
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.85s
                        Total time: 5.85s
                               ETA: 4878 mins 18.3 s

################################################################################
                       Learning iteration 1/50000                       

                       Computation: 112026 steps/s (collection: 0.746s, learning 0.131s)
               Value function loss: 8.3445
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -7.85
                Mean reward (task): -7.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0252
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0391
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0569
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0495
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.1995
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 1.1183
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.88s
                        Total time: 6.73s
                               ETA: 2804 mins 43.4 s

################################################################################
                       Learning iteration 2/50000                       

                       Computation: 128932 steps/s (collection: 0.619s, learning 0.143s)
               Value function loss: 6.7685
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -7.30
                Mean reward (task): -7.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0250
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0401
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0566
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0514
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.6750
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.76s
                        Total time: 7.49s
                               ETA: 2081 mins 33.6 s

################################################################################
                       Learning iteration 3/50000                       

                       Computation: 135629 steps/s (collection: 0.601s, learning 0.124s)
               Value function loss: 6.0987
                    Surrogate loss: 0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -8.11
                Mean reward (task): -8.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0249
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0398
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0584
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0509
       Mean episode rew_smoothness: -0.0131
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.3790
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.72s
                        Total time: 8.22s
                               ETA: 1712 mins 7.8 s

################################################################################
                       Learning iteration 4/50000                       

                       Computation: 128701 steps/s (collection: 0.639s, learning 0.125s)
               Value function loss: 5.0655
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -8.21
                Mean reward (task): -8.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0248
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0408
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0586
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0516
       Mean episode rew_smoothness: -0.0133
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.2046
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.76s
                        Total time: 8.98s
                               ETA: 1496 mins 58.1 s

################################################################################
                       Learning iteration 5/50000                       

                       Computation: 122869 steps/s (collection: 0.645s, learning 0.155s)
               Value function loss: 4.1661
                    Surrogate loss: 0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -7.30
                Mean reward (task): -7.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0247
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0409
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0560
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0515
       Mean episode rew_smoothness: -0.0135
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.1082
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.80s
                        Total time: 9.78s
                               ETA: 1358 mins 33.5 s

################################################################################
                       Learning iteration 6/50000                       

                       Computation: 123576 steps/s (collection: 0.672s, learning 0.124s)
               Value function loss: 3.9624
                    Surrogate loss: 0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -7.98
                Mean reward (task): -7.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0250
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0577
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0510
       Mean episode rew_smoothness: -0.0137
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0562
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.80s
                        Total time: 10.58s
                               ETA: 1259 mins 8.7 s

################################################################################
                       Learning iteration 7/50000                       

                       Computation: 124445 steps/s (collection: 0.666s, learning 0.124s)
               Value function loss: 3.3797
                    Surrogate loss: 0.0005
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -7.79
                Mean reward (task): -7.79
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0246
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0560
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0509
       Mean episode rew_smoothness: -0.0139
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.79s
                        Total time: 11.37s
                               ETA: 1184 mins 0.2 s

################################################################################
                       Learning iteration 8/50000                       

                       Computation: 123365 steps/s (collection: 0.673s, learning 0.124s)
               Value function loss: 2.8927
                    Surrogate loss: 0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -7.84
                Mean reward (task): -7.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0248
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0410
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0564
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0509
       Mean episode rew_smoothness: -0.0141
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.80s
                        Total time: 12.16s
                               ETA: 1126 mins 11.8 s

################################################################################
                       Learning iteration 9/50000                       

                       Computation: 123777 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 2.5207
                    Surrogate loss: -0.0004
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -7.06
                Mean reward (task): -7.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0245
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0556
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0512
       Mean episode rew_smoothness: -0.0143
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.79s
                        Total time: 12.96s
                               ETA: 1079 mins 43.7 s

################################################################################
                      Learning iteration 10/50000                       

                       Computation: 118784 steps/s (collection: 0.704s, learning 0.124s)
               Value function loss: 2.6825
                    Surrogate loss: 0.0015
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -7.43
                Mean reward (task): -7.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0241
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0572
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0505
       Mean episode rew_smoothness: -0.0145
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.83s
                        Total time: 13.79s
                               ETA: 1044 mins 14.1 s

################################################################################
                      Learning iteration 11/50000                       

                       Computation: 112797 steps/s (collection: 0.747s, learning 0.124s)
               Value function loss: 2.4085
                    Surrogate loss: 0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -7.75
                Mean reward (task): -7.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0240
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0542
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0511
       Mean episode rew_smoothness: -0.0147
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.87s
                        Total time: 14.66s
                               ETA: 1017 mins 42.3 s

################################################################################
                      Learning iteration 12/50000                       

                       Computation: 113061 steps/s (collection: 0.732s, learning 0.137s)
               Value function loss: 1.8821
                    Surrogate loss: -0.0020
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -7.17
                Mean reward (task): -7.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0235
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0402
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0552
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0504
       Mean episode rew_smoothness: -0.0149
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.87s
                        Total time: 15.53s
                               ETA: 995 mins 7.4 s

################################################################################
                      Learning iteration 13/50000                       

                       Computation: 119030 steps/s (collection: 0.702s, learning 0.124s)
               Value function loss: 1.8612
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -7.40
                Mean reward (task): -7.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0237
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0412
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0560
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0513
       Mean episode rew_smoothness: -0.0151
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.83s
                        Total time: 16.35s
                               ETA: 973 mins 10.2 s

################################################################################
                      Learning iteration 14/50000                       

                       Computation: 122549 steps/s (collection: 0.670s, learning 0.132s)
               Value function loss: 1.5650
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -7.44
                Mean reward (task): -7.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0237
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0415
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0552
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0514
       Mean episode rew_smoothness: -0.0154
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.80s
                        Total time: 17.16s
                               ETA: 952 mins 49.6 s

################################################################################
                      Learning iteration 15/50000                       

                       Computation: 115691 steps/s (collection: 0.726s, learning 0.124s)
               Value function loss: 1.5341
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -7.54
                Mean reward (task): -7.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0234
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0413
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0548
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0503
       Mean episode rew_smoothness: -0.0156
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.85s
                        Total time: 18.01s
                               ETA: 937 mins 29.9 s

################################################################################
                      Learning iteration 16/50000                       

                       Computation: 122584 steps/s (collection: 0.679s, learning 0.123s)
               Value function loss: 1.2375
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -7.20
                Mean reward (task): -7.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0231
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0406
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0529
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0502
       Mean episode rew_smoothness: -0.0155
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.80s
                        Total time: 18.81s
                               ETA: 921 mins 37.9 s

################################################################################
                      Learning iteration 17/50000                       

                       Computation: 122470 steps/s (collection: 0.678s, learning 0.125s)
               Value function loss: 1.1291
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -7.28
                Mean reward (task): -7.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0228
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0407
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0519
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0503
       Mean episode rew_smoothness: -0.0155
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.80s
                        Total time: 19.61s
                               ETA: 907 mins 33.7 s

################################################################################
                      Learning iteration 18/50000                       

                       Computation: 116502 steps/s (collection: 0.710s, learning 0.133s)
               Value function loss: 1.2428
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -7.58
                Mean reward (task): -7.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0229
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0397
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0511
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0488
       Mean episode rew_smoothness: -0.0156
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.84s
                        Total time: 20.45s
                               ETA: 896 mins 46.4 s

################################################################################
                      Learning iteration 19/50000                       

                       Computation: 122443 steps/s (collection: 0.666s, learning 0.137s)
               Value function loss: 0.9938
                    Surrogate loss: 0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -7.64
                Mean reward (task): -7.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0226
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0401
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0010
        Mean episode rew_lin_vel_z: -0.0497
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0496
       Mean episode rew_smoothness: -0.0158
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.80s
                        Total time: 21.26s
                               ETA: 885 mins 21.4 s

################################################################################
                      Learning iteration 20/50000                       

                       Computation: 116594 steps/s (collection: 0.704s, learning 0.139s)
               Value function loss: 0.9681
                    Surrogate loss: 0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -7.46
                Mean reward (task): -7.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0223
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0391
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0489
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0490
       Mean episode rew_smoothness: -0.0158
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.84s
                        Total time: 22.10s
                               ETA: 876 mins 37.4 s

################################################################################
                      Learning iteration 21/50000                       

                       Computation: 121823 steps/s (collection: 0.678s, learning 0.129s)
               Value function loss: 0.9338
                    Surrogate loss: 0.0002
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -7.54
                Mean reward (task): -7.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0223
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0388
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0470
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0483
       Mean episode rew_smoothness: -0.0160
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.81s
                        Total time: 22.91s
                               ETA: 867 mins 18.8 s

################################################################################
                      Learning iteration 22/50000                       

                       Computation: 127661 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.9099
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -7.27
                Mean reward (task): -7.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0221
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0386
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0470
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0477
       Mean episode rew_smoothness: -0.0159
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.77s
                        Total time: 23.68s
                               ETA: 857 mins 28.5 s

################################################################################
                      Learning iteration 23/50000                       

                       Computation: 125572 steps/s (collection: 0.657s, learning 0.125s)
               Value function loss: 1.0992
                    Surrogate loss: 0.0003
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -7.40
                Mean reward (task): -7.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0218
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0380
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0457
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0470
       Mean episode rew_smoothness: -0.0159
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.78s
                        Total time: 24.46s
                               ETA: 848 mins 54.0 s

################################################################################
                      Learning iteration 24/50000                       

                       Computation: 122850 steps/s (collection: 0.677s, learning 0.124s)
               Value function loss: 1.2059
                    Surrogate loss: 0.0006
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -7.20
                Mean reward (task): -7.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0223
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0471
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0476
       Mean episode rew_smoothness: -0.0159
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.80s
                        Total time: 25.26s
                               ETA: 841 mins 35.3 s

################################################################################
                      Learning iteration 25/50000                       

                       Computation: 113851 steps/s (collection: 0.726s, learning 0.138s)
               Value function loss: 1.1548
                    Surrogate loss: 0.0012
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -7.12
                Mean reward (task): -7.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0217
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0382
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0462
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0469
       Mean episode rew_smoothness: -0.0162
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.86s
                        Total time: 26.12s
                               ETA: 836 mins 51.8 s

################################################################################
                      Learning iteration 26/50000                       

                       Computation: 128179 steps/s (collection: 0.645s, learning 0.122s)
               Value function loss: 1.0166
                    Surrogate loss: -0.0017
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -7.07
                Mean reward (task): -7.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0214
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0383
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0441
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0473
       Mean episode rew_smoothness: -0.0162
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.77s
                        Total time: 26.89s
                               ETA: 829 mins 30.7 s

################################################################################
                      Learning iteration 27/50000                       

                       Computation: 121763 steps/s (collection: 0.685s, learning 0.123s)
               Value function loss: 0.8058
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -7.43
                Mean reward (task): -7.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0214
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0369
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0445
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0457
       Mean episode rew_smoothness: -0.0162
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.81s
                        Total time: 27.70s
                               ETA: 823 mins 53.1 s

################################################################################
                      Learning iteration 28/50000                       

                       Computation: 114530 steps/s (collection: 0.734s, learning 0.124s)
               Value function loss: 0.7568
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -6.89
                Mean reward (task): -6.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0209
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0357
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0414
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0443
       Mean episode rew_smoothness: -0.0162
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.86s
                        Total time: 28.56s
                               ETA: 820 mins 6.6 s

################################################################################
                      Learning iteration 29/50000                       

                       Computation: 124734 steps/s (collection: 0.660s, learning 0.128s)
               Value function loss: 0.7183
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -7.04
                Mean reward (task): -7.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0211
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0360
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0413
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0445
       Mean episode rew_smoothness: -0.0161
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.79s
                        Total time: 29.34s
                               ETA: 814 mins 38.1 s

################################################################################
                      Learning iteration 30/50000                       

                       Computation: 121244 steps/s (collection: 0.686s, learning 0.124s)
               Value function loss: 0.7645
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -7.00
                Mean reward (task): -7.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0208
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0355
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0413
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0441
       Mean episode rew_smoothness: -0.0160
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.81s
                        Total time: 30.15s
                               ETA: 810 mins 7.4 s

################################################################################
                      Learning iteration 31/50000                       

                       Computation: 114095 steps/s (collection: 0.737s, learning 0.125s)
               Value function loss: 0.8201
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -7.12
                Mean reward (task): -7.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0211
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0352
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0009
        Mean episode rew_lin_vel_z: -0.0405
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0428
       Mean episode rew_smoothness: -0.0157
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.86s
                        Total time: 31.02s
                               ETA: 807 mins 12.9 s

################################################################################
                      Learning iteration 32/50000                       

                       Computation: 119846 steps/s (collection: 0.697s, learning 0.123s)
               Value function loss: 0.6408
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -6.84
                Mean reward (task): -6.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0206
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0330
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0376
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0406
       Mean episode rew_smoothness: -0.0153
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.82s
                        Total time: 31.84s
                               ETA: 803 mins 26.3 s

################################################################################
                      Learning iteration 33/50000                       

                       Computation: 125983 steps/s (collection: 0.654s, learning 0.126s)
               Value function loss: 0.7230
                    Surrogate loss: -0.0001
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -6.62
                Mean reward (task): -6.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0206
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0320
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0370
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0395
       Mean episode rew_smoothness: -0.0149
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.78s
                        Total time: 32.62s
                               ETA: 798 mins 54.3 s

################################################################################
                      Learning iteration 34/50000                       

                       Computation: 123532 steps/s (collection: 0.673s, learning 0.123s)
               Value function loss: 0.6993
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -6.58
                Mean reward (task): -6.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0203
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0318
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0362
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0391
       Mean episode rew_smoothness: -0.0146
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.80s
                        Total time: 33.41s
                               ETA: 794 mins 59.8 s

################################################################################
                      Learning iteration 35/50000                       

                       Computation: 126627 steps/s (collection: 0.651s, learning 0.125s)
               Value function loss: 0.6187
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.28
                Mean reward (task): -6.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0204
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0308
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0349
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0374
       Mean episode rew_smoothness: -0.0143
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0082
        Mean episode terrain_level: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.78s
                        Total time: 34.19s
                               ETA: 790 mins 51.4 s

################################################################################
                      Learning iteration 36/50000                       

                       Computation: 127266 steps/s (collection: 0.647s, learning 0.125s)
               Value function loss: 0.6488
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.60
                Mean reward (task): -6.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0208
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0298
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0008
        Mean episode rew_lin_vel_z: -0.0339
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0364
       Mean episode rew_smoothness: -0.0138
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0078
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.77s
                        Total time: 34.96s
                               ETA: 786 mins 51.0 s

################################################################################
                      Learning iteration 37/50000                       

                       Computation: 123985 steps/s (collection: 0.667s, learning 0.126s)
               Value function loss: 0.4991
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.56
                Mean reward (task): -6.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0205
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0287
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0318
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0352
       Mean episode rew_smoothness: -0.0137
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.79s
                        Total time: 35.75s
                               ETA: 783 mins 30.2 s

################################################################################
                      Learning iteration 38/50000                       

                       Computation: 111299 steps/s (collection: 0.738s, learning 0.145s)
               Value function loss: 0.4291
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.12
                Mean reward (task): -6.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0204
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0271
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0330
       Mean episode rew_smoothness: -0.0131
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.88s
                        Total time: 36.64s
                               ETA: 782 mins 15.4 s

################################################################################
                      Learning iteration 39/50000                       

                       Computation: 128126 steps/s (collection: 0.642s, learning 0.125s)
               Value function loss: 0.3875
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.33
                Mean reward (task): -6.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0203
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0262
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0295
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0310
       Mean episode rew_smoothness: -0.0128
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.77s
                        Total time: 37.40s
                               ETA: 778 mins 39.4 s

################################################################################
                      Learning iteration 40/50000                       

                       Computation: 125235 steps/s (collection: 0.660s, learning 0.125s)
               Value function loss: 0.4484
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.40
                Mean reward (task): -6.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0205
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0265
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0302
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0316
       Mean episode rew_smoothness: -0.0126
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.78s
                        Total time: 38.19s
                               ETA: 775 mins 35.5 s

################################################################################
                      Learning iteration 41/50000                       

                       Computation: 122533 steps/s (collection: 0.676s, learning 0.126s)
               Value function loss: 0.4023
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.35
                Mean reward (task): -6.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0202
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0257
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0290
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0307
       Mean episode rew_smoothness: -0.0126
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.80s
                        Total time: 38.99s
                               ETA: 773 mins 0.9 s

################################################################################
                      Learning iteration 42/50000                       

                       Computation: 118716 steps/s (collection: 0.705s, learning 0.123s)
               Value function loss: 0.3718
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.05
                Mean reward (task): -6.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0201
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0250
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0289
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0297
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.83s
                        Total time: 39.82s
                               ETA: 771 mins 3.4 s

################################################################################
                      Learning iteration 43/50000                       

                       Computation: 117846 steps/s (collection: 0.710s, learning 0.124s)
               Value function loss: 0.3435
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.18
               Mean reward (total): -6.20
                Mean reward (task): -6.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0199
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0249
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0007
        Mean episode rew_lin_vel_z: -0.0288
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0294
       Mean episode rew_smoothness: -0.0124
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0076
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.83s
                        Total time: 40.65s
                               ETA: 769 mins 18.1 s

################################################################################
                      Learning iteration 44/50000                       

                       Computation: 123705 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 0.3445
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -6.22
                Mean reward (task): -6.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0201
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0242
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0278
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0287
       Mean episode rew_smoothness: -0.0122
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0074
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.79s
                        Total time: 41.45s
                               ETA: 766 mins 53.7 s

################################################################################
                      Learning iteration 45/50000                       

                       Computation: 125318 steps/s (collection: 0.660s, learning 0.124s)
               Value function loss: 0.2981
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -6.07
                Mean reward (task): -6.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0201
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0241
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0272
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0282
       Mean episode rew_smoothness: -0.0119
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.78s
                        Total time: 42.23s
                               ETA: 764 mins 24.4 s

################################################################################
                      Learning iteration 46/50000                       

                       Computation: 118524 steps/s (collection: 0.699s, learning 0.130s)
               Value function loss: 0.3460
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -6.21
                Mean reward (task): -6.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0201
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0230
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0270
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0116
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0072
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.83s
                        Total time: 43.06s
                               ETA: 762 mins 49.2 s

################################################################################
                      Learning iteration 47/50000                       

                       Computation: 122141 steps/s (collection: 0.682s, learning 0.123s)
               Value function loss: 0.2847
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -6.25
                Mean reward (task): -6.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0202
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0228
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0273
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0072
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.80s
                        Total time: 43.87s
                               ETA: 760 mins 52.3 s

################################################################################
                      Learning iteration 48/50000                       

                       Computation: 117841 steps/s (collection: 0.703s, learning 0.131s)
               Value function loss: 0.2419
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.17
               Mean reward (total): -6.13
                Mean reward (task): -6.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0199
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0222
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0262
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0072
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.83s
                        Total time: 44.70s
                               ETA: 759 mins 30.1 s

################################################################################
                      Learning iteration 49/50000                       

                       Computation: 120395 steps/s (collection: 0.691s, learning 0.125s)
               Value function loss: 0.2800
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -6.12
                Mean reward (task): -6.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0199
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0224
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0263
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0072
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.82s
                        Total time: 45.52s
                               ETA: 757 mins 53.6 s

################################################################################
                      Learning iteration 50/50000                       

                       Computation: 134960 steps/s (collection: 0.602s, learning 0.126s)
               Value function loss: 0.2493
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -5.99
                Mean reward (task): -5.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0197
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0222
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0255
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0071
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.73s
                        Total time: 46.25s
                               ETA: 754 mins 54.4 s

################################################################################
                      Learning iteration 51/50000                       

                       Computation: 117588 steps/s (collection: 0.713s, learning 0.123s)
               Value function loss: 0.2274
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.16
               Mean reward (total): -5.91
                Mean reward (task): -5.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0195
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0217
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0241
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.84s
                        Total time: 47.08s
                               ETA: 753 mins 45.5 s

################################################################################
                      Learning iteration 52/50000                       

                       Computation: 117727 steps/s (collection: 0.696s, learning 0.139s)
               Value function loss: 0.2425
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -5.95
                Mean reward (task): -5.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0194
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0218
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0250
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0070
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.84s
                        Total time: 47.92s
                               ETA: 752 mins 38.2 s

################################################################################
                      Learning iteration 53/50000                       

                       Computation: 123727 steps/s (collection: 0.670s, learning 0.125s)
               Value function loss: 0.2456
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -5.95
                Mean reward (task): -5.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0192
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0221
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0244
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0070
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.79s
                        Total time: 48.71s
                               ETA: 750 mins 56.0 s

################################################################################
                      Learning iteration 54/50000                       

                       Computation: 129942 steps/s (collection: 0.632s, learning 0.124s)
               Value function loss: 0.2170
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -6.13
                Mean reward (task): -6.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0192
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0222
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0240
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.76s
                        Total time: 49.47s
                               ETA: 748 mins 42.9 s

################################################################################
                      Learning iteration 55/50000                       

                       Computation: 117732 steps/s (collection: 0.713s, learning 0.122s)
               Value function loss: 0.2143
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.15
               Mean reward (total): -5.85
                Mean reward (task): -5.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0194
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0212
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0243
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.83s
                        Total time: 50.30s
                               ETA: 747 mins 44.5 s

################################################################################
                      Learning iteration 56/50000                       

                       Computation: 125170 steps/s (collection: 0.662s, learning 0.123s)
               Value function loss: 0.1973
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.14
               Mean reward (total): -5.84
                Mean reward (task): -5.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0192
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0213
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0236
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0068
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.79s
                        Total time: 51.09s
                               ETA: 746 mins 4.7 s

################################################################################
                      Learning iteration 57/50000                       

                       Computation: 120902 steps/s (collection: 0.689s, learning 0.124s)
               Value function loss: 0.2217
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -5.90
                Mean reward (task): -5.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0191
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0216
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0234
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.81s
                        Total time: 51.90s
                               ETA: 744 mins 52.1 s

################################################################################
                      Learning iteration 58/50000                       

                       Computation: 121167 steps/s (collection: 0.686s, learning 0.125s)
               Value function loss: 0.2885
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -5.96
                Mean reward (task): -5.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0190
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0212
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0068
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.81s
                        Total time: 52.71s
                               ETA: 743 mins 40.5 s

################################################################################
                      Learning iteration 59/50000                       

                       Computation: 119277 steps/s (collection: 0.694s, learning 0.130s)
               Value function loss: 0.2204
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -6.10
                Mean reward (task): -6.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0192
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0206
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0231
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0250
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.82s
                        Total time: 53.54s
                               ETA: 742 mins 41.9 s

################################################################################
                      Learning iteration 60/50000                       

                       Computation: 121104 steps/s (collection: 0.677s, learning 0.135s)
               Value function loss: 0.1780
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -6.11
                Mean reward (task): -6.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0042
       Mean episode rew_ang_vel_xy: -0.0192
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0207
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0225
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.81s
                        Total time: 54.35s
                               ETA: 741 mins 35.1 s

################################################################################
                      Learning iteration 61/50000                       

                       Computation: 123920 steps/s (collection: 0.670s, learning 0.123s)
               Value function loss: 0.1840
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -6.07
                Mean reward (task): -6.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0042
       Mean episode rew_ang_vel_xy: -0.0193
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0203
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0222
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.79s
                        Total time: 55.14s
                               ETA: 740 mins 15.5 s

################################################################################
                      Learning iteration 62/50000                       

                       Computation: 125663 steps/s (collection: 0.659s, learning 0.123s)
               Value function loss: 0.1730
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.13
               Mean reward (total): -5.89
                Mean reward (task): -5.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 12.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0194
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0196
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.78s
                        Total time: 55.92s
                               ETA: 738 mins 49.7 s

################################################################################
                      Learning iteration 63/50000                       

                       Computation: 123126 steps/s (collection: 0.675s, learning 0.123s)
               Value function loss: 0.1742
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -5.68
                Mean reward (task): -5.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0194
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0195
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0219
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.80s
                        Total time: 56.72s
                               ETA: 737 mins 39.2 s

################################################################################
                      Learning iteration 64/50000                       

                       Computation: 124046 steps/s (collection: 0.659s, learning 0.133s)
               Value function loss: 0.1991
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.12
               Mean reward (total): -5.82
                Mean reward (task): -5.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0192
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0190
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0215
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.79s
                        Total time: 57.52s
                               ETA: 736 mins 26.2 s

################################################################################
                      Learning iteration 65/50000                       

                       Computation: 114484 steps/s (collection: 0.735s, learning 0.124s)
               Value function loss: 0.1532
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.11
               Mean reward (total): -5.88
                Mean reward (task): -5.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0194
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0191
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0212
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.86s
                        Total time: 58.37s
                               ETA: 736 mins 5.5 s

################################################################################
                      Learning iteration 66/50000                       

                       Computation: 126627 steps/s (collection: 0.651s, learning 0.125s)
               Value function loss: 0.1593
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -6.03
                Mean reward (task): -6.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0193
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0210
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.78s
                        Total time: 59.15s
                               ETA: 734 mins 44.0 s

################################################################################
                      Learning iteration 67/50000                       

                       Computation: 126512 steps/s (collection: 0.652s, learning 0.125s)
               Value function loss: 0.1490
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.10
               Mean reward (total): -5.75
                Mean reward (task): -5.75
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 12.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0192
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0206
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.78s
                        Total time: 59.93s
                               ETA: 733 mins 25.4 s

################################################################################
                      Learning iteration 68/50000                       

                       Computation: 120807 steps/s (collection: 0.673s, learning 0.141s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -5.69
                Mean reward (task): -5.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0189
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0202
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0223
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.81s
                        Total time: 60.74s
                               ETA: 732 mins 35.7 s

################################################################################
                      Learning iteration 69/50000                       

                       Computation: 130230 steps/s (collection: 0.632s, learning 0.123s)
               Value function loss: 0.1597
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.09
               Mean reward (total): -5.72
                Mean reward (task): -5.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0190
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.75s
                        Total time: 61.50s
                               ETA: 731 mins 5.3 s

################################################################################
                      Learning iteration 70/50000                       

                       Computation: 117164 steps/s (collection: 0.704s, learning 0.135s)
               Value function loss: 0.1486
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -5.70
                Mean reward (task): -5.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0188
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0199
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.84s
                        Total time: 62.34s
                               ETA: 730 mins 36.6 s

################################################################################
                      Learning iteration 71/50000                       

                       Computation: 122635 steps/s (collection: 0.663s, learning 0.138s)
               Value function loss: 0.1183
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -5.69
                Mean reward (task): -5.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 12.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0190
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0195
           Mean episode rew_no_fly: 0.0007
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.80s
                        Total time: 63.14s
                               ETA: 729 mins 42.8 s

################################################################################
                      Learning iteration 72/50000                       

                       Computation: 117518 steps/s (collection: 0.713s, learning 0.124s)
               Value function loss: 0.1174
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.08
               Mean reward (total): -5.67
                Mean reward (task): -5.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 12.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0190
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0007
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.84s
                        Total time: 63.97s
                               ETA: 729 mins 14.3 s

################################################################################
                      Learning iteration 73/50000                       

                       Computation: 129724 steps/s (collection: 0.632s, learning 0.125s)
               Value function loss: 0.1317
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0188
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0193
           Mean episode rew_no_fly: 0.0007
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.76s
                        Total time: 64.73s
                               ETA: 727 mins 53.4 s

################################################################################
                      Learning iteration 74/50000                       

                       Computation: 119043 steps/s (collection: 0.698s, learning 0.128s)
               Value function loss: 0.1388
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.07
               Mean reward (total): -5.70
                Mean reward (task): -5.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0187
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0191
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.83s
                        Total time: 65.56s
                               ETA: 727 mins 20.0 s

################################################################################
                      Learning iteration 75/50000                       

                       Computation: 124584 steps/s (collection: 0.665s, learning 0.124s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -5.63
                Mean reward (task): -5.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0185
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0189
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.79s
                        Total time: 66.35s
                               ETA: 726 mins 23.2 s

################################################################################
                      Learning iteration 76/50000                       

                       Computation: 128450 steps/s (collection: 0.632s, learning 0.134s)
               Value function loss: 0.1106
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.06
               Mean reward (total): -5.78
                Mean reward (task): -5.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0184
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0012
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0186
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.77s
                        Total time: 67.11s
                               ETA: 725 mins 12.5 s

################################################################################
                      Learning iteration 77/50000                       

                       Computation: 122813 steps/s (collection: 0.668s, learning 0.133s)
               Value function loss: 0.1299
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -5.70
                Mean reward (task): -5.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0183
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0183
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.80s
                        Total time: 67.91s
                               ETA: 724 mins 26.1 s

################################################################################
                      Learning iteration 78/50000                       

                       Computation: 121663 steps/s (collection: 0.684s, learning 0.124s)
               Value function loss: 0.1077
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.05
               Mean reward (total): -5.62
                Mean reward (task): -5.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0181
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0178
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0078
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.81s
                        Total time: 68.72s
                               ETA: 723 mins 45.7 s

################################################################################
                      Learning iteration 79/50000                       

                       Computation: 117376 steps/s (collection: 0.714s, learning 0.123s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0182
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0174
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0210
       Mean episode rew_smoothness: -0.0077
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.84s
                        Total time: 69.56s
                               ETA: 723 mins 24.6 s

################################################################################
                      Learning iteration 80/50000                       

                       Computation: 129465 steps/s (collection: 0.636s, learning 0.124s)
               Value function loss: 0.1362
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.04
               Mean reward (total): -5.57
                Mean reward (task): -5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0183
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0178
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0212
       Mean episode rew_smoothness: -0.0075
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.76s
                        Total time: 70.32s
                               ETA: 722 mins 15.8 s

################################################################################
                      Learning iteration 81/50000                       

                       Computation: 134419 steps/s (collection: 0.608s, learning 0.123s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.63
                Mean reward (task): -5.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0182
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0172
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0212
       Mean episode rew_smoothness: -0.0075
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.73s
                        Total time: 71.05s
                               ETA: 720 mins 51.7 s

################################################################################
                      Learning iteration 82/50000                       

                       Computation: 127989 steps/s (collection: 0.644s, learning 0.124s)
               Value function loss: 0.1034
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.03
               Mean reward (total): -5.64
                Mean reward (task): -5.64
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0181
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0013
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0170
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0210
       Mean episode rew_smoothness: -0.0074
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.77s
                        Total time: 71.82s
                               ETA: 719 mins 51.7 s

################################################################################
                      Learning iteration 83/50000                       

                       Computation: 120690 steps/s (collection: 0.673s, learning 0.142s)
               Value function loss: 0.1102
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -5.55
                Mean reward (task): -5.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0180
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0168
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0206
       Mean episode rew_smoothness: -0.0073
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.81s
                        Total time: 72.63s
                               ETA: 719 mins 20.7 s

################################################################################
                      Learning iteration 84/50000                       

                       Computation: 124137 steps/s (collection: 0.666s, learning 0.126s)
               Value function loss: 0.1394
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0179
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0168
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0212
       Mean episode rew_smoothness: -0.0073
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.79s
                        Total time: 73.42s
                               ETA: 718 mins 37.1 s

################################################################################
                      Learning iteration 85/50000                       

                       Computation: 113956 steps/s (collection: 0.728s, learning 0.134s)
               Value function loss: 0.1083
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.02
               Mean reward (total): -5.58
                Mean reward (task): -5.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0179
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0167
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0073
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.86s
                        Total time: 74.29s
                               ETA: 718 mins 35.5 s

################################################################################
                      Learning iteration 86/50000                       

                       Computation: 126845 steps/s (collection: 0.649s, learning 0.126s)
               Value function loss: 0.1191
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0179
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0167
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.77s
                        Total time: 75.06s
                               ETA: 717 mins 43.7 s

################################################################################
                      Learning iteration 87/50000                       

                       Computation: 127085 steps/s (collection: 0.649s, learning 0.125s)
               Value function loss: 0.1108
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.01
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0179
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0167
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0206
       Mean episode rew_smoothness: -0.0071
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.77s
                        Total time: 75.83s
                               ETA: 716 mins 52.3 s

################################################################################
                      Learning iteration 88/50000                       

                       Computation: 118475 steps/s (collection: 0.698s, learning 0.131s)
               Value function loss: 0.1170
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -5.67
                Mean reward (task): -5.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0177
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0164
           Mean episode rew_no_fly: 0.0008
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0071
      Mean episode rew_stand_still: -0.0002
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.83s
                        Total time: 76.66s
                               ETA: 716 mins 33.4 s

################################################################################
                      Learning iteration 89/50000                       

                       Computation: 120197 steps/s (collection: 0.688s, learning 0.130s)
               Value function loss: 0.1024
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 1.00
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0176
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0167
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0070
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.82s
                        Total time: 77.48s
                               ETA: 716 mins 8.4 s

################################################################################
                      Learning iteration 90/50000                       

                       Computation: 124385 steps/s (collection: 0.658s, learning 0.132s)
               Value function loss: 0.1067
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0174
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0158
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0205
       Mean episode rew_smoothness: -0.0070
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.79s
                        Total time: 78.27s
                               ETA: 715 mins 28.9 s

################################################################################
                      Learning iteration 91/50000                       

                       Computation: 127091 steps/s (collection: 0.647s, learning 0.126s)
               Value function loss: 0.1000
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.99
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0176
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0159
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0209
       Mean episode rew_smoothness: -0.0069
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.77s
                        Total time: 79.04s
                               ETA: 714 mins 41.0 s

################################################################################
                      Learning iteration 92/50000                       

                       Computation: 129331 steps/s (collection: 0.637s, learning 0.123s)
               Value function loss: 0.1228
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0173
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0158
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0202
       Mean episode rew_smoothness: -0.0068
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.76s
                        Total time: 79.81s
                               ETA: 713 mins 47.0 s

################################################################################
                      Learning iteration 93/50000                       

                       Computation: 131622 steps/s (collection: 0.623s, learning 0.124s)
               Value function loss: 0.1105
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.98
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0174
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0158
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.75s
                        Total time: 80.55s
                               ETA: 712 mins 47.0 s

################################################################################
                      Learning iteration 94/50000                       

                       Computation: 121962 steps/s (collection: 0.681s, learning 0.125s)
               Value function loss: 0.0976
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0173
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0014
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0156
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0205
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0004
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.81s
                        Total time: 81.36s
                               ETA: 712 mins 19.4 s

################################################################################
                      Learning iteration 95/50000                       

                       Computation: 122164 steps/s (collection: 0.676s, learning 0.129s)
               Value function loss: 0.1182
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): -5.60
                Mean reward (task): -5.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0173
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0153
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.80s
                        Total time: 82.16s
                               ETA: 711 mins 51.7 s

################################################################################
                      Learning iteration 96/50000                       

                       Computation: 126212 steps/s (collection: 0.640s, learning 0.139s)
               Value function loss: 0.1162
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.97
               Mean reward (total): -5.62
                Mean reward (task): -5.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0171
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0152
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.78s
                        Total time: 82.94s
                               ETA: 711 mins 11.2 s

################################################################################
                      Learning iteration 97/50000                       

                       Computation: 113675 steps/s (collection: 0.732s, learning 0.133s)
               Value function loss: 0.1063
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0170
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0152
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.86s
                        Total time: 83.81s
                               ETA: 711 mins 15.3 s

################################################################################
                      Learning iteration 98/50000                       

                       Computation: 116555 steps/s (collection: 0.689s, learning 0.155s)
               Value function loss: 0.0958
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.96
               Mean reward (total): -5.62
                Mean reward (task): -5.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0170
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0153
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.84s
                        Total time: 84.65s
                               ETA: 711 mins 8.6 s

################################################################################
                      Learning iteration 99/50000                       

                       Computation: 123190 steps/s (collection: 0.648s, learning 0.150s)
               Value function loss: 0.0954
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0169
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0148
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.80s
                        Total time: 85.45s
                               ETA: 710 mins 39.2 s

################################################################################
                      Learning iteration 100/50000                      

                       Computation: 128478 steps/s (collection: 0.624s, learning 0.141s)
               Value function loss: 0.0988
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0169
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0148
           Mean episode rew_no_fly: 0.0009
      Mean episode rew_orientation: -0.0012
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.77s
                        Total time: 86.21s
                               ETA: 709 mins 54.2 s

################################################################################
                      Learning iteration 101/50000                      

                       Computation: 118926 steps/s (collection: 0.683s, learning 0.144s)
               Value function loss: 0.1137
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.95
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 13.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0167
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0148
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0205
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.83s
                        Total time: 87.04s
                               ETA: 709 mins 40.2 s

################################################################################
                      Learning iteration 102/50000                      

                       Computation: 126468 steps/s (collection: 0.637s, learning 0.140s)
               Value function loss: 0.1107
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0167
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0147
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0209
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.78s
                        Total time: 87.82s
                               ETA: 709 mins 2.5 s

################################################################################
                      Learning iteration 103/50000                      

                       Computation: 127178 steps/s (collection: 0.634s, learning 0.139s)
               Value function loss: 0.1217
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.94
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0165
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0015
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0145
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0018
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.77s
                        Total time: 88.59s
                               ETA: 708 mins 23.4 s

################################################################################
                      Learning iteration 104/50000                      

                       Computation: 128527 steps/s (collection: 0.623s, learning 0.142s)
               Value function loss: 0.0980
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0164
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0147
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.76s
                        Total time: 89.35s
                               ETA: 707 mins 41.2 s

################################################################################
                      Learning iteration 105/50000                      

                       Computation: 129155 steps/s (collection: 0.637s, learning 0.124s)
               Value function loss: 0.1016
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0165
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0145
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0198
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.76s
                        Total time: 90.12s
                               ETA: 706 mins 58.1 s

################################################################################
                      Learning iteration 106/50000                      

                       Computation: 130789 steps/s (collection: 0.630s, learning 0.122s)
               Value function loss: 0.0928
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.93
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0164
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0146
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.75s
                        Total time: 90.87s
                               ETA: 706 mins 11.3 s

################################################################################
                      Learning iteration 107/50000                      

                       Computation: 124763 steps/s (collection: 0.652s, learning 0.135s)
               Value function loss: 0.0889
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0164
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0145
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0202
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.79s
                        Total time: 91.66s
                               ETA: 705 mins 42.1 s

################################################################################
                      Learning iteration 108/50000                      

                       Computation: 123246 steps/s (collection: 0.666s, learning 0.131s)
               Value function loss: 0.0933
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0163
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0148
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.80s
                        Total time: 92.45s
                               ETA: 705 mins 17.9 s

################################################################################
                      Learning iteration 109/50000                      

                       Computation: 123587 steps/s (collection: 0.645s, learning 0.150s)
               Value function loss: 0.1105
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.92
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0163
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0148
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0203
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.80s
                        Total time: 93.25s
                               ETA: 704 mins 53.1 s

################################################################################
                      Learning iteration 110/50000                      

                       Computation: 125013 steps/s (collection: 0.658s, learning 0.128s)
               Value function loss: 0.0923
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0163
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0144
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.79s
                        Total time: 94.03s
                               ETA: 704 mins 24.7 s

################################################################################
                      Learning iteration 111/50000                      

                       Computation: 126879 steps/s (collection: 0.652s, learning 0.123s)
               Value function loss: 0.1038
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0163
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0145
           Mean episode rew_no_fly: 0.0010
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0202
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.77s
                        Total time: 94.81s
                               ETA: 703 mins 51.6 s

################################################################################
                      Learning iteration 112/50000                      

                       Computation: 123610 steps/s (collection: 0.670s, learning 0.125s)
               Value function loss: 0.1542
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.91
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0161
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0016
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0143
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.80s
                        Total time: 95.60s
                               ETA: 703 mins 28.2 s

################################################################################
                      Learning iteration 113/50000                      

                       Computation: 126159 steps/s (collection: 0.654s, learning 0.125s)
               Value function loss: 0.1105
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0161
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0140
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0198
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.78s
                        Total time: 96.38s
                               ETA: 702 mins 58.1 s

################################################################################
                      Learning iteration 114/50000                      

                       Computation: 132572 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0857
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0161
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0143
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.74s
                        Total time: 97.13s
                               ETA: 702 mins 12.1 s

################################################################################
                      Learning iteration 115/50000                      

                       Computation: 121175 steps/s (collection: 0.677s, learning 0.134s)
               Value function loss: 0.1052
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0160
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0145
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0199
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.81s
                        Total time: 97.94s
                               ETA: 701 mins 56.9 s

################################################################################
                      Learning iteration 116/50000                      

                       Computation: 121893 steps/s (collection: 0.684s, learning 0.122s)
               Value function loss: 0.0960
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0160
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0139
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0196
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.81s
                        Total time: 98.74s
                               ETA: 701 mins 40.0 s

################################################################################
                      Learning iteration 117/50000                      

                       Computation: 119433 steps/s (collection: 0.682s, learning 0.141s)
               Value function loss: 0.1112
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0035
       Mean episode rew_ang_vel_xy: -0.0159
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0143
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.82s
                        Total time: 99.57s
                               ETA: 701 mins 30.3 s

################################################################################
                      Learning iteration 118/50000                      

                       Computation: 126354 steps/s (collection: 0.646s, learning 0.132s)
               Value function loss: 0.1206
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.90
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0160
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0138
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0203
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.78s
                        Total time: 100.34s
                               ETA: 701 mins 1.9 s

################################################################################
                      Learning iteration 119/50000                      

                       Computation: 130662 steps/s (collection: 0.629s, learning 0.124s)
               Value function loss: 0.1051
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 14.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0158
      Mean episode rew_base_height: -0.0003
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0138
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.75s
                        Total time: 101.10s
                               ETA: 700 mins 23.3 s

################################################################################
                      Learning iteration 120/50000                      

                       Computation: 123880 steps/s (collection: 0.653s, learning 0.140s)
               Value function loss: 0.0983
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0157
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0139
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0019
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.79s
                        Total time: 101.89s
                               ETA: 700 mins 2.3 s

################################################################################
                      Learning iteration 121/50000                      

                       Computation: 121980 steps/s (collection: 0.684s, learning 0.122s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0158
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0017
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0137
           Mean episode rew_no_fly: 0.0011
      Mean episode rew_orientation: -0.0013
Mean episode rew_power_distribution: -0.0202
       Mean episode rew_smoothness: -0.0062
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.81s
                        Total time: 102.70s
                               ETA: 699 mins 46.6 s

################################################################################
                      Learning iteration 122/50000                      

                       Computation: 117471 steps/s (collection: 0.704s, learning 0.133s)
               Value function loss: 0.1068
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0157
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0136
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0203
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.84s
                        Total time: 103.53s
                               ETA: 699 mins 43.8 s

################################################################################
                      Learning iteration 123/50000                      

                       Computation: 128673 steps/s (collection: 0.642s, learning 0.122s)
               Value function loss: 0.0945
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.89
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0157
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0134
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.76s
                        Total time: 104.30s
                               ETA: 699 mins 11.7 s

################################################################################
                      Learning iteration 124/50000                      

                       Computation: 118224 steps/s (collection: 0.688s, learning 0.143s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0157
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0134
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.83s
                        Total time: 105.13s
                               ETA: 699 mins 7.0 s

################################################################################
                      Learning iteration 125/50000                      

                       Computation: 120613 steps/s (collection: 0.687s, learning 0.128s)
               Value function loss: 0.1075
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0036
       Mean episode rew_ang_vel_xy: -0.0156
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0131
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0206
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.82s
                        Total time: 105.94s
                               ETA: 698 mins 55.9 s

################################################################################
                      Learning iteration 126/50000                      

                       Computation: 136470 steps/s (collection: 0.597s, learning 0.123s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0154
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0130
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.72s
                        Total time: 106.66s
                               ETA: 698 mins 7.7 s

################################################################################
                      Learning iteration 127/50000                      

                       Computation: 122803 steps/s (collection: 0.676s, learning 0.124s)
               Value function loss: 0.0999
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0154
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0131
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.80s
                        Total time: 107.46s
                               ETA: 697 mins 51.5 s

################################################################################
                      Learning iteration 128/50000                      

                       Computation: 137567 steps/s (collection: 0.591s, learning 0.124s)
               Value function loss: 0.1410
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0154
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0132
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0205
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.71s
                        Total time: 108.18s
                               ETA: 697 mins 2.4 s

################################################################################
                      Learning iteration 129/50000                      

                       Computation: 124299 steps/s (collection: 0.667s, learning 0.123s)
               Value function loss: 0.0868
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.88
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0153
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0128
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0020
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.79s
                        Total time: 108.97s
                               ETA: 696 mins 43.2 s

################################################################################
                      Learning iteration 130/50000                      

                       Computation: 130423 steps/s (collection: 0.630s, learning 0.124s)
               Value function loss: 0.0886
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0153
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0130
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0202
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.75s
                        Total time: 109.72s
                               ETA: 696 mins 10.2 s

################################################################################
                      Learning iteration 131/50000                      

                       Computation: 115953 steps/s (collection: 0.717s, learning 0.131s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0151
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0127
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0203
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.85s
                        Total time: 110.57s
                               ETA: 696 mins 13.2 s

################################################################################
                      Learning iteration 132/50000                      

                       Computation: 128912 steps/s (collection: 0.631s, learning 0.131s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0152
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0126
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0005
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.76s
                        Total time: 111.33s
                               ETA: 695 mins 44.3 s

################################################################################
                      Learning iteration 133/50000                      

                       Computation: 122864 steps/s (collection: 0.670s, learning 0.130s)
               Value function loss: 0.0945
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.87
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0151
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0126
           Mean episode rew_no_fly: 0.0012
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0209
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.80s
                        Total time: 112.13s
                               ETA: 695 mins 29.6 s

################################################################################
                      Learning iteration 134/50000                      

                       Computation: 130292 steps/s (collection: 0.620s, learning 0.134s)
               Value function loss: 0.0798
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0150
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0124
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.75s
                        Total time: 112.89s
                               ETA: 694 mins 58.4 s

################################################################################
                      Learning iteration 135/50000                      

                       Computation: 127718 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0037
       Mean episode rew_ang_vel_xy: -0.0150
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0124
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0206
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.77s
                        Total time: 113.66s
                               ETA: 694 mins 33.2 s

################################################################################
                      Learning iteration 136/50000                      

                       Computation: 133355 steps/s (collection: 0.598s, learning 0.139s)
               Value function loss: 0.1051
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0149
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0018
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0125
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.74s
                        Total time: 114.40s
                               ETA: 693 mins 56.5 s

################################################################################
                      Learning iteration 137/50000                      

                       Computation: 130232 steps/s (collection: 0.622s, learning 0.132s)
               Value function loss: 0.0992
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0149
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0127
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.75s
                        Total time: 115.15s
                               ETA: 693 mins 26.7 s

################################################################################
                      Learning iteration 138/50000                      

                       Computation: 120276 steps/s (collection: 0.658s, learning 0.159s)
               Value function loss: 0.0967
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.86
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0148
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0123
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.82s
                        Total time: 115.97s
                               ETA: 693 mins 19.7 s

################################################################################
                      Learning iteration 139/50000                      

                       Computation: 135226 steps/s (collection: 0.593s, learning 0.134s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0038
       Mean episode rew_ang_vel_xy: -0.0148
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0120
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0021
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.73s
                        Total time: 116.69s
                               ETA: 692 mins 40.6 s

################################################################################
                      Learning iteration 140/50000                      

                       Computation: 133938 steps/s (collection: 0.591s, learning 0.143s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 15.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0148
      Mean episode rew_base_height: -0.0004
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0122
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.73s
                        Total time: 117.43s
                               ETA: 692 mins 4.6 s

################################################################################
                      Learning iteration 141/50000                      

                       Computation: 123657 steps/s (collection: 0.657s, learning 0.138s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0147
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0124
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0003
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.79s
                        Total time: 118.22s
                               ETA: 691 mins 50.5 s

################################################################################
                      Learning iteration 142/50000                      

                       Computation: 127961 steps/s (collection: 0.627s, learning 0.141s)
               Value function loss: 0.0949
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0146
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0118
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.77s
                        Total time: 118.99s
                               ETA: 691 mins 27.2 s

################################################################################
                      Learning iteration 143/50000                      

                       Computation: 123813 steps/s (collection: 0.641s, learning 0.153s)
               Value function loss: 0.1089
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.85
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0147
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0019
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0122
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0212
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.79s
                        Total time: 119.79s
                               ETA: 691 mins 13.2 s

################################################################################
                      Learning iteration 144/50000                      

                       Computation: 132940 steps/s (collection: 0.617s, learning 0.123s)
               Value function loss: 0.0933
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0147
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0120
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.74s
                        Total time: 120.52s
                               ETA: 690 mins 40.6 s

################################################################################
                      Learning iteration 145/50000                      

                       Computation: 124372 steps/s (collection: 0.665s, learning 0.126s)
               Value function loss: 0.0884
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0147
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0116
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.79s
                        Total time: 121.32s
                               ETA: 690 mins 25.8 s

################################################################################
                      Learning iteration 146/50000                      

                       Computation: 130503 steps/s (collection: 0.631s, learning 0.123s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0146
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0116
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.75s
                        Total time: 122.07s
                               ETA: 689 mins 58.6 s

################################################################################
                      Learning iteration 147/50000                      

                       Computation: 121837 steps/s (collection: 0.670s, learning 0.137s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.84
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0146
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0118
           Mean episode rew_no_fly: 0.0013
      Mean episode rew_orientation: -0.0014
Mean episode rew_power_distribution: -0.0209
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.81s
                        Total time: 122.88s
                               ETA: 689 mins 49.9 s

################################################################################
                      Learning iteration 148/50000                      

                       Computation: 120355 steps/s (collection: 0.695s, learning 0.122s)
               Value function loss: 0.1030
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0145
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0118
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.82s
                        Total time: 123.69s
                               ETA: 689 mins 44.5 s

################################################################################
                      Learning iteration 149/50000                      

                       Computation: 129447 steps/s (collection: 0.628s, learning 0.131s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0145
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0115
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0066
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.76s
                        Total time: 124.45s
                               ETA: 689 mins 20.2 s

################################################################################
                      Learning iteration 150/50000                      

                       Computation: 117205 steps/s (collection: 0.698s, learning 0.141s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0144
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0119
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0212
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.84s
                        Total time: 125.29s
                               ETA: 689 mins 22.4 s

################################################################################
                      Learning iteration 151/50000                      

                       Computation: 126355 steps/s (collection: 0.639s, learning 0.139s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0144
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0119
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.78s
                        Total time: 126.07s
                               ETA: 689 mins 4.6 s

################################################################################
                      Learning iteration 152/50000                      

                       Computation: 116651 steps/s (collection: 0.706s, learning 0.137s)
               Value function loss: 0.0857
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.83
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0142
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0020
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0117
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0022
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.84s
                        Total time: 126.91s
                               ETA: 689 mins 8.1 s

################################################################################
                      Learning iteration 153/50000                      

                       Computation: 127754 steps/s (collection: 0.644s, learning 0.126s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0143
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0117
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.77s
                        Total time: 127.68s
                               ETA: 688 mins 47.8 s

################################################################################
                      Learning iteration 154/50000                      

                       Computation: 124468 steps/s (collection: 0.660s, learning 0.130s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0142
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0116
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.79s
                        Total time: 128.47s
                               ETA: 688 mins 34.3 s

################################################################################
                      Learning iteration 155/50000                      

                       Computation: 128512 steps/s (collection: 0.641s, learning 0.124s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 16.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0142
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0112
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.76s
                        Total time: 129.24s
                               ETA: 688 mins 13.1 s

################################################################################
                      Learning iteration 156/50000                      

                       Computation: 133269 steps/s (collection: 0.613s, learning 0.125s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.82
               Mean reward (total): -5.63
                Mean reward (task): -5.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0039
       Mean episode rew_ang_vel_xy: -0.0142
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0112
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0209
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.74s
                        Total time: 129.97s
                               ETA: 687 mins 43.4 s

################################################################################
                      Learning iteration 157/50000                      

                       Computation: 129334 steps/s (collection: 0.633s, learning 0.127s)
               Value function loss: 0.0930
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0143
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0116
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0212
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0006
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.76s
                        Total time: 130.73s
                               ETA: 687 mins 21.2 s

################################################################################
                      Learning iteration 158/50000                      

                       Computation: 129850 steps/s (collection: 0.631s, learning 0.126s)
               Value function loss: 0.0915
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0143
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0110
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.76s
                        Total time: 131.49s
                               ETA: 686 mins 58.4 s

################################################################################
                      Learning iteration 159/50000                      

                       Computation: 132370 steps/s (collection: 0.620s, learning 0.122s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0141
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0109
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0063
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.74s
                        Total time: 132.23s
                               ETA: 686 mins 31.3 s

################################################################################
                      Learning iteration 160/50000                      

                       Computation: 129579 steps/s (collection: 0.634s, learning 0.124s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0142
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0109
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.76s
                        Total time: 132.99s
                               ETA: 686 mins 9.4 s

################################################################################
                      Learning iteration 161/50000                      

                       Computation: 130638 steps/s (collection: 0.614s, learning 0.138s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.81
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0141
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0111
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0023
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.75s
                        Total time: 133.74s
                               ETA: 685 mins 46.0 s

################################################################################
                      Learning iteration 162/50000                      

                       Computation: 120557 steps/s (collection: 0.679s, learning 0.137s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0040
       Mean episode rew_ang_vel_xy: -0.0141
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0108
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.82s
                        Total time: 134.56s
                               ETA: 685 mins 42.0 s

################################################################################
                      Learning iteration 163/50000                      

                       Computation: 118836 steps/s (collection: 0.702s, learning 0.125s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): -5.53
                Mean reward (task): -5.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0140
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0110
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.83s
                        Total time: 135.39s
                               ETA: 685 mins 41.7 s

################################################################################
                      Learning iteration 164/50000                      

                       Computation: 126647 steps/s (collection: 0.652s, learning 0.124s)
               Value function loss: 0.0890
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0139
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0107
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.78s
                        Total time: 136.16s
                               ETA: 685 mins 26.0 s

################################################################################
                      Learning iteration 165/50000                      

                       Computation: 115354 steps/s (collection: 0.720s, learning 0.132s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.80
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0140
      Mean episode rew_base_height: -0.0005
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0110
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0064
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.85s
                        Total time: 137.01s
                               ETA: 685 mins 33.3 s

################################################################################
                      Learning iteration 166/50000                      

                       Computation: 128522 steps/s (collection: 0.642s, learning 0.123s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0138
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0109
           Mean episode rew_no_fly: 0.0014
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.76s
                        Total time: 137.78s
                               ETA: 685 mins 14.4 s

################################################################################
                      Learning iteration 167/50000                      

                       Computation: 129924 steps/s (collection: 0.633s, learning 0.124s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 17.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0137
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0107
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.76s
                        Total time: 138.54s
                               ETA: 684 mins 53.3 s

################################################################################
                      Learning iteration 168/50000                      

                       Computation: 132514 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0137
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0108
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.74s
                        Total time: 139.28s
                               ETA: 684 mins 28.1 s

################################################################################
                      Learning iteration 169/50000                      

                       Computation: 118350 steps/s (collection: 0.706s, learning 0.124s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0137
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0106
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0015
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0024
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.83s
                        Total time: 140.11s
                               ETA: 684 mins 29.1 s

################################################################################
                      Learning iteration 170/50000                      

                       Computation: 114590 steps/s (collection: 0.721s, learning 0.137s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0136
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0107
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.86s
                        Total time: 140.97s
                               ETA: 684 mins 38.1 s

################################################################################
                      Learning iteration 171/50000                      

                       Computation: 130966 steps/s (collection: 0.623s, learning 0.128s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.79
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0136
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0105
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.75s
                        Total time: 141.72s
                               ETA: 684 mins 15.9 s

################################################################################
                      Learning iteration 172/50000                      

                       Computation: 121156 steps/s (collection: 0.686s, learning 0.126s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0136
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0021
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0105
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.81s
                        Total time: 142.53s
                               ETA: 684 mins 11.5 s

################################################################################
                      Learning iteration 173/50000                      

                       Computation: 127525 steps/s (collection: 0.648s, learning 0.123s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0137
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0102
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0066
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.77s
                        Total time: 143.30s
                               ETA: 683 mins 55.5 s

################################################################################
                      Learning iteration 174/50000                      

                       Computation: 124071 steps/s (collection: 0.667s, learning 0.125s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0041
       Mean episode rew_ang_vel_xy: -0.0136
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0102
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0065
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0025
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.79s
                        Total time: 144.09s
                               ETA: 683 mins 45.8 s

################################################################################
                      Learning iteration 175/50000                      

                       Computation: 124695 steps/s (collection: 0.665s, learning 0.124s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0042
       Mean episode rew_ang_vel_xy: -0.0136
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0105
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0066
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.79s
                        Total time: 144.88s
                               ETA: 683 mins 35.0 s

################################################################################
                      Learning iteration 176/50000                      

                       Computation: 129120 steps/s (collection: 0.616s, learning 0.145s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.78
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0042
       Mean episode rew_ang_vel_xy: -0.0134
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0022
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0104
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.76s
                        Total time: 145.64s
                               ETA: 683 mins 16.8 s

################################################################################
                      Learning iteration 177/50000                      

                       Computation: 132950 steps/s (collection: 0.615s, learning 0.124s)
               Value function loss: 0.0674
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0042
       Mean episode rew_ang_vel_xy: -0.0134
      Mean episode rew_base_height: -0.0006
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0105
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.74s
                        Total time: 146.38s
                               ETA: 682 mins 52.6 s

################################################################################
                      Learning iteration 178/50000                      

                       Computation: 130921 steps/s (collection: 0.616s, learning 0.135s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0042
       Mean episode rew_ang_vel_xy: -0.0133
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0102
           Mean episode rew_no_fly: 0.0015
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.75s
                        Total time: 147.13s
                               ETA: 682 mins 31.9 s

################################################################################
                      Learning iteration 179/50000                      

                       Computation: 126550 steps/s (collection: 0.648s, learning 0.129s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0042
       Mean episode rew_ang_vel_xy: -0.0134
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0104
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0223
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.78s
                        Total time: 147.91s
                               ETA: 682 mins 18.6 s

################################################################################
                      Learning iteration 180/50000                      

                       Computation: 127062 steps/s (collection: 0.640s, learning 0.134s)
               Value function loss: 0.0920
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0042
       Mean episode rew_ang_vel_xy: -0.0132
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0103
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.77s
                        Total time: 148.68s
                               ETA: 682 mins 4.5 s

################################################################################
                      Learning iteration 181/50000                      

                       Computation: 121938 steps/s (collection: 0.671s, learning 0.135s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0132
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0106
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0068
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0026
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.81s
                        Total time: 149.49s
                               ETA: 681 mins 59.5 s

################################################################################
                      Learning iteration 182/50000                      

                       Computation: 116916 steps/s (collection: 0.719s, learning 0.122s)
               Value function loss: 0.1233
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0133
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0104
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0067
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.84s
                        Total time: 150.33s
                               ETA: 682 mins 4.0 s

################################################################################
                      Learning iteration 183/50000                      

                       Computation: 135069 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0132
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0105
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0068
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.73s
                        Total time: 151.06s
                               ETA: 681 mins 37.8 s

################################################################################
                      Learning iteration 184/50000                      

                       Computation: 125499 steps/s (collection: 0.651s, learning 0.132s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0043
       Mean episode rew_ang_vel_xy: -0.0133
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0109
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0068
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.78s
                        Total time: 151.84s
                               ETA: 681 mins 26.9 s

################################################################################
                      Learning iteration 185/50000                      

                       Computation: 119075 steps/s (collection: 0.701s, learning 0.125s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.77
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0132
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0104
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0068
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.83s
                        Total time: 152.67s
                               ETA: 681 mins 27.3 s

################################################################################
                      Learning iteration 186/50000                      

                       Computation: 119479 steps/s (collection: 0.684s, learning 0.139s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0130
      Mean episode rew_base_height: -0.0007
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0104
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0069
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0027
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.82s
                        Total time: 153.49s
                               ETA: 681 mins 27.0 s

################################################################################
                      Learning iteration 187/50000                      

                       Computation: 119122 steps/s (collection: 0.686s, learning 0.139s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0131
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0102
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0016
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0069
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.83s
                        Total time: 154.31s
                               ETA: 681 mins 27.4 s

################################################################################
                      Learning iteration 188/50000                      

                       Computation: 126988 steps/s (collection: 0.632s, learning 0.142s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0130
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0023
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0101
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0069
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.77s
                        Total time: 155.09s
                               ETA: 681 mins 14.3 s

################################################################################
                      Learning iteration 189/50000                      

                       Computation: 118763 steps/s (collection: 0.704s, learning 0.124s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0129
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0105
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0230
       Mean episode rew_smoothness: -0.0069
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.83s
                        Total time: 155.92s
                               ETA: 681 mins 15.3 s

################################################################################
                      Learning iteration 190/50000                      

                       Computation: 122146 steps/s (collection: 0.679s, learning 0.126s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.76
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0129
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0102
           Mean episode rew_no_fly: 0.0016
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0068
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.80s
                        Total time: 156.72s
                               ETA: 681 mins 10.4 s

################################################################################
                      Learning iteration 191/50000                      

                       Computation: 128085 steps/s (collection: 0.633s, learning 0.135s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0128
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0069
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.77s
                        Total time: 157.49s
                               ETA: 680 mins 55.8 s

################################################################################
                      Learning iteration 192/50000                      

                       Computation: 119615 steps/s (collection: 0.699s, learning 0.123s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0128
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0101
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0069
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0028
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.82s
                        Total time: 158.31s
                               ETA: 680 mins 55.4 s

################################################################################
                      Learning iteration 193/50000                      

                       Computation: 125045 steps/s (collection: 0.631s, learning 0.155s)
               Value function loss: 0.0963
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0044
       Mean episode rew_ang_vel_xy: -0.0129
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0102
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0069
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.79s
                        Total time: 159.10s
                               ETA: 680 mins 45.8 s

################################################################################
                      Learning iteration 194/50000                      

                       Computation: 130789 steps/s (collection: 0.615s, learning 0.137s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0128
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0070
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.75s
                        Total time: 159.85s
                               ETA: 680 mins 27.5 s

################################################################################
                      Learning iteration 195/50000                      

                       Computation: 124878 steps/s (collection: 0.648s, learning 0.139s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0129
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0024
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0104
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0070
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0007
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.79s
                        Total time: 160.63s
                               ETA: 680 mins 18.4 s

################################################################################
                      Learning iteration 196/50000                      

                       Computation: 128222 steps/s (collection: 0.623s, learning 0.144s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.57
                Mean reward (task): -5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 18.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0127
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0102
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0071
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.77s
                        Total time: 161.40s
                               ETA: 680 mins 4.2 s

################################################################################
                      Learning iteration 197/50000                      

                       Computation: 123556 steps/s (collection: 0.657s, learning 0.139s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0126
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0071
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.80s
                        Total time: 162.20s
                               ETA: 679 mins 57.4 s

################################################################################
                      Learning iteration 198/50000                      

                       Computation: 132385 steps/s (collection: 0.604s, learning 0.138s)
               Value function loss: 0.1557
                    Surrogate loss: -0.0032
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0125
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0101
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0071
      Mean episode rew_stand_still: -0.0004
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.74s
                        Total time: 162.94s
                               ETA: 679 mins 37.5 s

################################################################################
                      Learning iteration 199/50000                      

                       Computation: 133019 steps/s (collection: 0.612s, learning 0.127s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0045
       Mean episode rew_ang_vel_xy: -0.0126
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0099
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0071
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0029
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.74s
                        Total time: 163.68s
                               ETA: 679 mins 16.8 s

################################################################################
                      Learning iteration 200/50000                      

                       Computation: 109306 steps/s (collection: 0.761s, learning 0.138s)
               Value function loss: 0.0841
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0125
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0071
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.90s
                        Total time: 164.58s
                               ETA: 679 mins 36.0 s

################################################################################
                      Learning iteration 201/50000                      

                       Computation: 127851 steps/s (collection: 0.644s, learning 0.124s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0126
      Mean episode rew_base_height: -0.0008
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.77s
                        Total time: 165.35s
                               ETA: 679 mins 22.9 s

################################################################################
                      Learning iteration 202/50000                      

                       Computation: 127734 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0126
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0103
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.77s
                        Total time: 166.12s
                               ETA: 679 mins 10.1 s

################################################################################
                      Learning iteration 203/50000                      

                       Computation: 117927 steps/s (collection: 0.701s, learning 0.132s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0124
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0099
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.83s
                        Total time: 166.95s
                               ETA: 679 mins 13.0 s

################################################################################
                      Learning iteration 204/50000                      

                       Computation: 121317 steps/s (collection: 0.668s, learning 0.142s)
               Value function loss: 0.0841
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0125
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.81s
                        Total time: 167.76s
                               ETA: 679 mins 10.2 s

################################################################################
                      Learning iteration 205/50000                      

                       Computation: 128444 steps/s (collection: 0.637s, learning 0.129s)
               Value function loss: 0.0925
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.68
                Mean reward (task): -5.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0125
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0101
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.77s
                        Total time: 168.53s
                               ETA: 678 mins 56.6 s

################################################################################
                      Learning iteration 206/50000                      

                       Computation: 123599 steps/s (collection: 0.672s, learning 0.124s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.55
                Mean reward (task): -5.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0125
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0097
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.80s
                        Total time: 169.32s
                               ETA: 678 mins 50.3 s

################################################################################
                      Learning iteration 207/50000                      

                       Computation: 120445 steps/s (collection: 0.672s, learning 0.144s)
               Value function loss: 0.0918
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0046
       Mean episode rew_ang_vel_xy: -0.0125
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0240
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.82s
                        Total time: 170.14s
                               ETA: 678 mins 49.0 s

################################################################################
                      Learning iteration 208/50000                      

                       Computation: 135013 steps/s (collection: 0.605s, learning 0.123s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0124
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0098
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.73s
                        Total time: 170.87s
                               ETA: 678 mins 26.8 s

################################################################################
                      Learning iteration 209/50000                      

                       Computation: 125604 steps/s (collection: 0.647s, learning 0.136s)
               Value function loss: 0.0803
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0124
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0097
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.78s
                        Total time: 171.65s
                               ETA: 678 mins 17.7 s

################################################################################
                      Learning iteration 210/50000                      

                       Computation: 115759 steps/s (collection: 0.707s, learning 0.142s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.75
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0124
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0025
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0030
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.85s
                        Total time: 172.50s
                               ETA: 678 mins 24.4 s

################################################################################
                      Learning iteration 211/50000                      

                       Computation: 116749 steps/s (collection: 0.699s, learning 0.143s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0124
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0005
        Mean episode rew_lin_vel_z: -0.0099
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0072
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.84s
                        Total time: 173.34s
                               ETA: 678 mins 29.3 s

################################################################################
                      Learning iteration 212/50000                      

                       Computation: 123054 steps/s (collection: 0.647s, learning 0.152s)
               Value function loss: 0.0819
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0047
       Mean episode rew_ang_vel_xy: -0.0123
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0098
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0073
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.80s
                        Total time: 174.14s
                               ETA: 678 mins 24.1 s

################################################################################
                      Learning iteration 213/50000                      

                       Computation: 128280 steps/s (collection: 0.627s, learning 0.139s)
               Value function loss: 0.0898
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0122
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0098
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0073
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.77s
                        Total time: 174.90s
                               ETA: 678 mins 11.4 s

################################################################################
                      Learning iteration 214/50000                      

                       Computation: 118902 steps/s (collection: 0.703s, learning 0.124s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0122
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0099
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0073
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.83s
                        Total time: 175.73s
                               ETA: 678 mins 12.8 s

################################################################################
                      Learning iteration 215/50000                      

                       Computation: 114896 steps/s (collection: 0.715s, learning 0.140s)
               Value function loss: 0.0798
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0122
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0096
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0241
       Mean episode rew_smoothness: -0.0074
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.86s
                        Total time: 176.59s
                               ETA: 678 mins 20.8 s

################################################################################
                      Learning iteration 216/50000                      

                       Computation: 134566 steps/s (collection: 0.606s, learning 0.124s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0048
       Mean episode rew_ang_vel_xy: -0.0122
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0098
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0243
       Mean episode rew_smoothness: -0.0074
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.73s
                        Total time: 177.32s
                               ETA: 677 mins 60.0 s

################################################################################
                      Learning iteration 217/50000                      

                       Computation: 108564 steps/s (collection: 0.762s, learning 0.143s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0121
      Mean episode rew_base_height: -0.0009
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0026
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0096
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0075
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.91s
                        Total time: 178.22s
                               ETA: 678 mins 19.3 s

################################################################################
                      Learning iteration 218/50000                      

                       Computation: 126174 steps/s (collection: 0.654s, learning 0.125s)
               Value function loss: 0.0898
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0122
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0100
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0075
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.78s
                        Total time: 179.00s
                               ETA: 678 mins 9.8 s

################################################################################
                      Learning iteration 219/50000                      

                       Computation: 118948 steps/s (collection: 0.701s, learning 0.126s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0049
       Mean episode rew_ang_vel_xy: -0.0121
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0096
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0247
       Mean episode rew_smoothness: -0.0075
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.83s
                        Total time: 179.83s
                               ETA: 678 mins 11.0 s

################################################################################
                      Learning iteration 220/50000                      

                       Computation: 119547 steps/s (collection: 0.688s, learning 0.134s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0120
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0097
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0075
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.82s
                        Total time: 180.65s
                               ETA: 678 mins 11.3 s

################################################################################
                      Learning iteration 221/50000                      

                       Computation: 132559 steps/s (collection: 0.613s, learning 0.128s)
               Value function loss: 0.0939
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0120
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0094
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0251
       Mean episode rew_smoothness: -0.0075
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.74s
                        Total time: 181.39s
                               ETA: 677 mins 53.5 s

################################################################################
                      Learning iteration 222/50000                      

                       Computation: 123903 steps/s (collection: 0.671s, learning 0.123s)
               Value function loss: 0.0894
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.57
                Mean reward (task): -5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 19.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0120
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0097
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0075
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.79s
                        Total time: 182.19s
                               ETA: 677 mins 47.4 s

################################################################################
                      Learning iteration 223/50000                      

                       Computation: 120664 steps/s (collection: 0.688s, learning 0.127s)
               Value function loss: 0.0894
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0050
       Mean episode rew_ang_vel_xy: -0.0121
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0096
           Mean episode rew_no_fly: 0.0017
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0247
       Mean episode rew_smoothness: -0.0076
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.81s
                        Total time: 183.00s
                               ETA: 677 mins 46.1 s

################################################################################
                      Learning iteration 224/50000                      

                       Computation: 119183 steps/s (collection: 0.679s, learning 0.146s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0119
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0098
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0246
       Mean episode rew_smoothness: -0.0076
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.82s
                        Total time: 183.82s
                               ETA: 677 mins 47.0 s

################################################################################
                      Learning iteration 225/50000                      

                       Computation: 125176 steps/s (collection: 0.648s, learning 0.138s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0119
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0094
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0247
       Mean episode rew_smoothness: -0.0076
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.79s
                        Total time: 184.61s
                               ETA: 677 mins 39.2 s

################################################################################
                      Learning iteration 226/50000                      

                       Computation: 126898 steps/s (collection: 0.647s, learning 0.128s)
               Value function loss: 0.0947
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0119
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0094
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0076
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0031
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.77s
                        Total time: 185.38s
                               ETA: 677 mins 29.1 s

################################################################################
                      Learning iteration 227/50000                      

                       Computation: 118921 steps/s (collection: 0.680s, learning 0.146s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0119
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0091
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0246
       Mean episode rew_smoothness: -0.0076
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.83s
                        Total time: 186.21s
                               ETA: 677 mins 30.5 s

################################################################################
                      Learning iteration 228/50000                      

                       Computation: 118758 steps/s (collection: 0.691s, learning 0.137s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0119
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0093
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0076
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.83s
                        Total time: 187.04s
                               ETA: 677 mins 32.1 s

################################################################################
                      Learning iteration 229/50000                      

                       Computation: 128135 steps/s (collection: 0.644s, learning 0.123s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0117
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0092
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0246
       Mean episode rew_smoothness: -0.0077
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.77s
                        Total time: 187.81s
                               ETA: 677 mins 20.5 s

################################################################################
                      Learning iteration 230/50000                      

                       Computation: 126981 steps/s (collection: 0.650s, learning 0.124s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.74
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0117
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0077
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.77s
                        Total time: 188.58s
                               ETA: 677 mins 10.6 s

################################################################################
                      Learning iteration 231/50000                      

                       Computation: 124983 steps/s (collection: 0.647s, learning 0.139s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.57
                Mean reward (task): -5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0116
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0091
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0246
       Mean episode rew_smoothness: -0.0077
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.79s
                        Total time: 189.37s
                               ETA: 677 mins 3.4 s

################################################################################
                      Learning iteration 232/50000                      

                       Computation: 126783 steps/s (collection: 0.652s, learning 0.124s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0116
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0091
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0077
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.78s
                        Total time: 190.14s
                               ETA: 676 mins 53.8 s

################################################################################
                      Learning iteration 233/50000                      

                       Computation: 129580 steps/s (collection: 0.634s, learning 0.124s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0116
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0091
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0077
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.76s
                        Total time: 190.90s
                               ETA: 676 mins 40.8 s

################################################################################
                      Learning iteration 234/50000                      

                       Computation: 124010 steps/s (collection: 0.663s, learning 0.130s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0116
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0077
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.79s
                        Total time: 191.69s
                               ETA: 676 mins 35.1 s

################################################################################
                      Learning iteration 235/50000                      

                       Computation: 126033 steps/s (collection: 0.655s, learning 0.125s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0116
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0243
       Mean episode rew_smoothness: -0.0077
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.78s
                        Total time: 192.47s
                               ETA: 676 mins 26.7 s

################################################################################
                      Learning iteration 236/50000                      

                       Computation: 122661 steps/s (collection: 0.664s, learning 0.137s)
               Value function loss: 0.0659
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0051
       Mean episode rew_ang_vel_xy: -0.0114
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0243
       Mean episode rew_smoothness: -0.0078
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.80s
                        Total time: 193.28s
                               ETA: 676 mins 22.9 s

################################################################################
                      Learning iteration 237/50000                      

                       Computation: 113845 steps/s (collection: 0.729s, learning 0.134s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0115
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.86s
                        Total time: 194.14s
                               ETA: 676 mins 32.2 s

################################################################################
                      Learning iteration 238/50000                      

                       Computation: 127145 steps/s (collection: 0.633s, learning 0.140s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0115
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0250
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0008
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.77s
                        Total time: 194.91s
                               ETA: 676 mins 22.5 s

################################################################################
                      Learning iteration 239/50000                      

                       Computation: 124826 steps/s (collection: 0.663s, learning 0.124s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0114
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.79s
                        Total time: 195.70s
                               ETA: 676 mins 15.9 s

################################################################################
                      Learning iteration 240/50000                      

                       Computation: 112743 steps/s (collection: 0.730s, learning 0.142s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0115
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.87s
                        Total time: 196.57s
                               ETA: 676 mins 26.7 s

################################################################################
                      Learning iteration 241/50000                      

                       Computation: 133951 steps/s (collection: 0.606s, learning 0.128s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 20.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0052
       Mean episode rew_ang_vel_xy: -0.0114
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0027
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0250
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0047
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.73s
                        Total time: 197.31s
                               ETA: 676 mins 9.1 s

################################################################################
                      Learning iteration 242/50000                      

                       Computation: 128772 steps/s (collection: 0.641s, learning 0.123s)
               Value function loss: 0.0793
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0114
      Mean episode rew_base_height: -0.0010
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0247
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.76s
                        Total time: 198.07s
                               ETA: 675 mins 57.6 s

################################################################################
                      Learning iteration 243/50000                      

                       Computation: 129045 steps/s (collection: 0.624s, learning 0.138s)
               Value function loss: 0.1009
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0114
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0091
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0250
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.76s
                        Total time: 198.83s
                               ETA: 675 mins 45.9 s

################################################################################
                      Learning iteration 244/50000                      

                       Computation: 116131 steps/s (collection: 0.723s, learning 0.123s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0114
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.85s
                        Total time: 199.68s
                               ETA: 675 mins 51.5 s

################################################################################
                      Learning iteration 245/50000                      

                       Computation: 114639 steps/s (collection: 0.708s, learning 0.149s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0113
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0252
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.86s
                        Total time: 200.53s
                               ETA: 675 mins 59.3 s

################################################################################
                      Learning iteration 246/50000                      

                       Computation: 126348 steps/s (collection: 0.651s, learning 0.127s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0113
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0018
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0032
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.78s
                        Total time: 201.31s
                               ETA: 675 mins 51.0 s

################################################################################
                      Learning iteration 247/50000                      

                       Computation: 117289 steps/s (collection: 0.693s, learning 0.145s)
               Value function loss: 0.0793
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0112
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.84s
                        Total time: 202.15s
                               ETA: 675 mins 54.8 s

################################################################################
                      Learning iteration 248/50000                      

                       Computation: 115683 steps/s (collection: 0.699s, learning 0.151s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.65
                Mean reward (task): -5.65
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0113
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0247
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.85s
                        Total time: 203.00s
                               ETA: 676 mins 1.0 s

################################################################################
                      Learning iteration 249/50000                      

                       Computation: 123402 steps/s (collection: 0.656s, learning 0.141s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.73
               Mean reward (total): -5.70
                Mean reward (task): -5.70
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0114
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.80s
                        Total time: 203.80s
                               ETA: 675 mins 56.4 s

################################################################################
                      Learning iteration 250/50000                      

                       Computation: 122866 steps/s (collection: 0.662s, learning 0.138s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0112
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0250
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.80s
                        Total time: 204.60s
                               ETA: 675 mins 52.6 s

################################################################################
                      Learning iteration 251/50000                      

                       Computation: 119471 steps/s (collection: 0.684s, learning 0.139s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0112
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.82s
                        Total time: 205.42s
                               ETA: 675 mins 53.3 s

################################################################################
                      Learning iteration 252/50000                      

                       Computation: 117237 steps/s (collection: 0.711s, learning 0.128s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0113
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0252
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.84s
                        Total time: 206.26s
                               ETA: 675 mins 57.1 s

################################################################################
                      Learning iteration 253/50000                      

                       Computation: 126735 steps/s (collection: 0.636s, learning 0.140s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0112
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.78s
                        Total time: 207.03s
                               ETA: 675 mins 48.5 s

################################################################################
                      Learning iteration 254/50000                      

                       Computation: 128986 steps/s (collection: 0.622s, learning 0.140s)
               Value function loss: 0.0950
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0111
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0251
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.76s
                        Total time: 207.80s
                               ETA: 675 mins 37.4 s

################################################################################
                      Learning iteration 255/50000                      

                       Computation: 116938 steps/s (collection: 0.716s, learning 0.125s)
               Value function loss: 0.0780
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0113
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.84s
                        Total time: 208.64s
                               ETA: 675 mins 41.6 s

################################################################################
                      Learning iteration 256/50000                      

                       Computation: 133562 steps/s (collection: 0.614s, learning 0.122s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0112
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.74s
                        Total time: 209.37s
                               ETA: 675 mins 25.5 s

################################################################################
                      Learning iteration 257/50000                      

                       Computation: 127140 steps/s (collection: 0.642s, learning 0.132s)
               Value function loss: 0.1002
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0111
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0246
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0033
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.77s
                        Total time: 210.15s
                               ETA: 675 mins 16.7 s

################################################################################
                      Learning iteration 258/50000                      

                       Computation: 127198 steps/s (collection: 0.650s, learning 0.123s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0112
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.77s
                        Total time: 210.92s
                               ETA: 675 mins 7.8 s

################################################################################
                      Learning iteration 259/50000                      

                       Computation: 126930 steps/s (collection: 0.652s, learning 0.123s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0251
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.77s
                        Total time: 211.69s
                               ETA: 674 mins 59.4 s

################################################################################
                      Learning iteration 260/50000                      

                       Computation: 133737 steps/s (collection: 0.613s, learning 0.122s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0047
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.74s
                        Total time: 212.43s
                               ETA: 674 mins 43.5 s

################################################################################
                      Learning iteration 261/50000                      

                       Computation: 125947 steps/s (collection: 0.638s, learning 0.143s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.78s
                        Total time: 213.21s
                               ETA: 674 mins 36.3 s

################################################################################
                      Learning iteration 262/50000                      

                       Computation: 131219 steps/s (collection: 0.626s, learning 0.123s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.75s
                        Total time: 213.96s
                               ETA: 674 mins 23.3 s

################################################################################
                      Learning iteration 263/50000                      

                       Computation: 134066 steps/s (collection: 0.600s, learning 0.133s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.73s
                        Total time: 214.69s
                               ETA: 674 mins 7.4 s

################################################################################
                      Learning iteration 264/50000                      

                       Computation: 130497 steps/s (collection: 0.614s, learning 0.140s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.75s
                        Total time: 215.44s
                               ETA: 673 mins 55.3 s

################################################################################
                      Learning iteration 265/50000                      

                       Computation: 129609 steps/s (collection: 0.614s, learning 0.144s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0111
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.76s
                        Total time: 216.20s
                               ETA: 673 mins 44.3 s

################################################################################
                      Learning iteration 266/50000                      

                       Computation: 129877 steps/s (collection: 0.615s, learning 0.142s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.53
                Mean reward (task): -5.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.76s
                        Total time: 216.96s
                               ETA: 673 mins 33.1 s

################################################################################
                      Learning iteration 267/50000                      

                       Computation: 109088 steps/s (collection: 0.728s, learning 0.173s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0047
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.90s
                        Total time: 217.86s
                               ETA: 673 mins 48.7 s

################################################################################
                      Learning iteration 268/50000                      

                       Computation: 124094 steps/s (collection: 0.667s, learning 0.125s)
               Value function loss: 0.0889
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0046
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.79s
                        Total time: 218.65s
                               ETA: 673 mins 44.0 s

################################################################################
                      Learning iteration 269/50000                      

                       Computation: 129843 steps/s (collection: 0.633s, learning 0.124s)
               Value function loss: 0.0926
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 21.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.76s
                        Total time: 219.41s
                               ETA: 673 mins 33.0 s

################################################################################
                      Learning iteration 270/50000                      

                       Computation: 121930 steps/s (collection: 0.684s, learning 0.122s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.81s
                        Total time: 220.22s
                               ETA: 673 mins 31.0 s

################################################################################
                      Learning iteration 271/50000                      

                       Computation: 124253 steps/s (collection: 0.629s, learning 0.162s)
               Value function loss: 0.0795
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.79s
                        Total time: 221.01s
                               ETA: 673 mins 26.2 s

################################################################################
                      Learning iteration 272/50000                      

                       Computation: 124624 steps/s (collection: 0.651s, learning 0.138s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0028
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0079
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.79s
                        Total time: 221.80s
                               ETA: 673 mins 21.1 s

################################################################################
                      Learning iteration 273/50000                      

                       Computation: 127021 steps/s (collection: 0.648s, learning 0.125s)
               Value function loss: 0.1134
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0053
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.77s
                        Total time: 222.57s
                               ETA: 673 mins 13.3 s

################################################################################
                      Learning iteration 274/50000                      

                       Computation: 118194 steps/s (collection: 0.708s, learning 0.124s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.83s
                        Total time: 223.40s
                               ETA: 673 mins 16.0 s

################################################################################
                      Learning iteration 275/50000                      

                       Computation: 127346 steps/s (collection: 0.650s, learning 0.122s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.77s
                        Total time: 224.17s
                               ETA: 673 mins 7.9 s

################################################################################
                      Learning iteration 276/50000                      

                       Computation: 121259 steps/s (collection: 0.680s, learning 0.130s)
               Value function loss: 0.1087
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.81s
                        Total time: 224.98s
                               ETA: 673 mins 6.8 s

################################################################################
                      Learning iteration 277/50000                      

                       Computation: 121535 steps/s (collection: 0.685s, learning 0.124s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0054
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0034
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.81s
                        Total time: 225.79s
                               ETA: 673 mins 5.4 s

################################################################################
                      Learning iteration 278/50000                      

                       Computation: 127379 steps/s (collection: 0.647s, learning 0.125s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.77s
                        Total time: 226.57s
                               ETA: 672 mins 57.4 s

################################################################################
                      Learning iteration 279/50000                      

                       Computation: 112790 steps/s (collection: 0.727s, learning 0.144s)
               Value function loss: 0.0954
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.87s
                        Total time: 227.44s
                               ETA: 673 mins 7.1 s

################################################################################
                      Learning iteration 280/50000                      

                       Computation: 122614 steps/s (collection: 0.678s, learning 0.124s)
               Value function loss: 0.0843
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.80s
                        Total time: 228.24s
                               ETA: 673 mins 4.5 s

################################################################################
                      Learning iteration 281/50000                      

                       Computation: 127716 steps/s (collection: 0.643s, learning 0.127s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.77s
                        Total time: 229.01s
                               ETA: 672 mins 56.1 s

################################################################################
                      Learning iteration 282/50000                      

                       Computation: 126779 steps/s (collection: 0.641s, learning 0.134s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.78s
                        Total time: 229.78s
                               ETA: 672 mins 48.9 s

################################################################################
                      Learning iteration 283/50000                      

                       Computation: 131412 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0868
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.75s
                        Total time: 230.53s
                               ETA: 672 mins 36.9 s

################################################################################
                      Learning iteration 284/50000                      

                       Computation: 127121 steps/s (collection: 0.638s, learning 0.135s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.77s
                        Total time: 231.31s
                               ETA: 672 mins 29.4 s

################################################################################
                      Learning iteration 285/50000                      

                       Computation: 119696 steps/s (collection: 0.694s, learning 0.127s)
               Value function loss: 0.0920
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.82s
                        Total time: 232.13s
                               ETA: 672 mins 30.2 s

################################################################################
                      Learning iteration 286/50000                      

                       Computation: 122158 steps/s (collection: 0.680s, learning 0.125s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0110
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0035
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.80s
                        Total time: 232.93s
                               ETA: 672 mins 28.2 s

################################################################################
                      Learning iteration 287/50000                      

                       Computation: 128180 steps/s (collection: 0.644s, learning 0.123s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0055
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0019
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.77s
                        Total time: 233.70s
                               ETA: 672 mins 19.7 s

################################################################################
                      Learning iteration 288/50000                      

                       Computation: 120780 steps/s (collection: 0.672s, learning 0.142s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.81s
                        Total time: 234.51s
                               ETA: 672 mins 19.3 s

################################################################################
                      Learning iteration 289/50000                      

                       Computation: 121794 steps/s (collection: 0.664s, learning 0.143s)
               Value function loss: 0.0974
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.81s
                        Total time: 235.32s
                               ETA: 672 mins 17.8 s

################################################################################
                      Learning iteration 290/50000                      

                       Computation: 117344 steps/s (collection: 0.701s, learning 0.137s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.84s
                        Total time: 236.16s
                               ETA: 672 mins 21.4 s

################################################################################
                      Learning iteration 291/50000                      

                       Computation: 130111 steps/s (collection: 0.632s, learning 0.124s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.76s
                        Total time: 236.91s
                               ETA: 672 mins 11.1 s

################################################################################
                      Learning iteration 292/50000                      

                       Computation: 118551 steps/s (collection: 0.694s, learning 0.135s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.55
                Mean reward (task): -5.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0056
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.83s
                        Total time: 237.74s
                               ETA: 672 mins 13.3 s

################################################################################
                      Learning iteration 293/50000                      

                       Computation: 110317 steps/s (collection: 0.730s, learning 0.161s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.89s
                        Total time: 238.63s
                               ETA: 672 mins 26.0 s

################################################################################
                      Learning iteration 294/50000                      

                       Computation: 123971 steps/s (collection: 0.668s, learning 0.125s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.79s
                        Total time: 239.43s
                               ETA: 672 mins 22.0 s

################################################################################
                      Learning iteration 295/50000                      

                       Computation: 130382 steps/s (collection: 0.630s, learning 0.124s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.75s
                        Total time: 240.18s
                               ETA: 672 mins 11.5 s

################################################################################
                      Learning iteration 296/50000                      

                       Computation: 127797 steps/s (collection: 0.629s, learning 0.140s)
               Value function loss: 0.0858
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.60
                Mean reward (task): -5.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.77s
                        Total time: 240.95s
                               ETA: 672 mins 3.7 s

################################################################################
                      Learning iteration 297/50000                      

                       Computation: 114033 steps/s (collection: 0.714s, learning 0.148s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.86s
                        Total time: 241.81s
                               ETA: 672 mins 11.3 s

################################################################################
                      Learning iteration 298/50000                      

                       Computation: 122992 steps/s (collection: 0.676s, learning 0.123s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.80s
                        Total time: 242.61s
                               ETA: 672 mins 8.5 s

################################################################################
                      Learning iteration 299/50000                      

                       Computation: 121477 steps/s (collection: 0.684s, learning 0.125s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0109
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0005
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.81s
                        Total time: 243.42s
                               ETA: 672 mins 7.3 s

################################################################################
                      Learning iteration 300/50000                      

                       Computation: 131049 steps/s (collection: 0.626s, learning 0.124s)
               Value function loss: 0.0970
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.75s
                        Total time: 244.17s
                               ETA: 671 mins 56.4 s

################################################################################
                      Learning iteration 301/50000                      

                       Computation: 122179 steps/s (collection: 0.677s, learning 0.128s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0037
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.80s
                        Total time: 244.97s
                               ETA: 671 mins 54.5 s

################################################################################
                      Learning iteration 302/50000                      

                       Computation: 136832 steps/s (collection: 0.596s, learning 0.122s)
               Value function loss: 0.0909
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.72s
                        Total time: 245.69s
                               ETA: 671 mins 38.5 s

################################################################################
                      Learning iteration 303/50000                      

                       Computation: 123661 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 0.0845
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0036
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.79s
                        Total time: 246.49s
                               ETA: 671 mins 35.0 s

################################################################################
                      Learning iteration 304/50000                      

                       Computation: 133366 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.74s
                        Total time: 247.22s
                               ETA: 671 mins 22.2 s

################################################################################
                      Learning iteration 305/50000                      

                       Computation: 126329 steps/s (collection: 0.653s, learning 0.125s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.78s
                        Total time: 248.00s
                               ETA: 671 mins 16.1 s

################################################################################
                      Learning iteration 306/50000                      

                       Computation: 133117 steps/s (collection: 0.605s, learning 0.133s)
               Value function loss: 0.1127
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0057
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.74s
                        Total time: 248.74s
                               ETA: 671 mins 3.7 s

################################################################################
                      Learning iteration 307/50000                      

                       Computation: 127652 steps/s (collection: 0.632s, learning 0.138s)
               Value function loss: 0.0901
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.77s
                        Total time: 249.51s
                               ETA: 670 mins 56.4 s

################################################################################
                      Learning iteration 308/50000                      

                       Computation: 120991 steps/s (collection: 0.670s, learning 0.143s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.81s
                        Total time: 250.32s
                               ETA: 670 mins 56.0 s

################################################################################
                      Learning iteration 309/50000                      

                       Computation: 121547 steps/s (collection: 0.667s, learning 0.142s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.81s
                        Total time: 251.13s
                               ETA: 670 mins 54.9 s

################################################################################
                      Learning iteration 310/50000                      

                       Computation: 127746 steps/s (collection: 0.629s, learning 0.141s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.77s
                        Total time: 251.90s
                               ETA: 670 mins 47.6 s

################################################################################
                      Learning iteration 311/50000                      

                       Computation: 122069 steps/s (collection: 0.677s, learning 0.128s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0058
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.81s
                        Total time: 252.71s
                               ETA: 670 mins 46.1 s

################################################################################
                      Learning iteration 312/50000                      

                       Computation: 126671 steps/s (collection: 0.634s, learning 0.142s)
               Value function loss: 0.0915
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.55
                Mean reward (task): -5.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.78s
                        Total time: 253.48s
                               ETA: 670 mins 39.9 s

################################################################################
                      Learning iteration 313/50000                      

                       Computation: 121686 steps/s (collection: 0.683s, learning 0.125s)
               Value function loss: 0.0819
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.57
                Mean reward (task): -5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0011
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.81s
                        Total time: 254.29s
                               ETA: 670 mins 38.8 s

################################################################################
                      Learning iteration 314/50000                      

                       Computation: 124277 steps/s (collection: 0.648s, learning 0.143s)
               Value function loss: 0.0934
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.79s
                        Total time: 255.08s
                               ETA: 670 mins 35.0 s

################################################################################
                      Learning iteration 315/50000                      

                       Computation: 133074 steps/s (collection: 0.611s, learning 0.128s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0059
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.74s
                        Total time: 255.82s
                               ETA: 670 mins 23.0 s

################################################################################
                      Learning iteration 316/50000                      

                       Computation: 129049 steps/s (collection: 0.637s, learning 0.124s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.58
                Mean reward (task): -5.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0009
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.76s
                        Total time: 256.58s
                               ETA: 670 mins 14.7 s

################################################################################
                      Learning iteration 317/50000                      

                       Computation: 132246 steps/s (collection: 0.620s, learning 0.123s)
               Value function loss: 0.1101
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.60
                Mean reward (task): -5.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.74s
                        Total time: 257.33s
                               ETA: 670 mins 3.6 s

################################################################################
                      Learning iteration 318/50000                      

                       Computation: 118457 steps/s (collection: 0.684s, learning 0.146s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.58
                Mean reward (task): -5.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0108
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.83s
                        Total time: 258.16s
                               ETA: 670 mins 6.0 s

################################################################################
                      Learning iteration 319/50000                      

                       Computation: 126901 steps/s (collection: 0.634s, learning 0.141s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.77s
                        Total time: 258.93s
                               ETA: 669 mins 59.8 s

################################################################################
                      Learning iteration 320/50000                      

                       Computation: 132011 steps/s (collection: 0.602s, learning 0.142s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0271
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.74s
                        Total time: 259.68s
                               ETA: 669 mins 49.0 s

################################################################################
                      Learning iteration 321/50000                      

                       Computation: 121099 steps/s (collection: 0.674s, learning 0.138s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.81s
                        Total time: 260.49s
                               ETA: 669 mins 48.6 s

################################################################################
                      Learning iteration 322/50000                      

                       Computation: 131351 steps/s (collection: 0.624s, learning 0.125s)
               Value function loss: 0.1183
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.75s
                        Total time: 261.24s
                               ETA: 669 mins 38.5 s

################################################################################
                      Learning iteration 323/50000                      

                       Computation: 131918 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.75s
                        Total time: 261.98s
                               ETA: 669 mins 28.0 s

################################################################################
                      Learning iteration 324/50000                      

                       Computation: 129764 steps/s (collection: 0.631s, learning 0.127s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.76s
                        Total time: 262.74s
                               ETA: 669 mins 19.3 s

################################################################################
                      Learning iteration 325/50000                      

                       Computation: 120897 steps/s (collection: 0.691s, learning 0.122s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.81s
                        Total time: 263.55s
                               ETA: 669 mins 19.3 s

################################################################################
                      Learning iteration 326/50000                      

                       Computation: 129521 steps/s (collection: 0.627s, learning 0.132s)
               Value function loss: 0.1063
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.76s
                        Total time: 264.31s
                               ETA: 669 mins 10.9 s

################################################################################
                      Learning iteration 327/50000                      

                       Computation: 119638 steps/s (collection: 0.682s, learning 0.139s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0020
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0038
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.82s
                        Total time: 265.13s
                               ETA: 669 mins 12.1 s

################################################################################
                      Learning iteration 328/50000                      

                       Computation: 105933 steps/s (collection: 0.770s, learning 0.158s)
               Value function loss: 0.0859
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.58
                Mean reward (task): -5.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.93s
                        Total time: 266.06s
                               ETA: 669 mins 29.4 s

################################################################################
                      Learning iteration 329/50000                      

                       Computation: 116862 steps/s (collection: 0.684s, learning 0.157s)
               Value function loss: 0.0977
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.84s
                        Total time: 266.90s
                               ETA: 669 mins 33.5 s

################################################################################
                      Learning iteration 330/50000                      

                       Computation: 123384 steps/s (collection: 0.657s, learning 0.140s)
               Value function loss: 0.1094
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.80
                Mean reward (task): -5.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0276
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.80s
                        Total time: 267.70s
                               ETA: 669 mins 30.9 s

################################################################################
                      Learning iteration 331/50000                      

                       Computation: 129250 steps/s (collection: 0.623s, learning 0.138s)
               Value function loss: 0.0953
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.68
                Mean reward (task): -5.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.76s
                        Total time: 268.46s
                               ETA: 669 mins 22.9 s

################################################################################
                      Learning iteration 332/50000                      

                       Computation: 120730 steps/s (collection: 0.663s, learning 0.151s)
               Value function loss: 0.0893
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0274
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.81s
                        Total time: 269.27s
                               ETA: 669 mins 22.9 s

################################################################################
                      Learning iteration 333/50000                      

                       Computation: 132785 steps/s (collection: 0.618s, learning 0.123s)
               Value function loss: 0.1815
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.93
                Mean reward (task): -5.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 0.74s
                        Total time: 270.01s
                               ETA: 669 mins 11.9 s

################################################################################
                      Learning iteration 334/50000                      

                       Computation: 114683 steps/s (collection: 0.734s, learning 0.124s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0273
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 0.86s
                        Total time: 270.87s
                               ETA: 669 mins 18.3 s

################################################################################
                      Learning iteration 335/50000                      

                       Computation: 132663 steps/s (collection: 0.617s, learning 0.124s)
               Value function loss: 0.1099
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.59
                Mean reward (task): -5.59
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 0.74s
                        Total time: 271.61s
                               ETA: 669 mins 7.5 s

################################################################################
                      Learning iteration 336/50000                      

                       Computation: 131690 steps/s (collection: 0.624s, learning 0.123s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0039
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 0.75s
                        Total time: 272.36s
                               ETA: 668 mins 57.6 s

################################################################################
                      Learning iteration 337/50000                      

                       Computation: 131658 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0847
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 0.75s
                        Total time: 273.10s
                               ETA: 668 mins 47.8 s

################################################################################
                      Learning iteration 338/50000                      

                       Computation: 110026 steps/s (collection: 0.749s, learning 0.144s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0060
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 0.89s
                        Total time: 274.00s
                               ETA: 668 mins 59.5 s

################################################################################
                      Learning iteration 339/50000                      

                       Computation: 120140 steps/s (collection: 0.660s, learning 0.158s)
               Value function loss: 0.0889
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.60
                Mean reward (task): -5.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 0.82s
                        Total time: 274.82s
                               ETA: 669 mins 0.1 s

################################################################################
                      Learning iteration 340/50000                      

                       Computation: 127102 steps/s (collection: 0.635s, learning 0.138s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 0.77s
                        Total time: 275.59s
                               ETA: 668 mins 54.2 s

################################################################################
                      Learning iteration 341/50000                      

                       Computation: 112501 steps/s (collection: 0.728s, learning 0.146s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 0.87s
                        Total time: 276.46s
                               ETA: 669 mins 3.0 s

################################################################################
                      Learning iteration 342/50000                      

                       Computation: 131130 steps/s (collection: 0.606s, learning 0.144s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.75s
                        Total time: 277.21s
                               ETA: 668 mins 53.7 s

################################################################################
                      Learning iteration 343/50000                      

                       Computation: 114764 steps/s (collection: 0.714s, learning 0.143s)
               Value function loss: 0.1418
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.86s
                        Total time: 278.07s
                               ETA: 668 mins 59.8 s

################################################################################
                      Learning iteration 344/50000                      

                       Computation: 123533 steps/s (collection: 0.651s, learning 0.145s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.80s
                        Total time: 278.87s
                               ETA: 668 mins 57.2 s

################################################################################
                      Learning iteration 345/50000                      

                       Computation: 131859 steps/s (collection: 0.609s, learning 0.136s)
               Value function loss: 0.0923
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.75s
                        Total time: 279.61s
                               ETA: 668 mins 47.4 s

################################################################################
                      Learning iteration 346/50000                      

                       Computation: 113872 steps/s (collection: 0.726s, learning 0.138s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0040
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.86s
                        Total time: 280.47s
                               ETA: 668 mins 54.5 s

################################################################################
                      Learning iteration 347/50000                      

                       Computation: 131660 steps/s (collection: 0.623s, learning 0.123s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0029
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.75s
                        Total time: 281.22s
                               ETA: 668 mins 44.9 s

################################################################################
                      Learning iteration 348/50000                      

                       Computation: 132621 steps/s (collection: 0.619s, learning 0.123s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.74s
                        Total time: 281.96s
                               ETA: 668 mins 34.5 s

################################################################################
                      Learning iteration 349/50000                      

                       Computation: 124289 steps/s (collection: 0.667s, learning 0.124s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.79s
                        Total time: 282.75s
                               ETA: 668 mins 31.3 s

################################################################################
                      Learning iteration 350/50000                      

                       Computation: 120394 steps/s (collection: 0.676s, learning 0.141s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.82s
                        Total time: 283.57s
                               ETA: 668 mins 31.7 s

################################################################################
                      Learning iteration 351/50000                      

                       Computation: 119431 steps/s (collection: 0.663s, learning 0.160s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.82s
                        Total time: 284.39s
                               ETA: 668 mins 33.1 s

################################################################################
                      Learning iteration 352/50000                      

                       Computation: 130645 steps/s (collection: 0.611s, learning 0.141s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.75s
                        Total time: 285.14s
                               ETA: 668 mins 24.5 s

################################################################################
                      Learning iteration 353/50000                      

                       Computation: 124103 steps/s (collection: 0.652s, learning 0.140s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.79s
                        Total time: 285.94s
                               ETA: 668 mins 21.5 s

################################################################################
                      Learning iteration 354/50000                      

                       Computation: 118130 steps/s (collection: 0.698s, learning 0.134s)
               Value function loss: 0.0657
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.55
                Mean reward (task): -5.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.83s
                        Total time: 286.77s
                               ETA: 668 mins 24.1 s

################################################################################
                      Learning iteration 355/50000                      

                       Computation: 125399 steps/s (collection: 0.659s, learning 0.125s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.69
                Mean reward (task): -5.69
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.78s
                        Total time: 287.55s
                               ETA: 668 mins 19.9 s

################################################################################
                      Learning iteration 356/50000                      

                       Computation: 126929 steps/s (collection: 0.650s, learning 0.124s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0107
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.77s
                        Total time: 288.33s
                               ETA: 668 mins 14.5 s

################################################################################
                      Learning iteration 357/50000                      

                       Computation: 126543 steps/s (collection: 0.653s, learning 0.124s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.78s
                        Total time: 289.10s
                               ETA: 668 mins 9.4 s

################################################################################
                      Learning iteration 358/50000                      

                       Computation: 119934 steps/s (collection: 0.686s, learning 0.134s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0061
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0041
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.82s
                        Total time: 289.92s
                               ETA: 668 mins 10.3 s

################################################################################
                      Learning iteration 359/50000                      

                       Computation: 130254 steps/s (collection: 0.628s, learning 0.127s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.75s
                        Total time: 290.68s
                               ETA: 668 mins 2.2 s

################################################################################
                      Learning iteration 360/50000                      

                       Computation: 135398 steps/s (collection: 0.603s, learning 0.123s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.73s
                        Total time: 291.40s
                               ETA: 667 mins 50.2 s

################################################################################
                      Learning iteration 361/50000                      

                       Computation: 125300 steps/s (collection: 0.640s, learning 0.145s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.78s
                        Total time: 292.19s
                               ETA: 667 mins 46.3 s

################################################################################
                      Learning iteration 362/50000                      

                       Computation: 133623 steps/s (collection: 0.613s, learning 0.123s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0030
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.74s
                        Total time: 292.93s
                               ETA: 667 mins 35.7 s

################################################################################
                      Learning iteration 363/50000                      

                       Computation: 117785 steps/s (collection: 0.706s, learning 0.129s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.83s
                        Total time: 293.76s
                               ETA: 667 mins 38.7 s

################################################################################
                      Learning iteration 364/50000                      

                       Computation: 116727 steps/s (collection: 0.690s, learning 0.152s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.84s
                        Total time: 294.60s
                               ETA: 667 mins 42.6 s

################################################################################
                      Learning iteration 365/50000                      

                       Computation: 130143 steps/s (collection: 0.631s, learning 0.124s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0271
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.76s
                        Total time: 295.36s
                               ETA: 667 mins 34.8 s

################################################################################
                      Learning iteration 366/50000                      

                       Computation: 126061 steps/s (collection: 0.656s, learning 0.124s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.78s
                        Total time: 296.14s
                               ETA: 667 mins 30.3 s

################################################################################
                      Learning iteration 367/50000                      

                       Computation: 131026 steps/s (collection: 0.626s, learning 0.124s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.75s
                        Total time: 296.89s
                               ETA: 667 mins 21.9 s

################################################################################
                      Learning iteration 368/50000                      

                       Computation: 134078 steps/s (collection: 0.606s, learning 0.127s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.73s
                        Total time: 297.62s
                               ETA: 667 mins 11.2 s

################################################################################
                      Learning iteration 369/50000                      

                       Computation: 123337 steps/s (collection: 0.675s, learning 0.122s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.80s
                        Total time: 298.42s
                               ETA: 667 mins 9.1 s

################################################################################
                      Learning iteration 370/50000                      

                       Computation: 128809 steps/s (collection: 0.641s, learning 0.122s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.76s
                        Total time: 299.18s
                               ETA: 667 mins 2.5 s

################################################################################
                      Learning iteration 371/50000                      

                       Computation: 127729 steps/s (collection: 0.644s, learning 0.125s)
               Value function loss: 0.0873
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.77s
                        Total time: 299.95s
                               ETA: 666 mins 56.8 s

################################################################################
                      Learning iteration 372/50000                      

                       Computation: 120758 steps/s (collection: 0.675s, learning 0.139s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.81s
                        Total time: 300.76s
                               ETA: 666 mins 57.0 s

################################################################################
                      Learning iteration 373/50000                      

                       Computation: 116155 steps/s (collection: 0.700s, learning 0.146s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0106
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.85s
                        Total time: 301.61s
                               ETA: 667 mins 1.5 s

################################################################################
                      Learning iteration 374/50000                      

                       Computation: 124371 steps/s (collection: 0.650s, learning 0.141s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0021
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.79s
                        Total time: 302.40s
                               ETA: 666 mins 58.6 s

################################################################################
                      Learning iteration 375/50000                      

                       Computation: 123898 steps/s (collection: 0.633s, learning 0.160s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0042
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.79s
                        Total time: 303.19s
                               ETA: 666 mins 56.0 s

################################################################################
                      Learning iteration 376/50000                      

                       Computation: 133145 steps/s (collection: 0.613s, learning 0.125s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.74s
                        Total time: 303.93s
                               ETA: 666 mins 46.3 s

################################################################################
                      Learning iteration 377/50000                      

                       Computation: 125876 steps/s (collection: 0.652s, learning 0.129s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.78s
                        Total time: 304.71s
                               ETA: 666 mins 42.2 s

################################################################################
                      Learning iteration 378/50000                      

                       Computation: 132368 steps/s (collection: 0.618s, learning 0.125s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.74s
                        Total time: 305.46s
                               ETA: 666 mins 33.0 s

################################################################################
                      Learning iteration 379/50000                      

                       Computation: 134384 steps/s (collection: 0.607s, learning 0.124s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.73s
                        Total time: 306.19s
                               ETA: 666 mins 22.5 s

################################################################################
                      Learning iteration 380/50000                      

                       Computation: 115995 steps/s (collection: 0.718s, learning 0.129s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.62
                Mean reward (task): -5.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.85s
                        Total time: 307.04s
                               ETA: 666 mins 27.1 s

################################################################################
                      Learning iteration 381/50000                      

                       Computation: 116161 steps/s (collection: 0.709s, learning 0.137s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.85s
                        Total time: 307.88s
                               ETA: 666 mins 31.6 s

################################################################################
                      Learning iteration 382/50000                      

                       Computation: 121301 steps/s (collection: 0.672s, learning 0.138s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.81s
                        Total time: 308.69s
                               ETA: 666 mins 31.3 s

################################################################################
                      Learning iteration 383/50000                      

                       Computation: 128764 steps/s (collection: 0.622s, learning 0.141s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.76s
                        Total time: 309.46s
                               ETA: 666 mins 25.0 s

################################################################################
                      Learning iteration 384/50000                      

                       Computation: 117416 steps/s (collection: 0.698s, learning 0.139s)
               Value function loss: 0.0934
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.84s
                        Total time: 310.29s
                               ETA: 666 mins 28.3 s

################################################################################
                      Learning iteration 385/50000                      

                       Computation: 122409 steps/s (collection: 0.666s, learning 0.137s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.80s
                        Total time: 311.10s
                               ETA: 666 mins 27.1 s

################################################################################
                      Learning iteration 386/50000                      

                       Computation: 131868 steps/s (collection: 0.621s, learning 0.124s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.75s
                        Total time: 311.84s
                               ETA: 666 mins 18.5 s

################################################################################
                      Learning iteration 387/50000                      

                       Computation: 124446 steps/s (collection: 0.668s, learning 0.122s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0271
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.79s
                        Total time: 312.63s
                               ETA: 666 mins 15.7 s

################################################################################
                      Learning iteration 388/50000                      

                       Computation: 131503 steps/s (collection: 0.624s, learning 0.123s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.75s
                        Total time: 313.38s
                               ETA: 666 mins 7.5 s

################################################################################
                      Learning iteration 389/50000                      

                       Computation: 117412 steps/s (collection: 0.713s, learning 0.124s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.84s
                        Total time: 314.22s
                               ETA: 666 mins 10.7 s

################################################################################
                      Learning iteration 390/50000                      

                       Computation: 120673 steps/s (collection: 0.692s, learning 0.123s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.81s
                        Total time: 315.03s
                               ETA: 666 mins 11.0 s

################################################################################
                      Learning iteration 391/50000                      

                       Computation: 135164 steps/s (collection: 0.603s, learning 0.124s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.73s
                        Total time: 315.76s
                               ETA: 666 mins 0.3 s

################################################################################
                      Learning iteration 392/50000                      

                       Computation: 125244 steps/s (collection: 0.662s, learning 0.123s)
               Value function loss: 0.0847
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.78s
                        Total time: 316.54s
                               ETA: 665 mins 56.9 s

################################################################################
                      Learning iteration 393/50000                      

                       Computation: 128279 steps/s (collection: 0.641s, learning 0.125s)
               Value function loss: 0.0889
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.77s
                        Total time: 317.31s
                               ETA: 665 mins 51.2 s

################################################################################
                      Learning iteration 394/50000                      

                       Computation: 134626 steps/s (collection: 0.599s, learning 0.131s)
               Value function loss: 0.1011
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0047
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.73s
                        Total time: 318.04s
                               ETA: 665 mins 40.9 s

################################################################################
                      Learning iteration 395/50000                      

                       Computation: 121804 steps/s (collection: 0.670s, learning 0.137s)
               Value function loss: 0.0839
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0047
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.81s
                        Total time: 318.85s
                               ETA: 665 mins 40.4 s

################################################################################
                      Learning iteration 396/50000                      

                       Computation: 129339 steps/s (collection: 0.637s, learning 0.123s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.76s
                        Total time: 319.61s
                               ETA: 665 mins 33.9 s

################################################################################
                      Learning iteration 397/50000                      

                       Computation: 133140 steps/s (collection: 0.612s, learning 0.126s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0043
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.74s
                        Total time: 320.34s
                               ETA: 665 mins 24.8 s

################################################################################
                      Learning iteration 398/50000                      

                       Computation: 124550 steps/s (collection: 0.667s, learning 0.122s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.79s
                        Total time: 321.13s
                               ETA: 665 mins 22.0 s

################################################################################
                      Learning iteration 399/50000                      

                       Computation: 128857 steps/s (collection: 0.633s, learning 0.130s)
               Value function loss: 0.1753
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.76s
                        Total time: 321.90s
                               ETA: 665 mins 16.0 s

################################################################################
                      Learning iteration 400/50000                      

                       Computation: 125533 steps/s (collection: 0.651s, learning 0.132s)
               Value function loss: 0.0793
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.78s
                        Total time: 322.68s
                               ETA: 665 mins 12.6 s

################################################################################
                      Learning iteration 401/50000                      

                       Computation: 113617 steps/s (collection: 0.728s, learning 0.137s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.63
                Mean reward (task): -5.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.87s
                        Total time: 323.55s
                               ETA: 665 mins 19.2 s

################################################################################
                      Learning iteration 402/50000                      

                       Computation: 125336 steps/s (collection: 0.644s, learning 0.140s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.78s
                        Total time: 324.33s
                               ETA: 665 mins 15.9 s

################################################################################
                      Learning iteration 403/50000                      

                       Computation: 120397 steps/s (collection: 0.684s, learning 0.132s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0044
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.82s
                        Total time: 325.15s
                               ETA: 665 mins 16.5 s

################################################################################
                      Learning iteration 404/50000                      

                       Computation: 130166 steps/s (collection: 0.631s, learning 0.124s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.76s
                        Total time: 325.90s
                               ETA: 665 mins 9.6 s

################################################################################
                      Learning iteration 405/50000                      

                       Computation: 127554 steps/s (collection: 0.612s, learning 0.159s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.77s
                        Total time: 326.67s
                               ETA: 665 mins 4.7 s

################################################################################
                      Learning iteration 406/50000                      

                       Computation: 136099 steps/s (collection: 0.596s, learning 0.126s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.60
                Mean reward (task): -5.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.72s
                        Total time: 327.39s
                               ETA: 664 mins 53.8 s

################################################################################
                      Learning iteration 407/50000                      

                       Computation: 123606 steps/s (collection: 0.671s, learning 0.124s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.80s
                        Total time: 328.19s
                               ETA: 664 mins 51.9 s

################################################################################
                      Learning iteration 408/50000                      

                       Computation: 120172 steps/s (collection: 0.694s, learning 0.124s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.82s
                        Total time: 329.01s
                               ETA: 664 mins 52.8 s

################################################################################
                      Learning iteration 409/50000                      

                       Computation: 116902 steps/s (collection: 0.701s, learning 0.140s)
               Value function loss: 0.1588
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0047
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.84s
                        Total time: 329.85s
                               ETA: 664 mins 56.4 s

################################################################################
                      Learning iteration 410/50000                      

                       Computation: 114182 steps/s (collection: 0.711s, learning 0.150s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.86s
                        Total time: 330.71s
                               ETA: 665 mins 2.4 s

################################################################################
                      Learning iteration 411/50000                      

                       Computation: 124473 steps/s (collection: 0.668s, learning 0.122s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.79s
                        Total time: 331.50s
                               ETA: 664 mins 59.8 s

################################################################################
                      Learning iteration 412/50000                      

                       Computation: 119897 steps/s (collection: 0.693s, learning 0.127s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.82s
                        Total time: 332.32s
                               ETA: 665 mins 0.8 s

################################################################################
                      Learning iteration 413/50000                      

                       Computation: 132853 steps/s (collection: 0.615s, learning 0.125s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.74s
                        Total time: 333.06s
                               ETA: 664 mins 52.3 s

################################################################################
                      Learning iteration 414/50000                      

                       Computation: 122772 steps/s (collection: 0.663s, learning 0.138s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.80s
                        Total time: 333.86s
                               ETA: 664 mins 51.0 s

################################################################################
                      Learning iteration 415/50000                      

                       Computation: 129842 steps/s (collection: 0.615s, learning 0.142s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.76s
                        Total time: 334.62s
                               ETA: 664 mins 44.6 s

################################################################################
                      Learning iteration 416/50000                      

                       Computation: 125488 steps/s (collection: 0.638s, learning 0.146s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.61
                Mean reward (task): -5.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0046
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.78s
                        Total time: 335.40s
                               ETA: 664 mins 41.3 s

################################################################################
                      Learning iteration 417/50000                      

                       Computation: 122739 steps/s (collection: 0.644s, learning 0.157s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.80s
                        Total time: 336.20s
                               ETA: 664 mins 40.1 s

################################################################################
                      Learning iteration 418/50000                      

                       Computation: 131605 steps/s (collection: 0.609s, learning 0.138s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.75s
                        Total time: 336.95s
                               ETA: 664 mins 32.5 s

################################################################################
                      Learning iteration 419/50000                      

                       Computation: 127830 steps/s (collection: 0.645s, learning 0.124s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.72
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0045
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.77s
                        Total time: 337.72s
                               ETA: 664 mins 27.5 s

################################################################################
                      Learning iteration 420/50000                      

                       Computation: 134755 steps/s (collection: 0.605s, learning 0.124s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.67
                Mean reward (task): -5.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0105
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.73s
                        Total time: 338.45s
                               ETA: 664 mins 17.9 s

################################################################################
                      Learning iteration 421/50000                      

                       Computation: 133859 steps/s (collection: 0.611s, learning 0.124s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.73s
                        Total time: 339.18s
                               ETA: 664 mins 9.0 s

################################################################################
                      Learning iteration 422/50000                      

                       Computation: 129822 steps/s (collection: 0.627s, learning 0.131s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.76s
                        Total time: 339.94s
                               ETA: 664 mins 2.7 s

################################################################################
                      Learning iteration 423/50000                      

                       Computation: 130389 steps/s (collection: 0.624s, learning 0.130s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.75s
                        Total time: 340.69s
                               ETA: 663 mins 56.1 s

################################################################################
                      Learning iteration 424/50000                      

                       Computation: 123583 steps/s (collection: 0.673s, learning 0.122s)
               Value function loss: 0.0795
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0271
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.80s
                        Total time: 341.49s
                               ETA: 663 mins 54.3 s

################################################################################
                      Learning iteration 425/50000                      

                       Computation: 127710 steps/s (collection: 0.634s, learning 0.135s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.77s
                        Total time: 342.26s
                               ETA: 663 mins 49.6 s

################################################################################
                      Learning iteration 426/50000                      

                       Computation: 129649 steps/s (collection: 0.627s, learning 0.131s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.76s
                        Total time: 343.02s
                               ETA: 663 mins 43.6 s

################################################################################
                      Learning iteration 427/50000                      

                       Computation: 129037 steps/s (collection: 0.616s, learning 0.146s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.76s
                        Total time: 343.78s
                               ETA: 663 mins 37.9 s

################################################################################
                      Learning iteration 428/50000                      

                       Computation: 132743 steps/s (collection: 0.617s, learning 0.124s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.74s
                        Total time: 344.52s
                               ETA: 663 mins 29.9 s

################################################################################
                      Learning iteration 429/50000                      

                       Computation: 125096 steps/s (collection: 0.658s, learning 0.128s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.79s
                        Total time: 345.30s
                               ETA: 663 mins 27.1 s

################################################################################
                      Learning iteration 430/50000                      

                       Computation: 134771 steps/s (collection: 0.599s, learning 0.130s)
               Value function loss: 0.0651
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.73s
                        Total time: 346.03s
                               ETA: 663 mins 17.8 s

################################################################################
                      Learning iteration 431/50000                      

                       Computation: 124504 steps/s (collection: 0.650s, learning 0.139s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.79s
                        Total time: 346.82s
                               ETA: 663 mins 15.5 s

################################################################################
                      Learning iteration 432/50000                      

                       Computation: 130688 steps/s (collection: 0.628s, learning 0.125s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0104
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.75s
                        Total time: 347.57s
                               ETA: 663 mins 8.9 s

################################################################################
                      Learning iteration 433/50000                      

                       Computation: 123051 steps/s (collection: 0.673s, learning 0.126s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0272
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.80s
                        Total time: 348.37s
                               ETA: 663 mins 7.7 s

################################################################################
                      Learning iteration 434/50000                      

                       Computation: 129446 steps/s (collection: 0.621s, learning 0.139s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.76s
                        Total time: 349.13s
                               ETA: 663 mins 1.9 s

################################################################################
                      Learning iteration 435/50000                      

                       Computation: 126202 steps/s (collection: 0.655s, learning 0.123s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 22.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.78s
                        Total time: 349.91s
                               ETA: 662 mins 58.4 s

################################################################################
                      Learning iteration 436/50000                      

                       Computation: 129073 steps/s (collection: 0.637s, learning 0.124s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.76s
                        Total time: 350.67s
                               ETA: 662 mins 53.0 s

################################################################################
                      Learning iteration 437/50000                      

                       Computation: 126407 steps/s (collection: 0.644s, learning 0.133s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.78s
                        Total time: 351.45s
                               ETA: 662 mins 49.4 s

################################################################################
                      Learning iteration 438/50000                      

                       Computation: 131682 steps/s (collection: 0.620s, learning 0.126s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.75s
                        Total time: 352.20s
                               ETA: 662 mins 42.3 s

################################################################################
                      Learning iteration 439/50000                      

                       Computation: 128445 steps/s (collection: 0.643s, learning 0.122s)
               Value function loss: 0.0877
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.77s
                        Total time: 352.96s
                               ETA: 662 mins 37.3 s

################################################################################
                      Learning iteration 440/50000                      

                       Computation: 123432 steps/s (collection: 0.651s, learning 0.146s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.80s
                        Total time: 353.76s
                               ETA: 662 mins 35.9 s

################################################################################
                      Learning iteration 441/50000                      

                       Computation: 123807 steps/s (collection: 0.669s, learning 0.125s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.79s
                        Total time: 354.55s
                               ETA: 662 mins 34.1 s

################################################################################
                      Learning iteration 442/50000                      

                       Computation: 125854 steps/s (collection: 0.639s, learning 0.142s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.78s
                        Total time: 355.33s
                               ETA: 662 mins 31.0 s

################################################################################
                      Learning iteration 443/50000                      

                       Computation: 120570 steps/s (collection: 0.652s, learning 0.163s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.82s
                        Total time: 356.15s
                               ETA: 662 mins 31.7 s

################################################################################
                      Learning iteration 444/50000                      

                       Computation: 116508 steps/s (collection: 0.701s, learning 0.143s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.84s
                        Total time: 356.99s
                               ETA: 662 mins 35.5 s

################################################################################
                      Learning iteration 445/50000                      

                       Computation: 132552 steps/s (collection: 0.602s, learning 0.140s)
               Value function loss: 0.0995
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.74s
                        Total time: 357.74s
                               ETA: 662 mins 28.0 s

################################################################################
                      Learning iteration 446/50000                      

                       Computation: 129783 steps/s (collection: 0.617s, learning 0.141s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.76s
                        Total time: 358.49s
                               ETA: 662 mins 22.2 s

################################################################################
                      Learning iteration 447/50000                      

                       Computation: 114105 steps/s (collection: 0.703s, learning 0.159s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.86s
                        Total time: 359.35s
                               ETA: 662 mins 28.0 s

################################################################################
                      Learning iteration 448/50000                      

                       Computation: 126243 steps/s (collection: 0.644s, learning 0.135s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.78s
                        Total time: 360.13s
                               ETA: 662 mins 24.6 s

################################################################################
                      Learning iteration 449/50000                      

                       Computation: 131119 steps/s (collection: 0.604s, learning 0.146s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.75s
                        Total time: 360.88s
                               ETA: 662 mins 18.0 s

################################################################################
                      Learning iteration 450/50000                      

                       Computation: 131319 steps/s (collection: 0.625s, learning 0.124s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.53
                Mean reward (task): -5.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.75s
                        Total time: 361.63s
                               ETA: 662 mins 11.4 s

################################################################################
                      Learning iteration 451/50000                      

                       Computation: 136295 steps/s (collection: 0.597s, learning 0.124s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.72s
                        Total time: 362.35s
                               ETA: 662 mins 1.7 s

################################################################################
                      Learning iteration 452/50000                      

                       Computation: 128025 steps/s (collection: 0.641s, learning 0.127s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.63
                Mean reward (task): -5.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0047
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.77s
                        Total time: 363.12s
                               ETA: 661 mins 57.2 s

################################################################################
                      Learning iteration 453/50000                      

                       Computation: 133104 steps/s (collection: 0.614s, learning 0.125s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.74s
                        Total time: 363.86s
                               ETA: 661 mins 49.5 s

################################################################################
                      Learning iteration 454/50000                      

                       Computation: 120564 steps/s (collection: 0.681s, learning 0.134s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.82s
                        Total time: 364.67s
                               ETA: 661 mins 50.3 s

################################################################################
                      Learning iteration 455/50000                      

                       Computation: 125337 steps/s (collection: 0.644s, learning 0.140s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.78s
                        Total time: 365.46s
                               ETA: 661 mins 47.6 s

################################################################################
                      Learning iteration 456/50000                      

                       Computation: 117822 steps/s (collection: 0.712s, learning 0.122s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.83s
                        Total time: 366.29s
                               ETA: 661 mins 50.4 s

################################################################################
                      Learning iteration 457/50000                      

                       Computation: 128386 steps/s (collection: 0.639s, learning 0.127s)
               Value function loss: 0.0798
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.53
                Mean reward (task): -5.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.77s
                        Total time: 367.06s
                               ETA: 661 mins 45.7 s

################################################################################
                      Learning iteration 458/50000                      

                       Computation: 127295 steps/s (collection: 0.648s, learning 0.124s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.77s
                        Total time: 367.83s
                               ETA: 661 mins 41.7 s

################################################################################
                      Learning iteration 459/50000                      

                       Computation: 116767 steps/s (collection: 0.712s, learning 0.130s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.84s
                        Total time: 368.67s
                               ETA: 661 mins 45.3 s

################################################################################
                      Learning iteration 460/50000                      

                       Computation: 127303 steps/s (collection: 0.647s, learning 0.125s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0270
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.77s
                        Total time: 369.45s
                               ETA: 661 mins 41.3 s

################################################################################
                      Learning iteration 461/50000                      

                       Computation: 118816 steps/s (collection: 0.689s, learning 0.138s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.83s
                        Total time: 370.27s
                               ETA: 661 mins 43.3 s

################################################################################
                      Learning iteration 462/50000                      

                       Computation: 119000 steps/s (collection: 0.697s, learning 0.129s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.83s
                        Total time: 371.10s
                               ETA: 661 mins 45.2 s

################################################################################
                      Learning iteration 463/50000                      

                       Computation: 132568 steps/s (collection: 0.618s, learning 0.123s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.74s
                        Total time: 371.84s
                               ETA: 661 mins 38.0 s

################################################################################
                      Learning iteration 464/50000                      

                       Computation: 121939 steps/s (collection: 0.674s, learning 0.132s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0048
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.81s
                        Total time: 372.65s
                               ETA: 661 mins 37.7 s

################################################################################
                      Learning iteration 465/50000                      

                       Computation: 123928 steps/s (collection: 0.665s, learning 0.128s)
               Value function loss: 0.0945
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.79s
                        Total time: 373.44s
                               ETA: 661 mins 36.0 s

################################################################################
                      Learning iteration 466/50000                      

                       Computation: 121553 steps/s (collection: 0.682s, learning 0.126s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.81s
                        Total time: 374.25s
                               ETA: 661 mins 36.0 s

################################################################################
                      Learning iteration 467/50000                      

                       Computation: 132199 steps/s (collection: 0.617s, learning 0.127s)
               Value function loss: 0.0848
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.74s
                        Total time: 374.99s
                               ETA: 661 mins 29.1 s

################################################################################
                      Learning iteration 468/50000                      

                       Computation: 136947 steps/s (collection: 0.594s, learning 0.124s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.72s
                        Total time: 375.71s
                               ETA: 661 mins 19.4 s

################################################################################
                      Learning iteration 469/50000                      

                       Computation: 131439 steps/s (collection: 0.624s, learning 0.124s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0271
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.75s
                        Total time: 376.46s
                               ETA: 661 mins 13.0 s

################################################################################
                      Learning iteration 470/50000                      

                       Computation: 130589 steps/s (collection: 0.629s, learning 0.124s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.75s
                        Total time: 377.21s
                               ETA: 661 mins 7.2 s

################################################################################
                      Learning iteration 471/50000                      

                       Computation: 125633 steps/s (collection: 0.647s, learning 0.135s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.63
                Mean reward (task): -5.63
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0103
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.78s
                        Total time: 377.99s
                               ETA: 661 mins 4.4 s

################################################################################
                      Learning iteration 472/50000                      

                       Computation: 132645 steps/s (collection: 0.619s, learning 0.122s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.74s
                        Total time: 378.73s
                               ETA: 660 mins 57.4 s

################################################################################
                      Learning iteration 473/50000                      

                       Computation: 134318 steps/s (collection: 0.607s, learning 0.124s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.73s
                        Total time: 379.47s
                               ETA: 660 mins 49.4 s

################################################################################
                      Learning iteration 474/50000                      

                       Computation: 123723 steps/s (collection: 0.659s, learning 0.136s)
               Value function loss: 0.0857
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0048
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.79s
                        Total time: 380.26s
                               ETA: 660 mins 48.0 s

################################################################################
                      Learning iteration 475/50000                      

                       Computation: 126147 steps/s (collection: 0.637s, learning 0.142s)
               Value function loss: 0.1050
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.78s
                        Total time: 381.04s
                               ETA: 660 mins 44.9 s

################################################################################
                      Learning iteration 476/50000                      

                       Computation: 120829 steps/s (collection: 0.671s, learning 0.142s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0031
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.81s
                        Total time: 381.85s
                               ETA: 660 mins 45.5 s

################################################################################
                      Learning iteration 477/50000                      

                       Computation: 116662 steps/s (collection: 0.709s, learning 0.133s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.84s
                        Total time: 382.70s
                               ETA: 660 mins 49.1 s

################################################################################
                      Learning iteration 478/50000                      

                       Computation: 120828 steps/s (collection: 0.675s, learning 0.139s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.81s
                        Total time: 383.51s
                               ETA: 660 mins 49.6 s

################################################################################
                      Learning iteration 479/50000                      

                       Computation: 131582 steps/s (collection: 0.610s, learning 0.137s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.75s
                        Total time: 384.26s
                               ETA: 660 mins 43.3 s

################################################################################
                      Learning iteration 480/50000                      

                       Computation: 128100 steps/s (collection: 0.641s, learning 0.126s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0049
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.77s
                        Total time: 385.02s
                               ETA: 660 mins 39.1 s

################################################################################
                      Learning iteration 481/50000                      

                       Computation: 121537 steps/s (collection: 0.684s, learning 0.125s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.81s
                        Total time: 385.83s
                               ETA: 660 mins 39.1 s

################################################################################
                      Learning iteration 482/50000                      

                       Computation: 122633 steps/s (collection: 0.678s, learning 0.124s)
               Value function loss: 0.0785
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.61
                Mean reward (task): -5.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.80s
                        Total time: 386.63s
                               ETA: 660 mins 38.4 s

################################################################################
                      Learning iteration 483/50000                      

                       Computation: 133859 steps/s (collection: 0.610s, learning 0.124s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.73s
                        Total time: 387.37s
                               ETA: 660 mins 30.9 s

################################################################################
                      Learning iteration 484/50000                      

                       Computation: 121481 steps/s (collection: 0.682s, learning 0.128s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.81s
                        Total time: 388.18s
                               ETA: 660 mins 31.0 s

################################################################################
                      Learning iteration 485/50000                      

                       Computation: 128028 steps/s (collection: 0.641s, learning 0.127s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.77s
                        Total time: 388.95s
                               ETA: 660 mins 26.9 s

################################################################################
                      Learning iteration 486/50000                      

                       Computation: 134652 steps/s (collection: 0.606s, learning 0.124s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.73s
                        Total time: 389.68s
                               ETA: 660 mins 18.9 s

################################################################################
                      Learning iteration 487/50000                      

                       Computation: 120273 steps/s (collection: 0.691s, learning 0.126s)
               Value function loss: 0.0793
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.82s
                        Total time: 390.49s
                               ETA: 660 mins 19.9 s

################################################################################
                      Learning iteration 488/50000                      

                       Computation: 131491 steps/s (collection: 0.622s, learning 0.125s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0021
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.75s
                        Total time: 391.24s
                               ETA: 660 mins 13.7 s

################################################################################
                      Learning iteration 489/50000                      

                       Computation: 132935 steps/s (collection: 0.616s, learning 0.124s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.74s
                        Total time: 391.98s
                               ETA: 660 mins 6.8 s

################################################################################
                      Learning iteration 490/50000                      

                       Computation: 123590 steps/s (collection: 0.672s, learning 0.123s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.61
                Mean reward (task): -5.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.80s
                        Total time: 392.78s
                               ETA: 660 mins 5.6 s

################################################################################
                      Learning iteration 491/50000                      

                       Computation: 134507 steps/s (collection: 0.607s, learning 0.124s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.73s
                        Total time: 393.51s
                               ETA: 659 mins 57.8 s

################################################################################
                      Learning iteration 492/50000                      

                       Computation: 127537 steps/s (collection: 0.632s, learning 0.139s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.77s
                        Total time: 394.28s
                               ETA: 659 mins 54.1 s

################################################################################
                      Learning iteration 493/50000                      

                       Computation: 119304 steps/s (collection: 0.685s, learning 0.139s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.82s
                        Total time: 395.10s
                               ETA: 659 mins 55.7 s

################################################################################
                      Learning iteration 494/50000                      

                       Computation: 123074 steps/s (collection: 0.657s, learning 0.142s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0252
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.80s
                        Total time: 395.90s
                               ETA: 659 mins 54.8 s

################################################################################
                      Learning iteration 495/50000                      

                       Computation: 123670 steps/s (collection: 0.663s, learning 0.132s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.79s
                        Total time: 396.69s
                               ETA: 659 mins 53.5 s

################################################################################
                      Learning iteration 496/50000                      

                       Computation: 132873 steps/s (collection: 0.615s, learning 0.125s)
               Value function loss: 0.0981
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.74s
                        Total time: 397.43s
                               ETA: 659 mins 46.7 s

################################################################################
                      Learning iteration 497/50000                      

                       Computation: 136311 steps/s (collection: 0.597s, learning 0.124s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.72s
                        Total time: 398.16s
                               ETA: 659 mins 38.1 s

################################################################################
                      Learning iteration 498/50000                      

                       Computation: 128660 steps/s (collection: 0.631s, learning 0.133s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.76s
                        Total time: 398.92s
                               ETA: 659 mins 33.8 s

################################################################################
                      Learning iteration 499/50000                      

                       Computation: 132290 steps/s (collection: 0.609s, learning 0.134s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0032
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.74s
                        Total time: 399.66s
                               ETA: 659 mins 27.4 s

################################################################################
                      Learning iteration 500/50000                      

                       Computation: 126812 steps/s (collection: 0.645s, learning 0.130s)
               Value function loss: 0.0949
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.78s
                        Total time: 400.44s
                               ETA: 659 mins 24.3 s

################################################################################
                      Learning iteration 501/50000                      

                       Computation: 122771 steps/s (collection: 0.674s, learning 0.127s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0049
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.80s
                        Total time: 401.24s
                               ETA: 659 mins 23.6 s

################################################################################
                      Learning iteration 502/50000                      

                       Computation: 127468 steps/s (collection: 0.648s, learning 0.124s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.77s
                        Total time: 402.01s
                               ETA: 659 mins 20.0 s

################################################################################
                      Learning iteration 503/50000                      

                       Computation: 111590 steps/s (collection: 0.746s, learning 0.135s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0050
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.88s
                        Total time: 402.89s
                               ETA: 659 mins 27.3 s

################################################################################
                      Learning iteration 504/50000                      

                       Computation: 134512 steps/s (collection: 0.605s, learning 0.125s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.73s
                        Total time: 403.62s
                               ETA: 659 mins 19.8 s

################################################################################
                      Learning iteration 505/50000                      

                       Computation: 133327 steps/s (collection: 0.614s, learning 0.123s)
               Value function loss: 0.0843
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.74s
                        Total time: 404.36s
                               ETA: 659 mins 12.9 s

################################################################################
                      Learning iteration 506/50000                      

                       Computation: 124193 steps/s (collection: 0.666s, learning 0.125s)
               Value function loss: 0.0679
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.79s
                        Total time: 405.15s
                               ETA: 659 mins 11.4 s

################################################################################
                      Learning iteration 507/50000                      

                       Computation: 133545 steps/s (collection: 0.613s, learning 0.123s)
               Value function loss: 0.0724
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.74s
                        Total time: 405.89s
                               ETA: 659 mins 4.4 s

################################################################################
                      Learning iteration 508/50000                      

                       Computation: 122308 steps/s (collection: 0.650s, learning 0.154s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.80s
                        Total time: 406.69s
                               ETA: 659 mins 4.1 s

################################################################################
                      Learning iteration 509/50000                      

                       Computation: 130067 steps/s (collection: 0.616s, learning 0.140s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.76s
                        Total time: 407.45s
                               ETA: 658 mins 59.1 s

################################################################################
                      Learning iteration 510/50000                      

                       Computation: 127488 steps/s (collection: 0.644s, learning 0.127s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0052
 Mean episode rew_tracking_lin_vel: 0.0050
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.77s
                        Total time: 408.22s
                               ETA: 658 mins 55.6 s

################################################################################
                      Learning iteration 511/50000                      

                       Computation: 130360 steps/s (collection: 0.629s, learning 0.125s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0051
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.75s
                        Total time: 408.97s
                               ETA: 658 mins 50.5 s

################################################################################
                      Learning iteration 512/50000                      

                       Computation: 116825 steps/s (collection: 0.674s, learning 0.167s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.84s
                        Total time: 409.81s
                               ETA: 658 mins 53.8 s

################################################################################
                      Learning iteration 513/50000                      

                       Computation: 123999 steps/s (collection: 0.655s, learning 0.138s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0080
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.79s
                        Total time: 410.61s
                               ETA: 658 mins 52.4 s

################################################################################
                      Learning iteration 514/50000                      

                       Computation: 132179 steps/s (collection: 0.620s, learning 0.124s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.74s
                        Total time: 411.35s
                               ETA: 658 mins 46.3 s

################################################################################
                      Learning iteration 515/50000                      

                       Computation: 136193 steps/s (collection: 0.598s, learning 0.124s)
               Value function loss: 0.0885
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.72s
                        Total time: 412.07s
                               ETA: 658 mins 38.1 s

################################################################################
                      Learning iteration 516/50000                      

                       Computation: 124228 steps/s (collection: 0.667s, learning 0.124s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.79s
                        Total time: 412.86s
                               ETA: 658 mins 36.6 s

################################################################################
                      Learning iteration 517/50000                      

                       Computation: 137409 steps/s (collection: 0.591s, learning 0.124s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0102
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0051
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.72s
                        Total time: 413.58s
                               ETA: 658 mins 27.9 s

################################################################################
                      Learning iteration 518/50000                      

                       Computation: 127927 steps/s (collection: 0.643s, learning 0.125s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.77s
                        Total time: 414.35s
                               ETA: 658 mins 24.2 s

################################################################################
                      Learning iteration 519/50000                      

                       Computation: 134003 steps/s (collection: 0.609s, learning 0.125s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.73s
                        Total time: 415.08s
                               ETA: 658 mins 17.3 s

################################################################################
                      Learning iteration 520/50000                      

                       Computation: 136035 steps/s (collection: 0.601s, learning 0.122s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.72s
                        Total time: 415.80s
                               ETA: 658 mins 9.3 s

################################################################################
                      Learning iteration 521/50000                      

                       Computation: 126757 steps/s (collection: 0.650s, learning 0.126s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.78s
                        Total time: 416.58s
                               ETA: 658 mins 6.4 s

################################################################################
                      Learning iteration 522/50000                      

                       Computation: 115490 steps/s (collection: 0.725s, learning 0.126s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.85s
                        Total time: 417.43s
                               ETA: 658 mins 10.6 s

################################################################################
                      Learning iteration 523/50000                      

                       Computation: 132488 steps/s (collection: 0.615s, learning 0.127s)
               Value function loss: 0.0711
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.74s
                        Total time: 418.17s
                               ETA: 658 mins 4.5 s

################################################################################
                      Learning iteration 524/50000                      

                       Computation: 123366 steps/s (collection: 0.669s, learning 0.128s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0053
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.80s
                        Total time: 418.97s
                               ETA: 658 mins 3.6 s

################################################################################
                      Learning iteration 525/50000                      

                       Computation: 122991 steps/s (collection: 0.669s, learning 0.130s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.80s
                        Total time: 419.77s
                               ETA: 658 mins 2.9 s

################################################################################
                      Learning iteration 526/50000                      

                       Computation: 137463 steps/s (collection: 0.592s, learning 0.124s)
               Value function loss: 0.0912
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0052
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.72s
                        Total time: 420.48s
                               ETA: 657 mins 54.3 s

################################################################################
                      Learning iteration 527/50000                      

                       Computation: 129406 steps/s (collection: 0.635s, learning 0.124s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0071
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.76s
                        Total time: 421.24s
                               ETA: 657 mins 49.9 s

################################################################################
                      Learning iteration 528/50000                      

                       Computation: 136364 steps/s (collection: 0.595s, learning 0.126s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.72s
                        Total time: 421.96s
                               ETA: 657 mins 41.9 s

################################################################################
                      Learning iteration 529/50000                      

                       Computation: 123123 steps/s (collection: 0.665s, learning 0.134s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.68
                Mean reward (task): -5.68
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.80s
                        Total time: 422.76s
                               ETA: 657 mins 41.2 s

################################################################################
                      Learning iteration 530/50000                      

                       Computation: 125939 steps/s (collection: 0.631s, learning 0.150s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.78s
                        Total time: 423.54s
                               ETA: 657 mins 38.8 s

################################################################################
                      Learning iteration 531/50000                      

                       Computation: 132388 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.74s
                        Total time: 424.28s
                               ETA: 657 mins 32.9 s

################################################################################
                      Learning iteration 532/50000                      

                       Computation: 130344 steps/s (collection: 0.631s, learning 0.124s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.75s
                        Total time: 425.04s
                               ETA: 657 mins 28.1 s

################################################################################
                      Learning iteration 533/50000                      

                       Computation: 130380 steps/s (collection: 0.632s, learning 0.122s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.75s
                        Total time: 425.79s
                               ETA: 657 mins 23.3 s

################################################################################
                      Learning iteration 534/50000                      

                       Computation: 131419 steps/s (collection: 0.623s, learning 0.125s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0062
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.75s
                        Total time: 426.54s
                               ETA: 657 mins 17.9 s

################################################################################
                      Learning iteration 535/50000                      

                       Computation: 133200 steps/s (collection: 0.598s, learning 0.140s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.74s
                        Total time: 427.28s
                               ETA: 657 mins 11.6 s

################################################################################
                      Learning iteration 536/50000                      

                       Computation: 127093 steps/s (collection: 0.632s, learning 0.142s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.77s
                        Total time: 428.05s
                               ETA: 657 mins 8.7 s

################################################################################
                      Learning iteration 537/50000                      

                       Computation: 127869 steps/s (collection: 0.643s, learning 0.125s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0081
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.77s
                        Total time: 428.82s
                               ETA: 657 mins 5.3 s

################################################################################
                      Learning iteration 538/50000                      

                       Computation: 125380 steps/s (collection: 0.657s, learning 0.127s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.78s
                        Total time: 429.61s
                               ETA: 657 mins 3.3 s

################################################################################
                      Learning iteration 539/50000                      

                       Computation: 139895 steps/s (collection: 0.580s, learning 0.122s)
               Value function loss: 0.0714
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.70s
                        Total time: 430.31s
                               ETA: 656 mins 53.8 s

################################################################################
                      Learning iteration 540/50000                      

                       Computation: 133300 steps/s (collection: 0.613s, learning 0.124s)
               Value function loss: 0.0703
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.74s
                        Total time: 431.05s
                               ETA: 656 mins 47.6 s

################################################################################
                      Learning iteration 541/50000                      

                       Computation: 128343 steps/s (collection: 0.623s, learning 0.143s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0022
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.77s
                        Total time: 431.81s
                               ETA: 656 mins 44.0 s

################################################################################
                      Learning iteration 542/50000                      

                       Computation: 121492 steps/s (collection: 0.665s, learning 0.144s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0252
       Mean episode rew_smoothness: -0.0082
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0053
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.81s
                        Total time: 432.62s
                               ETA: 656 mins 44.3 s

################################################################################
                      Learning iteration 543/50000                      

                       Computation: 114094 steps/s (collection: 0.707s, learning 0.155s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0083
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.86s
                        Total time: 433.48s
                               ETA: 656 mins 49.4 s

################################################################################
                      Learning iteration 544/50000                      

                       Computation: 118293 steps/s (collection: 0.706s, learning 0.125s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.83s
                        Total time: 434.31s
                               ETA: 656 mins 51.7 s

################################################################################
                      Learning iteration 545/50000                      

                       Computation: 123220 steps/s (collection: 0.674s, learning 0.124s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.61
                Mean reward (task): -5.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.80s
                        Total time: 435.11s
                               ETA: 656 mins 51.0 s

################################################################################
                      Learning iteration 546/50000                      

                       Computation: 121350 steps/s (collection: 0.687s, learning 0.123s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.81s
                        Total time: 435.92s
                               ETA: 656 mins 51.4 s

################################################################################
                      Learning iteration 547/50000                      

                       Computation: 124090 steps/s (collection: 0.668s, learning 0.124s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.79s
                        Total time: 436.71s
                               ETA: 656 mins 50.2 s

################################################################################
                      Learning iteration 548/50000                      

                       Computation: 131616 steps/s (collection: 0.623s, learning 0.124s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.75s
                        Total time: 437.46s
                               ETA: 656 mins 44.9 s

################################################################################
                      Learning iteration 549/50000                      

                       Computation: 130402 steps/s (collection: 0.630s, learning 0.124s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.75s
                        Total time: 438.21s
                               ETA: 656 mins 40.2 s

################################################################################
                      Learning iteration 550/50000                      

                       Computation: 121261 steps/s (collection: 0.672s, learning 0.138s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.81s
                        Total time: 439.02s
                               ETA: 656 mins 40.7 s

################################################################################
                      Learning iteration 551/50000                      

                       Computation: 125273 steps/s (collection: 0.636s, learning 0.149s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.78s
                        Total time: 439.81s
                               ETA: 656 mins 38.8 s

################################################################################
                      Learning iteration 552/50000                      

                       Computation: 127469 steps/s (collection: 0.643s, learning 0.128s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.77s
                        Total time: 440.58s
                               ETA: 656 mins 35.7 s

################################################################################
                      Learning iteration 553/50000                      

                       Computation: 129356 steps/s (collection: 0.623s, learning 0.137s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.76s
                        Total time: 441.34s
                               ETA: 656 mins 31.6 s

################################################################################
                      Learning iteration 554/50000                      

                       Computation: 133131 steps/s (collection: 0.615s, learning 0.123s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.74s
                        Total time: 442.08s
                               ETA: 656 mins 25.6 s

################################################################################
                      Learning iteration 555/50000                      

                       Computation: 127053 steps/s (collection: 0.637s, learning 0.137s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.77s
                        Total time: 442.85s
                               ETA: 656 mins 22.8 s

################################################################################
                      Learning iteration 556/50000                      

                       Computation: 117379 steps/s (collection: 0.691s, learning 0.146s)
               Value function loss: 0.0685
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.84s
                        Total time: 443.69s
                               ETA: 656 mins 25.7 s

################################################################################
                      Learning iteration 557/50000                      

                       Computation: 112771 steps/s (collection: 0.728s, learning 0.144s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.87s
                        Total time: 444.56s
                               ETA: 656 mins 31.5 s

################################################################################
                      Learning iteration 558/50000                      

                       Computation: 123099 steps/s (collection: 0.673s, learning 0.125s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.80s
                        Total time: 445.36s
                               ETA: 656 mins 30.9 s

################################################################################
                      Learning iteration 559/50000                      

                       Computation: 135317 steps/s (collection: 0.601s, learning 0.125s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.73s
                        Total time: 446.09s
                               ETA: 656 mins 23.9 s

################################################################################
                      Learning iteration 560/50000                      

                       Computation: 130256 steps/s (collection: 0.612s, learning 0.143s)
               Value function loss: 0.1123
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.75s
                        Total time: 446.84s
                               ETA: 656 mins 19.4 s

################################################################################
                      Learning iteration 561/50000                      

                       Computation: 131229 steps/s (collection: 0.605s, learning 0.144s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.75s
                        Total time: 447.59s
                               ETA: 656 mins 14.4 s

################################################################################
                      Learning iteration 562/50000                      

                       Computation: 129393 steps/s (collection: 0.635s, learning 0.125s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 23.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0055
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.76s
                        Total time: 448.35s
                               ETA: 656 mins 10.4 s

################################################################################
                      Learning iteration 563/50000                      

                       Computation: 128789 steps/s (collection: 0.639s, learning 0.124s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0054
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.76s
                        Total time: 449.11s
                               ETA: 656 mins 6.7 s

################################################################################
                      Learning iteration 564/50000                      

                       Computation: 135678 steps/s (collection: 0.599s, learning 0.125s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0054
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.72s
                        Total time: 449.84s
                               ETA: 655 mins 59.6 s

################################################################################
                      Learning iteration 565/50000                      

                       Computation: 122231 steps/s (collection: 0.682s, learning 0.123s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0033
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.80s
                        Total time: 450.64s
                               ETA: 655 mins 59.6 s

################################################################################
                      Learning iteration 566/50000                      

                       Computation: 136073 steps/s (collection: 0.600s, learning 0.122s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0101
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0084
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0055
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.72s
                        Total time: 451.36s
                               ETA: 655 mins 52.3 s

################################################################################
                      Learning iteration 567/50000                      

                       Computation: 118859 steps/s (collection: 0.689s, learning 0.138s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.60
                Mean reward (task): -5.60
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.83s
                        Total time: 452.19s
                               ETA: 655 mins 54.2 s

################################################################################
                      Learning iteration 568/50000                      

                       Computation: 118659 steps/s (collection: 0.704s, learning 0.125s)
               Value function loss: 0.0736
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.83s
                        Total time: 453.02s
                               ETA: 655 mins 56.2 s

################################################################################
                      Learning iteration 569/50000                      

                       Computation: 112873 steps/s (collection: 0.729s, learning 0.142s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.87s
                        Total time: 453.89s
                               ETA: 656 mins 1.9 s

################################################################################
                      Learning iteration 570/50000                      

                       Computation: 123728 steps/s (collection: 0.656s, learning 0.139s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0063
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.79s
                        Total time: 454.69s
                               ETA: 656 mins 1.0 s

################################################################################
                      Learning iteration 571/50000                      

                       Computation: 120247 steps/s (collection: 0.669s, learning 0.149s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.82s
                        Total time: 455.50s
                               ETA: 656 mins 2.0 s

################################################################################
                      Learning iteration 572/50000                      

                       Computation: 132427 steps/s (collection: 0.614s, learning 0.128s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.74s
                        Total time: 456.25s
                               ETA: 655 mins 56.6 s

################################################################################
                      Learning iteration 573/50000                      

                       Computation: 122981 steps/s (collection: 0.677s, learning 0.123s)
               Value function loss: 0.0853
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.80s
                        Total time: 457.05s
                               ETA: 655 mins 56.0 s

################################################################################
                      Learning iteration 574/50000                      

                       Computation: 121697 steps/s (collection: 0.678s, learning 0.130s)
               Value function loss: 0.0962
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0085
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.81s
                        Total time: 457.85s
                               ETA: 655 mins 56.2 s

################################################################################
                      Learning iteration 575/50000                      

                       Computation: 133408 steps/s (collection: 0.613s, learning 0.124s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.74s
                        Total time: 458.59s
                               ETA: 655 mins 50.3 s

################################################################################
                      Learning iteration 576/50000                      

                       Computation: 131671 steps/s (collection: 0.613s, learning 0.134s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.75s
                        Total time: 459.34s
                               ETA: 655 mins 45.3 s

################################################################################
                      Learning iteration 577/50000                      

                       Computation: 119002 steps/s (collection: 0.682s, learning 0.144s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.83s
                        Total time: 460.16s
                               ETA: 655 mins 47.1 s

################################################################################
                      Learning iteration 578/50000                      

                       Computation: 126063 steps/s (collection: 0.642s, learning 0.138s)
               Value function loss: 0.0881
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0056
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.78s
                        Total time: 460.94s
                               ETA: 655 mins 44.9 s

################################################################################
                      Learning iteration 579/50000                      

                       Computation: 129112 steps/s (collection: 0.621s, learning 0.141s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.76s
                        Total time: 461.70s
                               ETA: 655 mins 41.1 s

################################################################################
                      Learning iteration 580/50000                      

                       Computation: 125990 steps/s (collection: 0.639s, learning 0.141s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.78s
                        Total time: 462.48s
                               ETA: 655 mins 39.0 s

################################################################################
                      Learning iteration 581/50000                      

                       Computation: 128834 steps/s (collection: 0.625s, learning 0.138s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.76s
                        Total time: 463.25s
                               ETA: 655 mins 35.4 s

################################################################################
                      Learning iteration 582/50000                      

                       Computation: 125120 steps/s (collection: 0.652s, learning 0.134s)
               Value function loss: 0.0776
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.67
                Mean reward (task): -5.67
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.79s
                        Total time: 464.03s
                               ETA: 655 mins 33.7 s

################################################################################
                      Learning iteration 583/50000                      

                       Computation: 132254 steps/s (collection: 0.620s, learning 0.123s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.74s
                        Total time: 464.78s
                               ETA: 655 mins 28.5 s

################################################################################
                      Learning iteration 584/50000                      

                       Computation: 124393 steps/s (collection: 0.667s, learning 0.123s)
               Value function loss: 0.0862
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.79s
                        Total time: 465.57s
                               ETA: 655 mins 27.2 s

################################################################################
                      Learning iteration 585/50000                      

                       Computation: 130931 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.75s
                        Total time: 466.32s
                               ETA: 655 mins 22.6 s

################################################################################
                      Learning iteration 586/50000                      

                       Computation: 127751 steps/s (collection: 0.631s, learning 0.139s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.77s
                        Total time: 467.09s
                               ETA: 655 mins 19.6 s

################################################################################
                      Learning iteration 587/50000                      

                       Computation: 120055 steps/s (collection: 0.696s, learning 0.123s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.82s
                        Total time: 467.91s
                               ETA: 655 mins 20.7 s

################################################################################
                      Learning iteration 588/50000                      

                       Computation: 126132 steps/s (collection: 0.649s, learning 0.131s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.61
                Mean reward (task): -5.61
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0064
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.78s
                        Total time: 468.68s
                               ETA: 655 mins 18.6 s

################################################################################
                      Learning iteration 589/50000                      

                       Computation: 131292 steps/s (collection: 0.620s, learning 0.128s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.75s
                        Total time: 469.43s
                               ETA: 655 mins 13.8 s

################################################################################
                      Learning iteration 590/50000                      

                       Computation: 122268 steps/s (collection: 0.663s, learning 0.141s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.80s
                        Total time: 470.24s
                               ETA: 655 mins 13.7 s

################################################################################
                      Learning iteration 591/50000                      

                       Computation: 134925 steps/s (collection: 0.607s, learning 0.122s)
               Value function loss: 0.0947
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0057
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.73s
                        Total time: 470.97s
                               ETA: 655 mins 7.3 s

################################################################################
                      Learning iteration 592/50000                      

                       Computation: 124981 steps/s (collection: 0.643s, learning 0.144s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0034
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.79s
                        Total time: 471.75s
                               ETA: 655 mins 5.8 s

################################################################################
                      Learning iteration 593/50000                      

                       Computation: 127757 steps/s (collection: 0.628s, learning 0.141s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.77s
                        Total time: 472.52s
                               ETA: 655 mins 2.8 s

################################################################################
                      Learning iteration 594/50000                      

                       Computation: 117252 steps/s (collection: 0.711s, learning 0.127s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.84s
                        Total time: 473.36s
                               ETA: 655 mins 5.6 s

################################################################################
                      Learning iteration 595/50000                      

                       Computation: 123226 steps/s (collection: 0.653s, learning 0.145s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.80s
                        Total time: 474.16s
                               ETA: 655 mins 5.0 s

################################################################################
                      Learning iteration 596/50000                      

                       Computation: 125310 steps/s (collection: 0.645s, learning 0.140s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.78s
                        Total time: 474.94s
                               ETA: 655 mins 3.3 s

################################################################################
                      Learning iteration 597/50000                      

                       Computation: 132688 steps/s (collection: 0.602s, learning 0.139s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.74s
                        Total time: 475.68s
                               ETA: 654 mins 58.0 s

################################################################################
                      Learning iteration 598/50000                      

                       Computation: 126192 steps/s (collection: 0.655s, learning 0.124s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.78s
                        Total time: 476.46s
                               ETA: 654 mins 55.8 s

################################################################################
                      Learning iteration 599/50000                      

                       Computation: 135310 steps/s (collection: 0.604s, learning 0.123s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.73s
                        Total time: 477.19s
                               ETA: 654 mins 49.3 s

################################################################################
                      Learning iteration 600/50000                      

                       Computation: 119048 steps/s (collection: 0.687s, learning 0.139s)
               Value function loss: 0.0992
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.83s
                        Total time: 478.01s
                               ETA: 654 mins 51.0 s

################################################################################
                      Learning iteration 601/50000                      

                       Computation: 131990 steps/s (collection: 0.621s, learning 0.124s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.74s
                        Total time: 478.76s
                               ETA: 654 mins 46.1 s

################################################################################
                      Learning iteration 602/50000                      

                       Computation: 119966 steps/s (collection: 0.684s, learning 0.135s)
               Value function loss: 0.0872
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.82s
                        Total time: 479.58s
                               ETA: 654 mins 47.3 s

################################################################################
                      Learning iteration 603/50000                      

                       Computation: 121113 steps/s (collection: 0.687s, learning 0.125s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.81s
                        Total time: 480.39s
                               ETA: 654 mins 47.8 s

################################################################################
                      Learning iteration 604/50000                      

                       Computation: 124606 steps/s (collection: 0.657s, learning 0.132s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.79s
                        Total time: 481.18s
                               ETA: 654 mins 46.5 s

################################################################################
                      Learning iteration 605/50000                      

                       Computation: 131133 steps/s (collection: 0.621s, learning 0.128s)
               Value function loss: 0.0876
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.55
                Mean reward (task): -5.55
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0035
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0023
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0056
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.75s
                        Total time: 481.93s
                               ETA: 654 mins 42.0 s

################################################################################
                      Learning iteration 606/50000                      

                       Computation: 121756 steps/s (collection: 0.670s, learning 0.137s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0086
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.81s
                        Total time: 482.74s
                               ETA: 654 mins 42.2 s

################################################################################
                      Learning iteration 607/50000                      

                       Computation: 131777 steps/s (collection: 0.621s, learning 0.125s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0065
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.75s
                        Total time: 483.48s
                               ETA: 654 mins 37.4 s

################################################################################
                      Learning iteration 608/50000                      

                       Computation: 124510 steps/s (collection: 0.664s, learning 0.126s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.52
                Mean reward (task): -5.52
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0058
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.79s
                        Total time: 484.27s
                               ETA: 654 mins 36.1 s

################################################################################
                      Learning iteration 609/50000                      

                       Computation: 127207 steps/s (collection: 0.648s, learning 0.124s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0059
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.77s
                        Total time: 485.04s
                               ETA: 654 mins 33.5 s

################################################################################
                      Learning iteration 610/50000                      

                       Computation: 119197 steps/s (collection: 0.702s, learning 0.123s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.82s
                        Total time: 485.87s
                               ETA: 654 mins 35.1 s

################################################################################
                      Learning iteration 611/50000                      

                       Computation: 136862 steps/s (collection: 0.595s, learning 0.123s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.72s
                        Total time: 486.59s
                               ETA: 654 mins 28.1 s

################################################################################
                      Learning iteration 612/50000                      

                       Computation: 127136 steps/s (collection: 0.636s, learning 0.137s)
               Value function loss: 0.0928
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0269
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.77s
                        Total time: 487.36s
                               ETA: 654 mins 25.5 s

################################################################################
                      Learning iteration 613/50000                      

                       Computation: 127468 steps/s (collection: 0.638s, learning 0.133s)
               Value function loss: 0.0909
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.77s
                        Total time: 488.13s
                               ETA: 654 mins 22.8 s

################################################################################
                      Learning iteration 614/50000                      

                       Computation: 132456 steps/s (collection: 0.618s, learning 0.124s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0267
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.74s
                        Total time: 488.87s
                               ETA: 654 mins 17.8 s

################################################################################
                      Learning iteration 615/50000                      

                       Computation: 134122 steps/s (collection: 0.611s, learning 0.122s)
               Value function loss: 0.1442
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.73s
                        Total time: 489.61s
                               ETA: 654 mins 12.0 s

################################################################################
                      Learning iteration 616/50000                      

                       Computation: 112747 steps/s (collection: 0.709s, learning 0.163s)
               Value function loss: 0.0862
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0194
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0274
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.87s
                        Total time: 490.48s
                               ETA: 654 mins 17.4 s

################################################################################
                      Learning iteration 617/50000                      

                       Computation: 133623 steps/s (collection: 0.595s, learning 0.140s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.74s
                        Total time: 491.21s
                               ETA: 654 mins 11.9 s

################################################################################
                      Learning iteration 618/50000                      

                       Computation: 135952 steps/s (collection: 0.587s, learning 0.136s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0017
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.72s
                        Total time: 491.94s
                               ETA: 654 mins 5.4 s

################################################################################
                      Learning iteration 619/50000                      

                       Computation: 123624 steps/s (collection: 0.669s, learning 0.127s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.80s
                        Total time: 492.73s
                               ETA: 654 mins 4.6 s

################################################################################
                      Learning iteration 620/50000                      

                       Computation: 120020 steps/s (collection: 0.695s, learning 0.124s)
               Value function loss: 0.1036
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.82s
                        Total time: 493.55s
                               ETA: 654 mins 5.7 s

################################################################################
                      Learning iteration 621/50000                      

                       Computation: 134278 steps/s (collection: 0.600s, learning 0.132s)
               Value function loss: 0.0857
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.73s
                        Total time: 494.28s
                               ETA: 653 mins 60.0 s

################################################################################
                      Learning iteration 622/50000                      

                       Computation: 118890 steps/s (collection: 0.704s, learning 0.123s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.83s
                        Total time: 495.11s
                               ETA: 654 mins 1.7 s

################################################################################
                      Learning iteration 623/50000                      

                       Computation: 134364 steps/s (collection: 0.604s, learning 0.128s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.73s
                        Total time: 495.84s
                               ETA: 653 mins 55.9 s

################################################################################
                      Learning iteration 624/50000                      

                       Computation: 121339 steps/s (collection: 0.686s, learning 0.124s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0066
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.81s
                        Total time: 496.65s
                               ETA: 653 mins 56.4 s

################################################################################
                      Learning iteration 625/50000                      

                       Computation: 136578 steps/s (collection: 0.596s, learning 0.123s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.72s
                        Total time: 497.37s
                               ETA: 653 mins 49.7 s

################################################################################
                      Learning iteration 626/50000                      

                       Computation: 134878 steps/s (collection: 0.602s, learning 0.127s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0057
 Mean episode rew_tracking_lin_vel: 0.0060
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.73s
                        Total time: 498.10s
                               ETA: 653 mins 43.7 s

################################################################################
                      Learning iteration 627/50000                      

                       Computation: 124441 steps/s (collection: 0.666s, learning 0.124s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.79s
                        Total time: 498.89s
                               ETA: 653 mins 42.6 s

################################################################################
                      Learning iteration 628/50000                      

                       Computation: 119462 steps/s (collection: 0.698s, learning 0.125s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0085
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.82s
                        Total time: 499.71s
                               ETA: 653 mins 44.0 s

################################################################################
                      Learning iteration 629/50000                      

                       Computation: 123804 steps/s (collection: 0.670s, learning 0.124s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.79s
                        Total time: 500.51s
                               ETA: 653 mins 43.2 s

################################################################################
                      Learning iteration 630/50000                      

                       Computation: 130737 steps/s (collection: 0.628s, learning 0.124s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0072
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.75s
                        Total time: 501.26s
                               ETA: 653 mins 39.0 s

################################################################################
                      Learning iteration 631/50000                      

                       Computation: 131327 steps/s (collection: 0.611s, learning 0.138s)
               Value function loss: 0.0868
                    Surrogate loss: -0.0087
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.75s
                        Total time: 502.01s
                               ETA: 653 mins 34.7 s

################################################################################
                      Learning iteration 632/50000                      

                       Computation: 119622 steps/s (collection: 0.697s, learning 0.125s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.82s
                        Total time: 502.83s
                               ETA: 653 mins 36.0 s

################################################################################
                      Learning iteration 633/50000                      

                       Computation: 134410 steps/s (collection: 0.607s, learning 0.124s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0036
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0087
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.73s
                        Total time: 503.56s
                               ETA: 653 mins 30.3 s

################################################################################
                      Learning iteration 634/50000                      

                       Computation: 130920 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.75s
                        Total time: 504.31s
                               ETA: 653 mins 26.1 s

################################################################################
                      Learning iteration 635/50000                      

                       Computation: 128462 steps/s (collection: 0.634s, learning 0.131s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0268
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.77s
                        Total time: 505.08s
                               ETA: 653 mins 23.1 s

################################################################################
                      Learning iteration 636/50000                      

                       Computation: 112791 steps/s (collection: 0.731s, learning 0.140s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.87s
                        Total time: 505.95s
                               ETA: 653 mins 28.3 s

################################################################################
                      Learning iteration 637/50000                      

                       Computation: 126901 steps/s (collection: 0.627s, learning 0.148s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0088
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.77s
                        Total time: 506.72s
                               ETA: 653 mins 26.0 s

################################################################################
                      Learning iteration 638/50000                      

                       Computation: 122847 steps/s (collection: 0.643s, learning 0.157s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.80s
                        Total time: 507.52s
                               ETA: 653 mins 25.7 s

################################################################################
                      Learning iteration 639/50000                      

                       Computation: 122736 steps/s (collection: 0.667s, learning 0.134s)
               Value function loss: 0.0952
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.80s
                        Total time: 508.33s
                               ETA: 653 mins 25.4 s

################################################################################
                      Learning iteration 640/50000                      

                       Computation: 120834 steps/s (collection: 0.681s, learning 0.133s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0067
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0264
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.81s
                        Total time: 509.14s
                               ETA: 653 mins 26.1 s

################################################################################
                      Learning iteration 641/50000                      

                       Computation: 118709 steps/s (collection: 0.687s, learning 0.141s)
               Value function loss: 0.1067
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.83s
                        Total time: 509.97s
                               ETA: 653 mins 27.9 s

################################################################################
                      Learning iteration 642/50000                      

                       Computation: 116218 steps/s (collection: 0.713s, learning 0.133s)
               Value function loss: 0.0955
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0059
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.85s
                        Total time: 510.81s
                               ETA: 653 mins 31.0 s

################################################################################
                      Learning iteration 643/50000                      

                       Computation: 132707 steps/s (collection: 0.617s, learning 0.124s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0266
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.74s
                        Total time: 511.55s
                               ETA: 653 mins 26.1 s

################################################################################
                      Learning iteration 644/50000                      

                       Computation: 123472 steps/s (collection: 0.659s, learning 0.137s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.80s
                        Total time: 512.35s
                               ETA: 653 mins 25.5 s

################################################################################
                      Learning iteration 645/50000                      

                       Computation: 133401 steps/s (collection: 0.609s, learning 0.127s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.74s
                        Total time: 513.09s
                               ETA: 653 mins 20.3 s

################################################################################
                      Learning iteration 646/50000                      

                       Computation: 121127 steps/s (collection: 0.683s, learning 0.128s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0037
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0058
 Mean episode rew_tracking_lin_vel: 0.0063
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.81s
                        Total time: 513.90s
                               ETA: 653 mins 20.8 s

################################################################################
                      Learning iteration 647/50000                      

                       Computation: 121922 steps/s (collection: 0.679s, learning 0.128s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.81s
                        Total time: 514.70s
                               ETA: 653 mins 20.9 s

################################################################################
                      Learning iteration 648/50000                      

                       Computation: 111996 steps/s (collection: 0.754s, learning 0.124s)
               Value function loss: 0.0851
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.88s
                        Total time: 515.58s
                               ETA: 653 mins 26.5 s

################################################################################
                      Learning iteration 649/50000                      

                       Computation: 129314 steps/s (collection: 0.625s, learning 0.135s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.76s
                        Total time: 516.34s
                               ETA: 653 mins 23.1 s

################################################################################
                      Learning iteration 650/50000                      

                       Computation: 132047 steps/s (collection: 0.609s, learning 0.136s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.74s
                        Total time: 517.09s
                               ETA: 653 mins 18.5 s

################################################################################
                      Learning iteration 651/50000                      

                       Computation: 129058 steps/s (collection: 0.631s, learning 0.131s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.58
                Mean reward (task): -5.58
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 24.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.76s
                        Total time: 517.85s
                               ETA: 653 mins 15.3 s

################################################################################
                      Learning iteration 652/50000                      

                       Computation: 112419 steps/s (collection: 0.740s, learning 0.134s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0062
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.87s
                        Total time: 518.72s
                               ETA: 653 mins 20.5 s

################################################################################
                      Learning iteration 653/50000                      

                       Computation: 128913 steps/s (collection: 0.634s, learning 0.129s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.76s
                        Total time: 519.49s
                               ETA: 653 mins 17.3 s

################################################################################
                      Learning iteration 654/50000                      

                       Computation: 128602 steps/s (collection: 0.622s, learning 0.143s)
               Value function loss: 0.0859
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.76s
                        Total time: 520.25s
                               ETA: 653 mins 14.3 s

################################################################################
                      Learning iteration 655/50000                      

                       Computation: 116489 steps/s (collection: 0.705s, learning 0.139s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0068
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.84s
                        Total time: 521.09s
                               ETA: 653 mins 17.2 s

################################################################################
                      Learning iteration 656/50000                      

                       Computation: 121319 steps/s (collection: 0.688s, learning 0.122s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.81s
                        Total time: 521.90s
                               ETA: 653 mins 17.6 s

################################################################################
                      Learning iteration 657/50000                      

                       Computation: 109416 steps/s (collection: 0.764s, learning 0.135s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.90s
                        Total time: 522.80s
                               ETA: 653 mins 24.6 s

################################################################################
                      Learning iteration 658/50000                      

                       Computation: 124931 steps/s (collection: 0.657s, learning 0.130s)
               Value function loss: 0.0862
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0064
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.79s
                        Total time: 523.59s
                               ETA: 653 mins 23.3 s

################################################################################
                      Learning iteration 659/50000                      

                       Computation: 133147 steps/s (collection: 0.610s, learning 0.129s)
               Value function loss: 0.0877
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.74s
                        Total time: 524.33s
                               ETA: 653 mins 18.3 s

################################################################################
                      Learning iteration 660/50000                      

                       Computation: 125626 steps/s (collection: 0.644s, learning 0.138s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0265
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.78s
                        Total time: 525.11s
                               ETA: 653 mins 16.6 s

################################################################################
                      Learning iteration 661/50000                      

                       Computation: 131787 steps/s (collection: 0.622s, learning 0.124s)
               Value function loss: 0.0819
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0089
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0060
 Mean episode rew_tracking_lin_vel: 0.0065
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.75s
                        Total time: 525.86s
                               ETA: 653 mins 12.2 s

################################################################################
                      Learning iteration 662/50000                      

                       Computation: 132026 steps/s (collection: 0.617s, learning 0.127s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.74s
                        Total time: 526.60s
                               ETA: 653 mins 7.7 s

################################################################################
                      Learning iteration 663/50000                      

                       Computation: 132768 steps/s (collection: 0.613s, learning 0.128s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0024
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0061
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.74s
                        Total time: 527.34s
                               ETA: 653 mins 2.9 s

################################################################################
                      Learning iteration 664/50000                      

                       Computation: 134665 steps/s (collection: 0.608s, learning 0.122s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.73s
                        Total time: 528.07s
                               ETA: 652 mins 57.3 s

################################################################################
                      Learning iteration 665/50000                      

                       Computation: 123538 steps/s (collection: 0.665s, learning 0.130s)
               Value function loss: 0.0925
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0070
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.80s
                        Total time: 528.87s
                               ETA: 652 mins 56.7 s

################################################################################
                      Learning iteration 666/50000                      

                       Computation: 123742 steps/s (collection: 0.670s, learning 0.125s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0188
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 0.79s
                        Total time: 529.66s
                               ETA: 652 mins 55.9 s

################################################################################
                      Learning iteration 667/50000                      

                       Computation: 133847 steps/s (collection: 0.609s, learning 0.125s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 0.73s
                        Total time: 530.40s
                               ETA: 652 mins 50.7 s

################################################################################
                      Learning iteration 668/50000                      

                       Computation: 131021 steps/s (collection: 0.621s, learning 0.129s)
               Value function loss: 0.0908
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0069
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 0.75s
                        Total time: 531.15s
                               ETA: 652 mins 46.7 s

################################################################################
                      Learning iteration 669/50000                      

                       Computation: 129740 steps/s (collection: 0.632s, learning 0.125s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0188
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 0.76s
                        Total time: 531.90s
                               ETA: 652 mins 43.2 s

################################################################################
                      Learning iteration 670/50000                      

                       Computation: 126732 steps/s (collection: 0.651s, learning 0.125s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 0.78s
                        Total time: 532.68s
                               ETA: 652 mins 41.1 s

################################################################################
                      Learning iteration 671/50000                      

                       Computation: 132674 steps/s (collection: 0.616s, learning 0.125s)
               Value function loss: 0.0927
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0251
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 0.74s
                        Total time: 533.42s
                               ETA: 652 mins 36.4 s

################################################################################
                      Learning iteration 672/50000                      

                       Computation: 116983 steps/s (collection: 0.717s, learning 0.123s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0068
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 0.84s
                        Total time: 534.26s
                               ETA: 652 mins 39.0 s

################################################################################
                      Learning iteration 673/50000                      

                       Computation: 129029 steps/s (collection: 0.640s, learning 0.122s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0068
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 0.76s
                        Total time: 535.02s
                               ETA: 652 mins 35.9 s

################################################################################
                      Learning iteration 674/50000                      

                       Computation: 124270 steps/s (collection: 0.639s, learning 0.152s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0090
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 0.79s
                        Total time: 535.81s
                               ETA: 652 mins 34.9 s

################################################################################
                      Learning iteration 675/50000                      

                       Computation: 134721 steps/s (collection: 0.605s, learning 0.125s)
               Value function loss: 0.0760
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 0.73s
                        Total time: 536.54s
                               ETA: 652 mins 29.4 s

################################################################################
                      Learning iteration 676/50000                      

                       Computation: 122239 steps/s (collection: 0.654s, learning 0.150s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0070
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.80s
                        Total time: 537.35s
                               ETA: 652 mins 29.4 s

################################################################################
                      Learning iteration 677/50000                      

                       Computation: 122645 steps/s (collection: 0.658s, learning 0.143s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0070
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.80s
                        Total time: 538.15s
                               ETA: 652 mins 29.2 s

################################################################################
                      Learning iteration 678/50000                      

                       Computation: 123474 steps/s (collection: 0.651s, learning 0.145s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0068
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.80s
                        Total time: 538.95s
                               ETA: 652 mins 28.5 s

################################################################################
                      Learning iteration 679/50000                      

                       Computation: 132009 steps/s (collection: 0.622s, learning 0.122s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.74s
                        Total time: 539.69s
                               ETA: 652 mins 24.2 s

################################################################################
                      Learning iteration 680/50000                      

                       Computation: 124151 steps/s (collection: 0.669s, learning 0.123s)
               Value function loss: 0.1022
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0038
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.79s
                        Total time: 540.48s
                               ETA: 652 mins 23.3 s

################################################################################
                      Learning iteration 681/50000                      

                       Computation: 121636 steps/s (collection: 0.686s, learning 0.122s)
               Value function loss: 0.0988
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.81s
                        Total time: 541.29s
                               ETA: 652 mins 23.5 s

################################################################################
                      Learning iteration 682/50000                      

                       Computation: 132770 steps/s (collection: 0.615s, learning 0.125s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0189
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.74s
                        Total time: 542.03s
                               ETA: 652 mins 18.9 s

################################################################################
                      Learning iteration 683/50000                      

                       Computation: 137652 steps/s (collection: 0.592s, learning 0.122s)
               Value function loss: 0.0793
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.48
                Mean reward (task): -5.48
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0260
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0070
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.71s
                        Total time: 542.74s
                               ETA: 652 mins 12.3 s

################################################################################
                      Learning iteration 684/50000                      

                       Computation: 127102 steps/s (collection: 0.649s, learning 0.124s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0251
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.77s
                        Total time: 543.52s
                               ETA: 652 mins 10.1 s

################################################################################
                      Learning iteration 685/50000                      

                       Computation: 135483 steps/s (collection: 0.601s, learning 0.125s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.73s
                        Total time: 544.24s
                               ETA: 652 mins 4.4 s

################################################################################
                      Learning iteration 686/50000                      

                       Computation: 123386 steps/s (collection: 0.655s, learning 0.142s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0252
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0069
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.80s
                        Total time: 545.04s
                               ETA: 652 mins 3.9 s

################################################################################
                      Learning iteration 687/50000                      

                       Computation: 136590 steps/s (collection: 0.597s, learning 0.123s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.72s
                        Total time: 545.76s
                               ETA: 651 mins 57.8 s

################################################################################
                      Learning iteration 688/50000                      

                       Computation: 117855 steps/s (collection: 0.684s, learning 0.150s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.83s
                        Total time: 546.59s
                               ETA: 651 mins 59.9 s

################################################################################
                      Learning iteration 689/50000                      

                       Computation: 131405 steps/s (collection: 0.625s, learning 0.124s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0066
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.75s
                        Total time: 547.34s
                               ETA: 651 mins 55.9 s

################################################################################
                      Learning iteration 690/50000                      

                       Computation: 127202 steps/s (collection: 0.649s, learning 0.124s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0188
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0262
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.77s
                        Total time: 548.11s
                               ETA: 651 mins 53.7 s

################################################################################
                      Learning iteration 691/50000                      

                       Computation: 126090 steps/s (collection: 0.657s, learning 0.123s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.78s
                        Total time: 548.89s
                               ETA: 651 mins 51.9 s

################################################################################
                      Learning iteration 692/50000                      

                       Computation: 131878 steps/s (collection: 0.622s, learning 0.124s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0071
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.75s
                        Total time: 549.64s
                               ETA: 651 mins 47.7 s

################################################################################
                      Learning iteration 693/50000                      

                       Computation: 121469 steps/s (collection: 0.683s, learning 0.127s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0188
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0072
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.81s
                        Total time: 550.45s
                               ETA: 651 mins 48.1 s

################################################################################
                      Learning iteration 694/50000                      

                       Computation: 137278 steps/s (collection: 0.592s, learning 0.124s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0261
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0061
 Mean episode rew_tracking_lin_vel: 0.0067
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.72s
                        Total time: 551.17s
                               ETA: 651 mins 41.8 s

################################################################################
                      Learning iteration 695/50000                      

                       Computation: 123453 steps/s (collection: 0.667s, learning 0.129s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.80s
                        Total time: 551.96s
                               ETA: 651 mins 41.2 s

################################################################################
                      Learning iteration 696/50000                      

                       Computation: 135669 steps/s (collection: 0.582s, learning 0.143s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0072
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.72s
                        Total time: 552.69s
                               ETA: 651 mins 35.6 s

################################################################################
                      Learning iteration 697/50000                      

                       Computation: 127519 steps/s (collection: 0.640s, learning 0.131s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0095
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0071
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.77s
                        Total time: 553.46s
                               ETA: 651 mins 33.3 s

################################################################################
                      Learning iteration 698/50000                      

                       Computation: 127456 steps/s (collection: 0.649s, learning 0.122s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0071
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.77s
                        Total time: 554.23s
                               ETA: 651 mins 30.9 s

################################################################################
                      Learning iteration 699/50000                      

                       Computation: 128694 steps/s (collection: 0.631s, learning 0.132s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0070
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0025
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0252
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.76s
                        Total time: 554.99s
                               ETA: 651 mins 28.1 s

################################################################################
                      Learning iteration 700/50000                      

                       Computation: 131980 steps/s (collection: 0.617s, learning 0.127s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.74s
                        Total time: 555.74s
                               ETA: 651 mins 23.9 s

################################################################################
                      Learning iteration 701/50000                      

                       Computation: 125770 steps/s (collection: 0.658s, learning 0.124s)
               Value function loss: 0.0941
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.78s
                        Total time: 556.52s
                               ETA: 651 mins 22.4 s

################################################################################
                      Learning iteration 702/50000                      

                       Computation: 123758 steps/s (collection: 0.671s, learning 0.123s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0091
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.79s
                        Total time: 557.31s
                               ETA: 651 mins 21.7 s

################################################################################
                      Learning iteration 703/50000                      

                       Computation: 124472 steps/s (collection: 0.667s, learning 0.123s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0071
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.79s
                        Total time: 558.10s
                               ETA: 651 mins 20.7 s

################################################################################
                      Learning iteration 704/50000                      

                       Computation: 132042 steps/s (collection: 0.620s, learning 0.124s)
               Value function loss: 0.0988
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0252
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0072
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.74s
                        Total time: 558.85s
                               ETA: 651 mins 16.5 s

################################################################################
                      Learning iteration 705/50000                      

                       Computation: 117484 steps/s (collection: 0.698s, learning 0.139s)
               Value function loss: 0.0977
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0039
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0063
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.84s
                        Total time: 559.68s
                               ETA: 651 mins 18.8 s

################################################################################
                      Learning iteration 706/50000                      

                       Computation: 118955 steps/s (collection: 0.679s, learning 0.147s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0074
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.83s
                        Total time: 560.51s
                               ETA: 651 mins 20.3 s

################################################################################
                      Learning iteration 707/50000                      

                       Computation: 123344 steps/s (collection: 0.628s, learning 0.169s)
               Value function loss: 0.0978
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0062
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.80s
                        Total time: 561.31s
                               ETA: 651 mins 19.8 s

################################################################################
                      Learning iteration 708/50000                      

                       Computation: 122796 steps/s (collection: 0.660s, learning 0.140s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0257
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.80s
                        Total time: 562.11s
                               ETA: 651 mins 19.6 s

################################################################################
                      Learning iteration 709/50000                      

                       Computation: 132094 steps/s (collection: 0.619s, learning 0.125s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.74s
                        Total time: 562.85s
                               ETA: 651 mins 15.4 s

################################################################################
                      Learning iteration 710/50000                      

                       Computation: 125347 steps/s (collection: 0.640s, learning 0.144s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0189
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0259
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0011
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.78s
                        Total time: 563.64s
                               ETA: 651 mins 14.0 s

################################################################################
                      Learning iteration 711/50000                      

                       Computation: 128342 steps/s (collection: 0.642s, learning 0.124s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.57
                Mean reward (task): -5.57
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.77s
                        Total time: 564.40s
                               ETA: 651 mins 11.4 s

################################################################################
                      Learning iteration 712/50000                      

                       Computation: 112719 steps/s (collection: 0.736s, learning 0.136s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0071
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.87s
                        Total time: 565.27s
                               ETA: 651 mins 16.1 s

################################################################################
                      Learning iteration 713/50000                      

                       Computation: 138167 steps/s (collection: 0.587s, learning 0.124s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.45
                Mean reward (task): -5.45
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0076
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.71s
                        Total time: 565.99s
                               ETA: 651 mins 9.7 s

################################################################################
                      Learning iteration 714/50000                      

                       Computation: 126913 steps/s (collection: 0.630s, learning 0.144s)
               Value function loss: 0.0895
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0195
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0263
       Mean episode rew_smoothness: -0.0093
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0073
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.77s
                        Total time: 566.76s
                               ETA: 651 mins 7.6 s

################################################################################
                      Learning iteration 715/50000                      

                       Computation: 133854 steps/s (collection: 0.610s, learning 0.124s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0077
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.73s
                        Total time: 567.49s
                               ETA: 651 mins 2.8 s

################################################################################
                      Learning iteration 716/50000                      

                       Computation: 119240 steps/s (collection: 0.699s, learning 0.125s)
               Value function loss: 0.0946
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0077
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.82s
                        Total time: 568.32s
                               ETA: 651 mins 4.2 s

################################################################################
                      Learning iteration 717/50000                      

                       Computation: 128933 steps/s (collection: 0.637s, learning 0.126s)
               Value function loss: 0.0912
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.62
                Mean reward (task): -5.62
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0072
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.76s
                        Total time: 569.08s
                               ETA: 651 mins 1.4 s

################################################################################
                      Learning iteration 718/50000                      

                       Computation: 125837 steps/s (collection: 0.657s, learning 0.124s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0077
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.78s
                        Total time: 569.86s
                               ETA: 650 mins 59.8 s

################################################################################
                      Learning iteration 719/50000                      

                       Computation: 121254 steps/s (collection: 0.663s, learning 0.148s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0189
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0258
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0076
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.81s
                        Total time: 570.67s
                               ETA: 651 mins 0.2 s

################################################################################
                      Learning iteration 720/50000                      

                       Computation: 131339 steps/s (collection: 0.620s, learning 0.129s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0189
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0256
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.75s
                        Total time: 571.42s
                               ETA: 650 mins 56.4 s

################################################################################
                      Learning iteration 721/50000                      

                       Computation: 128554 steps/s (collection: 0.634s, learning 0.131s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0253
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.76s
                        Total time: 572.19s
                               ETA: 650 mins 53.7 s

################################################################################
                      Learning iteration 722/50000                      

                       Computation: 127486 steps/s (collection: 0.647s, learning 0.124s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0072
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0252
       Mean episode rew_smoothness: -0.0092
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.77s
                        Total time: 572.96s
                               ETA: 650 mins 51.5 s

################################################################################
                      Learning iteration 723/50000                      

                       Computation: 135736 steps/s (collection: 0.600s, learning 0.124s)
               Value function loss: 0.0896
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0250
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.72s
                        Total time: 573.68s
                               ETA: 650 mins 46.0 s

################################################################################
                      Learning iteration 724/50000                      

                       Computation: 135444 steps/s (collection: 0.597s, learning 0.128s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0040
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0077
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.73s
                        Total time: 574.41s
                               ETA: 650 mins 40.7 s

################################################################################
                      Learning iteration 725/50000                      

                       Computation: 115266 steps/s (collection: 0.711s, learning 0.142s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0064
 Mean episode rew_tracking_lin_vel: 0.0078
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.85s
                        Total time: 575.26s
                               ETA: 650 mins 44.0 s

################################################################################
                      Learning iteration 726/50000                      

                       Computation: 131705 steps/s (collection: 0.622s, learning 0.124s)
               Value function loss: 0.0813
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0100
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0188
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0026
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0076
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.75s
                        Total time: 576.01s
                               ETA: 650 mins 40.1 s

################################################################################
                      Learning iteration 727/50000                      

                       Computation: 135361 steps/s (collection: 0.601s, learning 0.126s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0075
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.73s
                        Total time: 576.73s
                               ETA: 650 mins 34.9 s

################################################################################
                      Learning iteration 728/50000                      

                       Computation: 135857 steps/s (collection: 0.599s, learning 0.124s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 25.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0188
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0255
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.72s
                        Total time: 577.46s
                               ETA: 650 mins 29.4 s

################################################################################
                      Learning iteration 729/50000                      

                       Computation: 124925 steps/s (collection: 0.636s, learning 0.151s)
               Value function loss: 0.1091
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0189
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0254
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.79s
                        Total time: 578.24s
                               ETA: 650 mins 28.3 s

################################################################################
                      Learning iteration 730/50000                      

                       Computation: 129769 steps/s (collection: 0.636s, learning 0.122s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0244
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.76s
                        Total time: 579.00s
                               ETA: 650 mins 25.2 s

################################################################################
                      Learning iteration 731/50000                      

                       Computation: 117707 steps/s (collection: 0.710s, learning 0.125s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.84s
                        Total time: 579.84s
                               ETA: 650 mins 27.3 s

################################################################################
                      Learning iteration 732/50000                      

                       Computation: 127759 steps/s (collection: 0.644s, learning 0.126s)
               Value function loss: 0.0952
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0041
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0247
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0077
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.77s
                        Total time: 580.61s
                               ETA: 650 mins 25.0 s

################################################################################
                      Learning iteration 733/50000                      

                       Computation: 126466 steps/s (collection: 0.654s, learning 0.124s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0249
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.78s
                        Total time: 581.38s
                               ETA: 650 mins 23.2 s

################################################################################
                      Learning iteration 734/50000                      

                       Computation: 120908 steps/s (collection: 0.682s, learning 0.131s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0251
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.81s
                        Total time: 582.20s
                               ETA: 650 mins 23.8 s

################################################################################
                      Learning iteration 735/50000                      

                       Computation: 133917 steps/s (collection: 0.612s, learning 0.122s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0247
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.73s
                        Total time: 582.93s
                               ETA: 650 mins 19.1 s

################################################################################
                      Learning iteration 736/50000                      

                       Computation: 127883 steps/s (collection: 0.645s, learning 0.124s)
               Value function loss: 0.1019
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0251
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0076
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.77s
                        Total time: 583.70s
                               ETA: 650 mins 16.8 s

################################################################################
                      Learning iteration 737/50000                      

                       Computation: 133117 steps/s (collection: 0.605s, learning 0.134s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0246
       Mean episode rew_smoothness: -0.0094
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.74s
                        Total time: 584.44s
                               ETA: 650 mins 12.4 s

################################################################################
                      Learning iteration 738/50000                      

                       Computation: 117816 steps/s (collection: 0.692s, learning 0.142s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.83s
                        Total time: 585.27s
                               ETA: 650 mins 14.4 s

################################################################################
                      Learning iteration 739/50000                      

                       Computation: 123411 steps/s (collection: 0.648s, learning 0.149s)
               Value function loss: 0.0841
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.80s
                        Total time: 586.07s
                               ETA: 650 mins 13.9 s

################################################################################
                      Learning iteration 740/50000                      

                       Computation: 131681 steps/s (collection: 0.613s, learning 0.134s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0246
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0082
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.75s
                        Total time: 586.82s
                               ETA: 650 mins 10.1 s

################################################################################
                      Learning iteration 741/50000                      

                       Computation: 131112 steps/s (collection: 0.624s, learning 0.125s)
               Value function loss: 0.0859
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0243
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.75s
                        Total time: 587.56s
                               ETA: 650 mins 6.5 s

################################################################################
                      Learning iteration 742/50000                      

                       Computation: 136692 steps/s (collection: 0.596s, learning 0.123s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0244
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.72s
                        Total time: 588.28s
                               ETA: 650 mins 0.9 s

################################################################################
                      Learning iteration 743/50000                      

                       Computation: 123341 steps/s (collection: 0.673s, learning 0.124s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0099
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0246
       Mean episode rew_smoothness: -0.0095
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0065
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.80s
                        Total time: 589.08s
                               ETA: 650 mins 0.5 s

################################################################################
                      Learning iteration 744/50000                      

                       Computation: 127643 steps/s (collection: 0.647s, learning 0.123s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0241
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.77s
                        Total time: 589.85s
                               ETA: 649 mins 58.3 s

################################################################################
                      Learning iteration 745/50000                      

                       Computation: 133683 steps/s (collection: 0.611s, learning 0.125s)
               Value function loss: 0.0839
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0248
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.74s
                        Total time: 590.59s
                               ETA: 649 mins 53.7 s

################################################################################
                      Learning iteration 746/50000                      

                       Computation: 129052 steps/s (collection: 0.640s, learning 0.122s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0244
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0082
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.76s
                        Total time: 591.35s
                               ETA: 649 mins 51.0 s

################################################################################
                      Learning iteration 747/50000                      

                       Computation: 119767 steps/s (collection: 0.687s, learning 0.134s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.82s
                        Total time: 592.17s
                               ETA: 649 mins 52.1 s

################################################################################
                      Learning iteration 748/50000                      

                       Computation: 130173 steps/s (collection: 0.631s, learning 0.124s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.76s
                        Total time: 592.92s
                               ETA: 649 mins 48.9 s

################################################################################
                      Learning iteration 749/50000                      

                       Computation: 124045 steps/s (collection: 0.661s, learning 0.132s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0240
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0006
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0079
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.79s
                        Total time: 593.72s
                               ETA: 649 mins 48.2 s

################################################################################
                      Learning iteration 750/50000                      

                       Computation: 130134 steps/s (collection: 0.605s, learning 0.151s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0250
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0066
 Mean episode rew_tracking_lin_vel: 0.0080
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.76s
                        Total time: 594.47s
                               ETA: 649 mins 45.0 s

################################################################################
                      Learning iteration 751/50000                      

                       Computation: 125428 steps/s (collection: 0.651s, learning 0.133s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0042
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0244
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.78s
                        Total time: 595.26s
                               ETA: 649 mins 43.7 s

################################################################################
                      Learning iteration 752/50000                      

                       Computation: 124700 steps/s (collection: 0.663s, learning 0.125s)
               Value function loss: 0.1040
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.79s
                        Total time: 596.04s
                               ETA: 649 mins 42.7 s

################################################################################
                      Learning iteration 753/50000                      

                       Computation: 138371 steps/s (collection: 0.586s, learning 0.124s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.71s
                        Total time: 596.75s
                               ETA: 649 mins 36.6 s

################################################################################
                      Learning iteration 754/50000                      

                       Computation: 129041 steps/s (collection: 0.638s, learning 0.124s)
               Value function loss: 0.0940
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0245
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.76s
                        Total time: 597.52s
                               ETA: 649 mins 33.9 s

################################################################################
                      Learning iteration 755/50000                      

                       Computation: 128912 steps/s (collection: 0.639s, learning 0.124s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0244
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.76s
                        Total time: 598.28s
                               ETA: 649 mins 31.2 s

################################################################################
                      Learning iteration 756/50000                      

                       Computation: 138971 steps/s (collection: 0.583s, learning 0.124s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.71s
                        Total time: 598.99s
                               ETA: 649 mins 25.0 s

################################################################################
                      Learning iteration 757/50000                      

                       Computation: 119477 steps/s (collection: 0.689s, learning 0.134s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0073
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0243
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0081
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.82s
                        Total time: 599.81s
                               ETA: 649 mins 26.2 s

################################################################################
                      Learning iteration 758/50000                      

                       Computation: 125362 steps/s (collection: 0.653s, learning 0.132s)
               Value function loss: 0.0971
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0027
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.78s
                        Total time: 600.59s
                               ETA: 649 mins 25.0 s

################################################################################
                      Learning iteration 759/50000                      

                       Computation: 133161 steps/s (collection: 0.596s, learning 0.142s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0043
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0240
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0083
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.74s
                        Total time: 601.33s
                               ETA: 649 mins 20.7 s

################################################################################
                      Learning iteration 760/50000                      

                       Computation: 123698 steps/s (collection: 0.644s, learning 0.151s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0082
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.79s
                        Total time: 602.13s
                               ETA: 649 mins 20.2 s

################################################################################
                      Learning iteration 761/50000                      

                       Computation: 121706 steps/s (collection: 0.653s, learning 0.154s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.81s
                        Total time: 602.93s
                               ETA: 649 mins 20.4 s

################################################################################
                      Learning iteration 762/50000                      

                       Computation: 117210 steps/s (collection: 0.701s, learning 0.137s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.84s
                        Total time: 603.77s
                               ETA: 649 mins 22.7 s

################################################################################
                      Learning iteration 763/50000                      

                       Computation: 128303 steps/s (collection: 0.626s, learning 0.140s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0240
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.77s
                        Total time: 604.54s
                               ETA: 649 mins 20.3 s

################################################################################
                      Learning iteration 764/50000                      

                       Computation: 125888 steps/s (collection: 0.651s, learning 0.130s)
               Value function loss: 0.0878
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.78s
                        Total time: 605.32s
                               ETA: 649 mins 18.8 s

################################################################################
                      Learning iteration 765/50000                      

                       Computation: 136217 steps/s (collection: 0.596s, learning 0.126s)
               Value function loss: 0.0887
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0240
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.72s
                        Total time: 606.04s
                               ETA: 649 mins 13.6 s

################################################################################
                      Learning iteration 766/50000                      

                       Computation: 123399 steps/s (collection: 0.669s, learning 0.127s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.80s
                        Total time: 606.84s
                               ETA: 649 mins 13.1 s

################################################################################
                      Learning iteration 767/50000                      

                       Computation: 129097 steps/s (collection: 0.629s, learning 0.132s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.76s
                        Total time: 607.60s
                               ETA: 649 mins 10.4 s

################################################################################
                      Learning iteration 768/50000                      

                       Computation: 119041 steps/s (collection: 0.668s, learning 0.157s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0241
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.83s
                        Total time: 608.43s
                               ETA: 649 mins 11.9 s

################################################################################
                      Learning iteration 769/50000                      

                       Computation: 133202 steps/s (collection: 0.615s, learning 0.123s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0067
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.74s
                        Total time: 609.16s
                               ETA: 649 mins 7.7 s

################################################################################
                      Learning iteration 770/50000                      

                       Computation: 116137 steps/s (collection: 0.717s, learning 0.130s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0240
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.85s
                        Total time: 610.01s
                               ETA: 649 mins 10.4 s

################################################################################
                      Learning iteration 771/50000                      

                       Computation: 119706 steps/s (collection: 0.680s, learning 0.142s)
               Value function loss: 0.0859
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.82s
                        Total time: 610.83s
                               ETA: 649 mins 11.5 s

################################################################################
                      Learning iteration 772/50000                      

                       Computation: 131735 steps/s (collection: 0.610s, learning 0.136s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.75s
                        Total time: 611.58s
                               ETA: 649 mins 7.9 s

################################################################################
                      Learning iteration 773/50000                      

                       Computation: 124250 steps/s (collection: 0.644s, learning 0.147s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0044
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.79s
                        Total time: 612.37s
                               ETA: 649 mins 7.1 s

################################################################################
                      Learning iteration 774/50000                      

                       Computation: 129999 steps/s (collection: 0.626s, learning 0.130s)
               Value function loss: 0.0945
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0243
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.76s
                        Total time: 613.12s
                               ETA: 649 mins 4.1 s

################################################################################
                      Learning iteration 775/50000                      

                       Computation: 125950 steps/s (collection: 0.653s, learning 0.128s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.78s
                        Total time: 613.90s
                               ETA: 649 mins 2.6 s

################################################################################
                      Learning iteration 776/50000                      

                       Computation: 119779 steps/s (collection: 0.697s, learning 0.124s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.82s
                        Total time: 614.73s
                               ETA: 649 mins 3.7 s

################################################################################
                      Learning iteration 777/50000                      

                       Computation: 133744 steps/s (collection: 0.610s, learning 0.125s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0241
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.74s
                        Total time: 615.46s
                               ETA: 648 mins 59.4 s

################################################################################
                      Learning iteration 778/50000                      

                       Computation: 129278 steps/s (collection: 0.633s, learning 0.128s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0069
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.76s
                        Total time: 616.22s
                               ETA: 648 mins 56.6 s

################################################################################
                      Learning iteration 779/50000                      

                       Computation: 125727 steps/s (collection: 0.633s, learning 0.149s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.78s
                        Total time: 617.00s
                               ETA: 648 mins 55.3 s

################################################################################
                      Learning iteration 780/50000                      

                       Computation: 131741 steps/s (collection: 0.620s, learning 0.126s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.75s
                        Total time: 617.75s
                               ETA: 648 mins 51.6 s

################################################################################
                      Learning iteration 781/50000                      

                       Computation: 123098 steps/s (collection: 0.665s, learning 0.133s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.71
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.80s
                        Total time: 618.55s
                               ETA: 648 mins 51.3 s

################################################################################
                      Learning iteration 782/50000                      

                       Computation: 132693 steps/s (collection: 0.601s, learning 0.140s)
               Value function loss: 0.0805
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.74s
                        Total time: 619.29s
                               ETA: 648 mins 47.4 s

################################################################################
                      Learning iteration 783/50000                      

                       Computation: 127021 steps/s (collection: 0.649s, learning 0.125s)
               Value function loss: 0.0696
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.77s
                        Total time: 620.06s
                               ETA: 648 mins 45.5 s

################################################################################
                      Learning iteration 784/50000                      

                       Computation: 125687 steps/s (collection: 0.660s, learning 0.122s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0098
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0012
 Mean episode rew_tracking_ang_vel: 0.0068
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.78s
                        Total time: 620.84s
                               ETA: 648 mins 44.2 s

################################################################################
                      Learning iteration 785/50000                      

                       Computation: 119011 steps/s (collection: 0.697s, learning 0.129s)
               Value function loss: 0.0714
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.83s
                        Total time: 621.67s
                               ETA: 648 mins 45.6 s

################################################################################
                      Learning iteration 786/50000                      

                       Computation: 120494 steps/s (collection: 0.683s, learning 0.132s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0084
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.82s
                        Total time: 622.49s
                               ETA: 648 mins 46.4 s

################################################################################
                      Learning iteration 787/50000                      

                       Computation: 132528 steps/s (collection: 0.615s, learning 0.127s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.43
                Mean reward (task): -5.43
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.74s
                        Total time: 623.23s
                               ETA: 648 mins 42.5 s

################################################################################
                      Learning iteration 788/50000                      

                       Computation: 131199 steps/s (collection: 0.617s, learning 0.132s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.75s
                        Total time: 623.98s
                               ETA: 648 mins 39.1 s

################################################################################
                      Learning iteration 789/50000                      

                       Computation: 119061 steps/s (collection: 0.685s, learning 0.141s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.83s
                        Total time: 624.80s
                               ETA: 648 mins 40.5 s

################################################################################
                      Learning iteration 790/50000                      

                       Computation: 125691 steps/s (collection: 0.646s, learning 0.136s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.78s
                        Total time: 625.59s
                               ETA: 648 mins 39.2 s

################################################################################
                      Learning iteration 791/50000                      

                       Computation: 136918 steps/s (collection: 0.591s, learning 0.127s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.54
                Mean reward (task): -5.54
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.72s
                        Total time: 626.30s
                               ETA: 648 mins 33.8 s

################################################################################
                      Learning iteration 792/50000                      

                       Computation: 127579 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.77s
                        Total time: 627.07s
                               ETA: 648 mins 31.8 s

################################################################################
                      Learning iteration 793/50000                      

                       Computation: 128382 steps/s (collection: 0.641s, learning 0.125s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.77s
                        Total time: 627.84s
                               ETA: 648 mins 29.4 s

################################################################################
                      Learning iteration 794/50000                      

                       Computation: 125778 steps/s (collection: 0.656s, learning 0.126s)
               Value function loss: 0.0757
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0085
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.78s
                        Total time: 628.62s
                               ETA: 648 mins 28.1 s

################################################################################
                      Learning iteration 795/50000                      

                       Computation: 130763 steps/s (collection: 0.615s, learning 0.136s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0086
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.75s
                        Total time: 629.37s
                               ETA: 648 mins 24.9 s

################################################################################
                      Learning iteration 796/50000                      

                       Computation: 126034 steps/s (collection: 0.642s, learning 0.138s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0070
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.78s
                        Total time: 630.15s
                               ETA: 648 mins 23.4 s

################################################################################
                      Learning iteration 797/50000                      

                       Computation: 127639 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.77s
                        Total time: 630.92s
                               ETA: 648 mins 21.4 s

################################################################################
                      Learning iteration 798/50000                      

                       Computation: 136187 steps/s (collection: 0.598s, learning 0.124s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.72s
                        Total time: 631.64s
                               ETA: 648 mins 16.4 s

################################################################################
                      Learning iteration 799/50000                      

                       Computation: 129823 steps/s (collection: 0.632s, learning 0.125s)
               Value function loss: 0.0736
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.76s
                        Total time: 632.40s
                               ETA: 648 mins 13.5 s

################################################################################
                      Learning iteration 800/50000                      

                       Computation: 134423 steps/s (collection: 0.606s, learning 0.125s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.73s
                        Total time: 633.13s
                               ETA: 648 mins 9.1 s

################################################################################
                      Learning iteration 801/50000                      

                       Computation: 119555 steps/s (collection: 0.697s, learning 0.125s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.11
                Mean reward (task): -5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.82s
                        Total time: 633.96s
                               ETA: 648 mins 10.2 s

################################################################################
                      Learning iteration 802/50000                      

                       Computation: 125878 steps/s (collection: 0.643s, learning 0.138s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0243
       Mean episode rew_smoothness: -0.0096
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0088
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.78s
                        Total time: 634.74s
                               ETA: 648 mins 8.9 s

################################################################################
                      Learning iteration 803/50000                      

                       Computation: 125868 steps/s (collection: 0.637s, learning 0.144s)
               Value function loss: 0.0711
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.78s
                        Total time: 635.52s
                               ETA: 648 mins 7.5 s

################################################################################
                      Learning iteration 804/50000                      

                       Computation: 136173 steps/s (collection: 0.576s, learning 0.146s)
               Value function loss: 0.0998
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.53
                Mean reward (task): -5.53
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0074
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.72s
                        Total time: 636.24s
                               ETA: 648 mins 2.5 s

################################################################################
                      Learning iteration 805/50000                      

                       Computation: 129664 steps/s (collection: 0.623s, learning 0.135s)
               Value function loss: 0.0975
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.76s
                        Total time: 637.00s
                               ETA: 647 mins 59.8 s

################################################################################
                      Learning iteration 806/50000                      

                       Computation: 129970 steps/s (collection: 0.632s, learning 0.124s)
               Value function loss: 0.0869
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.76s
                        Total time: 637.75s
                               ETA: 647 mins 56.9 s

################################################################################
                      Learning iteration 807/50000                      

                       Computation: 126634 steps/s (collection: 0.652s, learning 0.124s)
               Value function loss: 0.0872
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.78s
                        Total time: 638.53s
                               ETA: 647 mins 55.3 s

################################################################################
                      Learning iteration 808/50000                      

                       Computation: 131836 steps/s (collection: 0.620s, learning 0.125s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.75s
                        Total time: 639.28s
                               ETA: 647 mins 51.8 s

################################################################################
                      Learning iteration 809/50000                      

                       Computation: 133613 steps/s (collection: 0.612s, learning 0.124s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.74s
                        Total time: 640.01s
                               ETA: 647 mins 47.7 s

################################################################################
                      Learning iteration 810/50000                      

                       Computation: 121138 steps/s (collection: 0.688s, learning 0.123s)
               Value function loss: 0.0780
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.81s
                        Total time: 640.82s
                               ETA: 647 mins 48.2 s

################################################################################
                      Learning iteration 811/50000                      

                       Computation: 131265 steps/s (collection: 0.625s, learning 0.124s)
               Value function loss: 0.0868
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.38
                Mean reward (task): -5.38
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.75s
                        Total time: 641.57s
                               ETA: 647 mins 44.9 s

################################################################################
                      Learning iteration 812/50000                      

                       Computation: 134770 steps/s (collection: 0.603s, learning 0.126s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.73s
                        Total time: 642.30s
                               ETA: 647 mins 40.4 s

################################################################################
                      Learning iteration 813/50000                      

                       Computation: 131659 steps/s (collection: 0.616s, learning 0.131s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0242
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.75s
                        Total time: 643.05s
                               ETA: 647 mins 37.0 s

################################################################################
                      Learning iteration 814/50000                      

                       Computation: 127639 steps/s (collection: 0.622s, learning 0.148s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.77s
                        Total time: 643.82s
                               ETA: 647 mins 35.0 s

################################################################################
                      Learning iteration 815/50000                      

                       Computation: 118316 steps/s (collection: 0.674s, learning 0.157s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.83s
                        Total time: 644.65s
                               ETA: 647 mins 36.7 s

################################################################################
                      Learning iteration 816/50000                      

                       Computation: 129421 steps/s (collection: 0.636s, learning 0.124s)
               Value function loss: 0.0798
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.76s
                        Total time: 645.41s
                               ETA: 647 mins 34.1 s

################################################################################
                      Learning iteration 817/50000                      

                       Computation: 134436 steps/s (collection: 0.605s, learning 0.126s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0186
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0240
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.73s
                        Total time: 646.14s
                               ETA: 647 mins 29.8 s

################################################################################
                      Learning iteration 818/50000                      

                       Computation: 120621 steps/s (collection: 0.692s, learning 0.123s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.81s
                        Total time: 646.95s
                               ETA: 647 mins 30.5 s

################################################################################
                      Learning iteration 819/50000                      

                       Computation: 114218 steps/s (collection: 0.737s, learning 0.124s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.86s
                        Total time: 647.82s
                               ETA: 647 mins 33.9 s

################################################################################
                      Learning iteration 820/50000                      

                       Computation: 125888 steps/s (collection: 0.654s, learning 0.127s)
               Value function loss: 0.0799
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0071
 Mean episode rew_tracking_lin_vel: 0.0087
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.78s
                        Total time: 648.60s
                               ETA: 647 mins 32.6 s

################################################################################
                      Learning iteration 821/50000                      

                       Computation: 125749 steps/s (collection: 0.644s, learning 0.137s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.78s
                        Total time: 649.38s
                               ETA: 647 mins 31.3 s

################################################################################
                      Learning iteration 822/50000                      

                       Computation: 129128 steps/s (collection: 0.635s, learning 0.127s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.41
                Mean reward (task): -5.41
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.76s
                        Total time: 650.14s
                               ETA: 647 mins 28.8 s

################################################################################
                      Learning iteration 823/50000                      

                       Computation: 135160 steps/s (collection: 0.603s, learning 0.124s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0089
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.73s
                        Total time: 650.87s
                               ETA: 647 mins 24.3 s

################################################################################
                      Learning iteration 824/50000                      

                       Computation: 130827 steps/s (collection: 0.627s, learning 0.125s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 26.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0090
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.75s
                        Total time: 651.62s
                               ETA: 647 mins 21.2 s

################################################################################
                      Learning iteration 825/50000                      

                       Computation: 122112 steps/s (collection: 0.657s, learning 0.148s)
               Value function loss: 0.0651
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.81s
                        Total time: 652.42s
                               ETA: 647 mins 21.3 s

################################################################################
                      Learning iteration 826/50000                      

                       Computation: 126018 steps/s (collection: 0.637s, learning 0.143s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0073
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.78s
                        Total time: 653.20s
                               ETA: 647 mins 19.9 s

################################################################################
                      Learning iteration 827/50000                      

                       Computation: 128170 steps/s (collection: 0.642s, learning 0.125s)
               Value function loss: 0.0742
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.77s
                        Total time: 653.97s
                               ETA: 647 mins 17.8 s

################################################################################
                      Learning iteration 828/50000                      

                       Computation: 119490 steps/s (collection: 0.689s, learning 0.134s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.47
                Mean reward (task): -5.47
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.82s
                        Total time: 654.79s
                               ETA: 647 mins 18.9 s

################################################################################
                      Learning iteration 829/50000                      

                       Computation: 131103 steps/s (collection: 0.612s, learning 0.137s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.75s
                        Total time: 655.54s
                               ETA: 647 mins 15.8 s

################################################################################
                      Learning iteration 830/50000                      

                       Computation: 132024 steps/s (collection: 0.620s, learning 0.124s)
               Value function loss: 0.0863
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0187
   Mean episode rew_dof_pos_limits: -0.0045
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0244
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0072
 Mean episode rew_tracking_lin_vel: 0.0093
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.74s
                        Total time: 656.29s
                               ETA: 647 mins 12.3 s

################################################################################
                      Learning iteration 831/50000                      

                       Computation: 131999 steps/s (collection: 0.613s, learning 0.132s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.46
                Mean reward (task): -5.46
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.74s
                        Total time: 657.03s
                               ETA: 647 mins 8.9 s

################################################################################
                      Learning iteration 832/50000                      

                       Computation: 134497 steps/s (collection: 0.588s, learning 0.143s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.73s
                        Total time: 657.76s
                               ETA: 647 mins 4.6 s

################################################################################
                      Learning iteration 833/50000                      

                       Computation: 130098 steps/s (collection: 0.613s, learning 0.142s)
               Value function loss: 0.0981
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.56
                Mean reward (task): -5.56
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0183
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.76s
                        Total time: 658.52s
                               ETA: 647 mins 1.8 s

################################################################################
                      Learning iteration 834/50000                      

                       Computation: 116819 steps/s (collection: 0.697s, learning 0.144s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.84s
                        Total time: 659.36s
                               ETA: 647 mins 4.1 s

################################################################################
                      Learning iteration 835/50000                      

                       Computation: 121498 steps/s (collection: 0.675s, learning 0.134s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.81s
                        Total time: 660.17s
                               ETA: 647 mins 4.4 s

################################################################################
                      Learning iteration 836/50000                      

                       Computation: 118796 steps/s (collection: 0.687s, learning 0.141s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0075
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.83s
                        Total time: 661.00s
                               ETA: 647 mins 5.9 s

################################################################################
                      Learning iteration 837/50000                      

                       Computation: 117065 steps/s (collection: 0.686s, learning 0.153s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.84s
                        Total time: 661.84s
                               ETA: 647 mins 8.0 s

################################################################################
                      Learning iteration 838/50000                      

                       Computation: 126330 steps/s (collection: 0.655s, learning 0.123s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0075
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0046
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0244
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0074
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.78s
                        Total time: 662.61s
                               ETA: 647 mins 6.5 s

################################################################################
                      Learning iteration 839/50000                      

                       Computation: 129851 steps/s (collection: 0.626s, learning 0.131s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.36
                Mean reward (task): -5.36
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.76s
                        Total time: 663.37s
                               ETA: 647 mins 3.8 s

################################################################################
                      Learning iteration 840/50000                      

                       Computation: 129654 steps/s (collection: 0.633s, learning 0.125s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.76s
                        Total time: 664.13s
                               ETA: 647 mins 1.2 s

################################################################################
                      Learning iteration 841/50000                      

                       Computation: 118005 steps/s (collection: 0.707s, learning 0.126s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.83s
                        Total time: 664.96s
                               ETA: 647 mins 2.9 s

################################################################################
                      Learning iteration 842/50000                      

                       Computation: 133822 steps/s (collection: 0.610s, learning 0.125s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0239
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.73s
                        Total time: 665.70s
                               ETA: 646 mins 58.9 s

################################################################################
                      Learning iteration 843/50000                      

                       Computation: 130793 steps/s (collection: 0.627s, learning 0.124s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.75s
                        Total time: 666.45s
                               ETA: 646 mins 55.9 s

################################################################################
                      Learning iteration 844/50000                      

                       Computation: 129132 steps/s (collection: 0.638s, learning 0.124s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0076
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.76s
                        Total time: 667.21s
                               ETA: 646 mins 53.5 s

################################################################################
                      Learning iteration 845/50000                      

                       Computation: 137636 steps/s (collection: 0.588s, learning 0.126s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0091
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.71s
                        Total time: 667.92s
                               ETA: 646 mins 48.3 s

################################################################################
                      Learning iteration 846/50000                      

                       Computation: 122280 steps/s (collection: 0.667s, learning 0.137s)
               Value function loss: 0.0796
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0047
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0092
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.80s
                        Total time: 668.73s
                               ETA: 646 mins 48.4 s

################################################################################
                      Learning iteration 847/50000                      

                       Computation: 119860 steps/s (collection: 0.680s, learning 0.140s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.82s
                        Total time: 669.55s
                               ETA: 646 mins 49.3 s

################################################################################
                      Learning iteration 848/50000                      

                       Computation: 130271 steps/s (collection: 0.628s, learning 0.127s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.75s
                        Total time: 670.30s
                               ETA: 646 mins 46.5 s

################################################################################
                      Learning iteration 849/50000                      

                       Computation: 134672 steps/s (collection: 0.604s, learning 0.126s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.50
                Mean reward (task): -5.50
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.73s
                        Total time: 671.03s
                               ETA: 646 mins 42.3 s

################################################################################
                      Learning iteration 850/50000                      

                       Computation: 131183 steps/s (collection: 0.626s, learning 0.124s)
               Value function loss: 0.0673
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.75s
                        Total time: 671.78s
                               ETA: 646 mins 39.2 s

################################################################################
                      Learning iteration 851/50000                      

                       Computation: 127730 steps/s (collection: 0.645s, learning 0.125s)
               Value function loss: 0.0794
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 27.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.77s
                        Total time: 672.55s
                               ETA: 646 mins 37.3 s

################################################################################
                      Learning iteration 852/50000                      

                       Computation: 126486 steps/s (collection: 0.653s, learning 0.124s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0078
 Mean episode rew_tracking_lin_vel: 0.0096
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.78s
                        Total time: 673.33s
                               ETA: 646 mins 35.8 s

################################################################################
                      Learning iteration 853/50000                      

                       Computation: 127665 steps/s (collection: 0.636s, learning 0.134s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.11
                Mean reward (task): -5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.66
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0237
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0013
 Mean episode rew_tracking_ang_vel: 0.0077
 Mean episode rew_tracking_lin_vel: 0.0095
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.77s
                        Total time: 674.10s
                               ETA: 646 mins 33.9 s

################################################################################
                      Learning iteration 854/50000                      

                       Computation: 123084 steps/s (collection: 0.671s, learning 0.127s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.80s
                        Total time: 674.90s
                               ETA: 646 mins 33.6 s

################################################################################
                      Learning iteration 855/50000                      

                       Computation: 128066 steps/s (collection: 0.630s, learning 0.138s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.77s
                        Total time: 675.67s
                               ETA: 646 mins 31.6 s

################################################################################
                      Learning iteration 856/50000                      

                       Computation: 139096 steps/s (collection: 0.585s, learning 0.122s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.71s
                        Total time: 676.37s
                               ETA: 646 mins 26.0 s

################################################################################
                      Learning iteration 857/50000                      

                       Computation: 123530 steps/s (collection: 0.672s, learning 0.124s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0097
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.80s
                        Total time: 677.17s
                               ETA: 646 mins 25.6 s

################################################################################
                      Learning iteration 858/50000                      

                       Computation: 126954 steps/s (collection: 0.647s, learning 0.127s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0048
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0028
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0238
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0094
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.77s
                        Total time: 677.94s
                               ETA: 646 mins 24.0 s

################################################################################
                      Learning iteration 859/50000                      

                       Computation: 122480 steps/s (collection: 0.676s, learning 0.126s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.42
                Mean reward (task): -5.42
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0100
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.80s
                        Total time: 678.75s
                               ETA: 646 mins 24.0 s

################################################################################
                      Learning iteration 860/50000                      

                       Computation: 129595 steps/s (collection: 0.636s, learning 0.123s)
               Value function loss: 0.0826
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0185
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0241
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.76s
                        Total time: 679.50s
                               ETA: 646 mins 21.4 s

################################################################################
                      Learning iteration 861/50000                      

                       Computation: 119841 steps/s (collection: 0.697s, learning 0.124s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.82s
                        Total time: 680.32s
                               ETA: 646 mins 22.4 s

################################################################################
                      Learning iteration 862/50000                      

                       Computation: 124701 steps/s (collection: 0.665s, learning 0.123s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.79s
                        Total time: 681.11s
                               ETA: 646 mins 21.6 s

################################################################################
                      Learning iteration 863/50000                      

                       Computation: 128705 steps/s (collection: 0.637s, learning 0.127s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0235
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.76s
                        Total time: 681.88s
                               ETA: 646 mins 19.3 s

################################################################################
                      Learning iteration 864/50000                      

                       Computation: 123277 steps/s (collection: 0.674s, learning 0.123s)
               Value function loss: 0.0856
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.80s
                        Total time: 682.67s
                               ETA: 646 mins 19.0 s

################################################################################
                      Learning iteration 865/50000                      

                       Computation: 126302 steps/s (collection: 0.654s, learning 0.125s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0230
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0098
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.78s
                        Total time: 683.45s
                               ETA: 646 mins 17.6 s

################################################################################
                      Learning iteration 866/50000                      

                       Computation: 134806 steps/s (collection: 0.605s, learning 0.124s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.73s
                        Total time: 684.18s
                               ETA: 646 mins 13.4 s

################################################################################
                      Learning iteration 867/50000                      

                       Computation: 134549 steps/s (collection: 0.592s, learning 0.139s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.51
                Mean reward (task): -5.51
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0099
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.73s
                        Total time: 684.91s
                               ETA: 646 mins 9.3 s

################################################################################
                      Learning iteration 868/50000                      

                       Computation: 129876 steps/s (collection: 0.618s, learning 0.139s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0029
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.76s
                        Total time: 685.67s
                               ETA: 646 mins 6.7 s

################################################################################
                      Learning iteration 869/50000                      

                       Computation: 131802 steps/s (collection: 0.617s, learning 0.129s)
               Value function loss: 0.0866
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0076
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0230
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.75s
                        Total time: 686.41s
                               ETA: 646 mins 3.5 s

################################################################################
                      Learning iteration 870/50000                      

                       Computation: 122254 steps/s (collection: 0.680s, learning 0.124s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0101
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.80s
                        Total time: 687.22s
                               ETA: 646 mins 3.5 s

################################################################################
                      Learning iteration 871/50000                      

                       Computation: 122203 steps/s (collection: 0.680s, learning 0.125s)
               Value function loss: 0.0886
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0236
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0104
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.80s
                        Total time: 688.02s
                               ETA: 646 mins 3.6 s

################################################################################
                      Learning iteration 872/50000                      

                       Computation: 134974 steps/s (collection: 0.603s, learning 0.126s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0230
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0079
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.73s
                        Total time: 688.75s
                               ETA: 645 mins 59.4 s

################################################################################
                      Learning iteration 873/50000                      

                       Computation: 137111 steps/s (collection: 0.592s, learning 0.125s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.72s
                        Total time: 689.47s
                               ETA: 645 mins 54.6 s

################################################################################
                      Learning iteration 874/50000                      

                       Computation: 126274 steps/s (collection: 0.655s, learning 0.123s)
               Value function loss: 0.0679
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0049
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.78s
                        Total time: 690.25s
                               ETA: 645 mins 53.2 s

################################################################################
                      Learning iteration 875/50000                      

                       Computation: 121705 steps/s (collection: 0.684s, learning 0.124s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.81s
                        Total time: 691.05s
                               ETA: 645 mins 53.5 s

################################################################################
                      Learning iteration 876/50000                      

                       Computation: 133028 steps/s (collection: 0.605s, learning 0.134s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.74s
                        Total time: 691.79s
                               ETA: 645 mins 49.9 s

################################################################################
                      Learning iteration 877/50000                      

                       Computation: 132839 steps/s (collection: 0.615s, learning 0.125s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0233
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0080
 Mean episode rew_tracking_lin_vel: 0.0104
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.74s
                        Total time: 692.53s
                               ETA: 645 mins 46.4 s

################################################################################
                      Learning iteration 878/50000                      

                       Computation: 127540 steps/s (collection: 0.642s, learning 0.129s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0232
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.77s
                        Total time: 693.30s
                               ETA: 645 mins 44.6 s

################################################################################
                      Learning iteration 879/50000                      

                       Computation: 112688 steps/s (collection: 0.745s, learning 0.128s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.11
                Mean reward (task): -5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.87s
                        Total time: 694.18s
                               ETA: 645 mins 48.5 s

################################################################################
                      Learning iteration 880/50000                      

                       Computation: 130911 steps/s (collection: 0.627s, learning 0.123s)
               Value function loss: 0.0703
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0097
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.75s
                        Total time: 694.93s
                               ETA: 645 mins 45.6 s

################################################################################
                      Learning iteration 881/50000                      

                       Computation: 120722 steps/s (collection: 0.666s, learning 0.149s)
               Value function loss: 0.0745
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0050
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.81s
                        Total time: 695.74s
                               ETA: 645 mins 46.2 s

################################################################################
                      Learning iteration 882/50000                      

                       Computation: 117523 steps/s (collection: 0.713s, learning 0.124s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0103
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.84s
                        Total time: 696.58s
                               ETA: 645 mins 48.0 s

################################################################################
                      Learning iteration 883/50000                      

                       Computation: 127546 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0105
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.77s
                        Total time: 697.35s
                               ETA: 645 mins 46.2 s

################################################################################
                      Learning iteration 884/50000                      

                       Computation: 123442 steps/s (collection: 0.657s, learning 0.140s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.80s
                        Total time: 698.15s
                               ETA: 645 mins 45.9 s

################################################################################
                      Learning iteration 885/50000                      

                       Computation: 120479 steps/s (collection: 0.674s, learning 0.142s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.82s
                        Total time: 698.96s
                               ETA: 645 mins 46.6 s

################################################################################
                      Learning iteration 886/50000                      

                       Computation: 121209 steps/s (collection: 0.663s, learning 0.148s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.81s
                        Total time: 699.77s
                               ETA: 645 mins 47.0 s

################################################################################
                      Learning iteration 887/50000                      

                       Computation: 131957 steps/s (collection: 0.621s, learning 0.124s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.74s
                        Total time: 700.52s
                               ETA: 645 mins 43.8 s

################################################################################
                      Learning iteration 888/50000                      

                       Computation: 121943 steps/s (collection: 0.683s, learning 0.123s)
               Value function loss: 0.0678
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.81s
                        Total time: 701.32s
                               ETA: 645 mins 44.0 s

################################################################################
                      Learning iteration 889/50000                      

                       Computation: 132199 steps/s (collection: 0.621s, learning 0.123s)
               Value function loss: 0.0685
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0106
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.74s
                        Total time: 702.07s
                               ETA: 645 mins 40.7 s

################################################################################
                      Learning iteration 890/50000                      

                       Computation: 135431 steps/s (collection: 0.599s, learning 0.127s)
               Value function loss: 0.0669
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.30
                Mean reward (task): -5.30
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0105
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.73s
                        Total time: 702.79s
                               ETA: 645 mins 36.4 s

################################################################################
                      Learning iteration 891/50000                      

                       Computation: 118061 steps/s (collection: 0.699s, learning 0.134s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.83s
                        Total time: 703.63s
                               ETA: 645 mins 38.0 s

################################################################################
                      Learning iteration 892/50000                      

                       Computation: 128712 steps/s (collection: 0.640s, learning 0.123s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.34
                Mean reward (task): -5.34
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0102
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.76s
                        Total time: 704.39s
                               ETA: 645 mins 35.9 s

################################################################################
                      Learning iteration 893/50000                      

                       Computation: 120694 steps/s (collection: 0.670s, learning 0.144s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0077
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.81s
                        Total time: 705.20s
                               ETA: 645 mins 36.5 s

################################################################################
                      Learning iteration 894/50000                      

                       Computation: 119696 steps/s (collection: 0.675s, learning 0.146s)
               Value function loss: 0.0678
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.82s
                        Total time: 706.02s
                               ETA: 645 mins 37.5 s

################################################################################
                      Learning iteration 895/50000                      

                       Computation: 128247 steps/s (collection: 0.642s, learning 0.125s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0223
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0110
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.77s
                        Total time: 706.79s
                               ETA: 645 mins 35.5 s

################################################################################
                      Learning iteration 896/50000                      

                       Computation: 126294 steps/s (collection: 0.654s, learning 0.124s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.78s
                        Total time: 707.57s
                               ETA: 645 mins 34.1 s

################################################################################
                      Learning iteration 897/50000                      

                       Computation: 129787 steps/s (collection: 0.632s, learning 0.125s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.76s
                        Total time: 708.33s
                               ETA: 645 mins 31.6 s

################################################################################
                      Learning iteration 898/50000                      

                       Computation: 138740 steps/s (collection: 0.586s, learning 0.123s)
               Value function loss: 0.0662
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0104
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.71s
                        Total time: 709.04s
                               ETA: 645 mins 26.4 s

################################################################################
                      Learning iteration 899/50000                      

                       Computation: 133459 steps/s (collection: 0.612s, learning 0.125s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0051
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0081
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.74s
                        Total time: 709.77s
                               ETA: 645 mins 22.8 s

################################################################################
                      Learning iteration 900/50000                      

                       Computation: 120226 steps/s (collection: 0.686s, learning 0.132s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0105
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.82s
                        Total time: 710.59s
                               ETA: 645 mins 23.6 s

################################################################################
                      Learning iteration 901/50000                      

                       Computation: 132214 steps/s (collection: 0.620s, learning 0.124s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.74s
                        Total time: 711.33s
                               ETA: 645 mins 20.4 s

################################################################################
                      Learning iteration 902/50000                      

                       Computation: 127913 steps/s (collection: 0.630s, learning 0.138s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.77s
                        Total time: 712.10s
                               ETA: 645 mins 18.5 s

################################################################################
                      Learning iteration 903/50000                      

                       Computation: 132788 steps/s (collection: 0.598s, learning 0.142s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.11
                Mean reward (task): -5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0105
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.74s
                        Total time: 712.84s
                               ETA: 645 mins 15.1 s

################################################################################
                      Learning iteration 904/50000                      

                       Computation: 128308 steps/s (collection: 0.622s, learning 0.144s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0108
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.77s
                        Total time: 713.61s
                               ETA: 645 mins 13.1 s

################################################################################
                      Learning iteration 905/50000                      

                       Computation: 122797 steps/s (collection: 0.644s, learning 0.157s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0078
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0097
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.80s
                        Total time: 714.41s
                               ETA: 645 mins 12.9 s

################################################################################
                      Learning iteration 906/50000                      

                       Computation: 124212 steps/s (collection: 0.655s, learning 0.136s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.79s
                        Total time: 715.20s
                               ETA: 645 mins 12.3 s

################################################################################
                      Learning iteration 907/50000                      

                       Computation: 124631 steps/s (collection: 0.662s, learning 0.127s)
               Value function loss: 0.0865
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0082
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.79s
                        Total time: 715.99s
                               ETA: 645 mins 11.5 s

################################################################################
                      Learning iteration 908/50000                      

                       Computation: 124358 steps/s (collection: 0.664s, learning 0.127s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0110
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.79s
                        Total time: 716.78s
                               ETA: 645 mins 10.8 s

################################################################################
                      Learning iteration 909/50000                      

                       Computation: 132821 steps/s (collection: 0.598s, learning 0.142s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.27
                Mean reward (task): -5.27
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.74s
                        Total time: 717.52s
                               ETA: 645 mins 7.4 s

################################################################################
                      Learning iteration 910/50000                      

                       Computation: 130381 steps/s (collection: 0.607s, learning 0.147s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0098
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.75s
                        Total time: 718.27s
                               ETA: 645 mins 4.8 s

################################################################################
                      Learning iteration 911/50000                      

                       Computation: 119226 steps/s (collection: 0.699s, learning 0.125s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.82s
                        Total time: 719.10s
                               ETA: 645 mins 5.9 s

################################################################################
                      Learning iteration 912/50000                      

                       Computation: 124162 steps/s (collection: 0.665s, learning 0.127s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 28.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0052
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0223
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0083
 Mean episode rew_tracking_lin_vel: 0.0107
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.79s
                        Total time: 719.89s
                               ETA: 645 mins 5.3 s

################################################################################
                      Learning iteration 913/50000                      

                       Computation: 126649 steps/s (collection: 0.642s, learning 0.134s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.33
                Mean reward (task): -5.33
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0110
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.78s
                        Total time: 720.67s
                               ETA: 645 mins 3.9 s

################################################################################
                      Learning iteration 914/50000                      

                       Computation: 123503 steps/s (collection: 0.647s, learning 0.149s)
               Value function loss: 0.0860
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.80s
                        Total time: 721.46s
                               ETA: 645 mins 3.5 s

################################################################################
                      Learning iteration 915/50000                      

                       Computation: 132873 steps/s (collection: 0.606s, learning 0.134s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0109
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.74s
                        Total time: 722.20s
                               ETA: 645 mins 0.1 s

################################################################################
                      Learning iteration 916/50000                      

                       Computation: 115531 steps/s (collection: 0.717s, learning 0.134s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0030
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0111
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.85s
                        Total time: 723.05s
                               ETA: 645 mins 2.6 s

################################################################################
                      Learning iteration 917/50000                      

                       Computation: 121665 steps/s (collection: 0.664s, learning 0.144s)
               Value function loss: 0.0763
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.81s
                        Total time: 723.86s
                               ETA: 645 mins 2.9 s

################################################################################
                      Learning iteration 918/50000                      

                       Computation: 127759 steps/s (collection: 0.643s, learning 0.127s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.77s
                        Total time: 724.63s
                               ETA: 645 mins 1.1 s

################################################################################
                      Learning iteration 919/50000                      

                       Computation: 124159 steps/s (collection: 0.667s, learning 0.124s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.79s
                        Total time: 725.42s
                               ETA: 645 mins 0.5 s

################################################################################
                      Learning iteration 920/50000                      

                       Computation: 136078 steps/s (collection: 0.599s, learning 0.124s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.72s
                        Total time: 726.14s
                               ETA: 644 mins 56.2 s

################################################################################
                      Learning iteration 921/50000                      

                       Computation: 129596 steps/s (collection: 0.631s, learning 0.128s)
               Value function loss: 0.0819
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.94
                Mean reward (task): -4.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0079
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0099
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.76s
                        Total time: 726.90s
                               ETA: 644 mins 53.8 s

################################################################################
                      Learning iteration 922/50000                      

                       Computation: 131359 steps/s (collection: 0.625s, learning 0.123s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.75s
                        Total time: 727.65s
                               ETA: 644 mins 50.9 s

################################################################################
                      Learning iteration 923/50000                      

                       Computation: 127682 steps/s (collection: 0.647s, learning 0.122s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.77s
                        Total time: 728.42s
                               ETA: 644 mins 49.1 s

################################################################################
                      Learning iteration 924/50000                      

                       Computation: 130865 steps/s (collection: 0.628s, learning 0.123s)
               Value function loss: 0.0681
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0080
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.75s
                        Total time: 729.17s
                               ETA: 644 mins 46.3 s

################################################################################
                      Learning iteration 925/50000                      

                       Computation: 133104 steps/s (collection: 0.614s, learning 0.125s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.44
                Mean reward (task): -5.44
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.74s
                        Total time: 729.91s
                               ETA: 644 mins 42.9 s

################################################################################
                      Learning iteration 926/50000                      

                       Computation: 127459 steps/s (collection: 0.637s, learning 0.134s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0231
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.77s
                        Total time: 730.68s
                               ETA: 644 mins 41.2 s

################################################################################
                      Learning iteration 927/50000                      

                       Computation: 124626 steps/s (collection: 0.666s, learning 0.123s)
               Value function loss: 0.0876
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0112
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.79s
                        Total time: 731.47s
                               ETA: 644 mins 40.5 s

################################################################################
                      Learning iteration 928/50000                      

                       Computation: 132080 steps/s (collection: 0.614s, learning 0.130s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0081
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0014
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.74s
                        Total time: 732.22s
                               ETA: 644 mins 37.4 s

################################################################################
                      Learning iteration 929/50000                      

                       Computation: 125476 steps/s (collection: 0.641s, learning 0.142s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.78s
                        Total time: 733.00s
                               ETA: 644 mins 36.3 s

################################################################################
                      Learning iteration 930/50000                      

                       Computation: 127495 steps/s (collection: 0.632s, learning 0.139s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0234
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.77s
                        Total time: 733.77s
                               ETA: 644 mins 34.6 s

################################################################################
                      Learning iteration 931/50000                      

                       Computation: 115984 steps/s (collection: 0.704s, learning 0.144s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0082
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.85s
                        Total time: 734.62s
                               ETA: 644 mins 37.0 s

################################################################################
                      Learning iteration 932/50000                      

                       Computation: 129222 steps/s (collection: 0.621s, learning 0.140s)
               Value function loss: 0.0741
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.76s
                        Total time: 735.38s
                               ETA: 644 mins 34.7 s

################################################################################
                      Learning iteration 933/50000                      

                       Computation: 130166 steps/s (collection: 0.602s, learning 0.153s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.76s
                        Total time: 736.13s
                               ETA: 644 mins 32.2 s

################################################################################
                      Learning iteration 934/50000                      

                       Computation: 132591 steps/s (collection: 0.615s, learning 0.126s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.74s
                        Total time: 736.87s
                               ETA: 644 mins 29.0 s

################################################################################
                      Learning iteration 935/50000                      

                       Computation: 129391 steps/s (collection: 0.634s, learning 0.126s)
               Value function loss: 0.0750
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.76s
                        Total time: 737.63s
                               ETA: 644 mins 26.7 s

################################################################################
                      Learning iteration 936/50000                      

                       Computation: 127052 steps/s (collection: 0.651s, learning 0.122s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.77s
                        Total time: 738.41s
                               ETA: 644 mins 25.2 s

################################################################################
                      Learning iteration 937/50000                      

                       Computation: 129670 steps/s (collection: 0.634s, learning 0.124s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.76s
                        Total time: 739.17s
                               ETA: 644 mins 22.8 s

################################################################################
                      Learning iteration 938/50000                      

                       Computation: 126932 steps/s (collection: 0.651s, learning 0.123s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0110
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.77s
                        Total time: 739.94s
                               ETA: 644 mins 21.3 s

################################################################################
                      Learning iteration 939/50000                      

                       Computation: 136348 steps/s (collection: 0.597s, learning 0.124s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.72s
                        Total time: 740.66s
                               ETA: 644 mins 17.0 s

################################################################################
                      Learning iteration 940/50000                      

                       Computation: 134965 steps/s (collection: 0.604s, learning 0.124s)
               Value function loss: 0.0913
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.73s
                        Total time: 741.39s
                               ETA: 644 mins 13.1 s

################################################################################
                      Learning iteration 941/50000                      

                       Computation: 134578 steps/s (collection: 0.606s, learning 0.124s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0012
          Mean episode rew_dof_acc: -0.0184
   Mean episode rew_dof_pos_limits: -0.0053
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0230
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.73s
                        Total time: 742.12s
                               ETA: 644 mins 9.4 s

################################################################################
                      Learning iteration 942/50000                      

                       Computation: 138118 steps/s (collection: 0.588s, learning 0.124s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.71s
                        Total time: 742.83s
                               ETA: 644 mins 4.6 s

################################################################################
                      Learning iteration 943/50000                      

                       Computation: 131018 steps/s (collection: 0.626s, learning 0.124s)
               Value function loss: 0.0942
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.75s
                        Total time: 743.58s
                               ETA: 644 mins 1.9 s

################################################################################
                      Learning iteration 944/50000                      

                       Computation: 129864 steps/s (collection: 0.615s, learning 0.142s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0230
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.76s
                        Total time: 744.34s
                               ETA: 643 mins 59.5 s

################################################################################
                      Learning iteration 945/50000                      

                       Computation: 110410 steps/s (collection: 0.724s, learning 0.166s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.89s
                        Total time: 745.23s
                               ETA: 644 mins 4.0 s

################################################################################
                      Learning iteration 946/50000                      

                       Computation: 137138 steps/s (collection: 0.592s, learning 0.125s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.72s
                        Total time: 745.95s
                               ETA: 643 mins 59.6 s

################################################################################
                      Learning iteration 947/50000                      

                       Computation: 118657 steps/s (collection: 0.704s, learning 0.124s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0182
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0230
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.83s
                        Total time: 746.78s
                               ETA: 644 mins 0.9 s

################################################################################
                      Learning iteration 948/50000                      

                       Computation: 131767 steps/s (collection: 0.604s, learning 0.142s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.11
                Mean reward (task): -5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0179
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.75s
                        Total time: 747.52s
                               ETA: 643 mins 57.9 s

################################################################################
                      Learning iteration 949/50000                      

                       Computation: 133347 steps/s (collection: 0.602s, learning 0.136s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.25
                Mean reward (task): -5.25
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.74s
                        Total time: 748.26s
                               ETA: 643 mins 54.6 s

################################################################################
                      Learning iteration 950/50000                      

                       Computation: 132742 steps/s (collection: 0.615s, learning 0.126s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0090
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.74s
                        Total time: 749.00s
                               ETA: 643 mins 51.3 s

################################################################################
                      Learning iteration 951/50000                      

                       Computation: 130567 steps/s (collection: 0.630s, learning 0.123s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.75s
                        Total time: 749.75s
                               ETA: 643 mins 48.8 s

################################################################################
                      Learning iteration 952/50000                      

                       Computation: 132084 steps/s (collection: 0.621s, learning 0.123s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0100
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.74s
                        Total time: 750.50s
                               ETA: 643 mins 45.7 s

################################################################################
                      Learning iteration 953/50000                      

                       Computation: 120971 steps/s (collection: 0.678s, learning 0.135s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0101
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.81s
                        Total time: 751.31s
                               ETA: 643 mins 46.3 s

################################################################################
                      Learning iteration 954/50000                      

                       Computation: 135566 steps/s (collection: 0.603s, learning 0.122s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.35
                Mean reward (task): -5.35
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.73s
                        Total time: 752.03s
                               ETA: 643 mins 42.3 s

################################################################################
                      Learning iteration 955/50000                      

                       Computation: 127911 steps/s (collection: 0.638s, learning 0.131s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.77s
                        Total time: 752.80s
                               ETA: 643 mins 40.5 s

################################################################################
                      Learning iteration 956/50000                      

                       Computation: 136667 steps/s (collection: 0.595s, learning 0.124s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.11
                Mean reward (task): -5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0115
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.72s
                        Total time: 753.52s
                               ETA: 643 mins 36.2 s

################################################################################
                      Learning iteration 957/50000                      

                       Computation: 131453 steps/s (collection: 0.623s, learning 0.124s)
               Value function loss: 0.0701
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.75s
                        Total time: 754.27s
                               ETA: 643 mins 33.4 s

################################################################################
                      Learning iteration 958/50000                      

                       Computation: 122023 steps/s (collection: 0.650s, learning 0.156s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.81s
                        Total time: 755.08s
                               ETA: 643 mins 33.6 s

################################################################################
                      Learning iteration 959/50000                      

                       Computation: 124485 steps/s (collection: 0.642s, learning 0.147s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0007
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.79s
                        Total time: 755.86s
                               ETA: 643 mins 32.9 s

################################################################################
                      Learning iteration 960/50000                      

                       Computation: 134004 steps/s (collection: 0.602s, learning 0.132s)
               Value function loss: 0.0841
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0102
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.73s
                        Total time: 756.60s
                               ETA: 643 mins 29.4 s

################################################################################
                      Learning iteration 961/50000                      

                       Computation: 135115 steps/s (collection: 0.600s, learning 0.128s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0230
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.73s
                        Total time: 757.33s
                               ETA: 643 mins 25.5 s

################################################################################
                      Learning iteration 962/50000                      

                       Computation: 132536 steps/s (collection: 0.599s, learning 0.143s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0084
 Mean episode rew_tracking_lin_vel: 0.0113
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.74s
                        Total time: 758.07s
                               ETA: 643 mins 22.4 s

################################################################################
                      Learning iteration 963/50000                      

                       Computation: 129635 steps/s (collection: 0.634s, learning 0.124s)
               Value function loss: 0.0735
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0229
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.76s
                        Total time: 758.83s
                               ETA: 643 mins 20.2 s

################################################################################
                      Learning iteration 964/50000                      

                       Computation: 136299 steps/s (collection: 0.597s, learning 0.124s)
               Value function loss: 0.0668
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.72s
                        Total time: 759.55s
                               ETA: 643 mins 16.0 s

################################################################################
                      Learning iteration 965/50000                      

                       Computation: 130250 steps/s (collection: 0.627s, learning 0.127s)
               Value function loss: 0.0690
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0114
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.75s
                        Total time: 760.30s
                               ETA: 643 mins 13.6 s

################################################################################
                      Learning iteration 966/50000                      

                       Computation: 137668 steps/s (collection: 0.590s, learning 0.124s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.71s
                        Total time: 761.02s
                               ETA: 643 mins 9.1 s

################################################################################
                      Learning iteration 967/50000                      

                       Computation: 117564 steps/s (collection: 0.706s, learning 0.130s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.84s
                        Total time: 761.85s
                               ETA: 643 mins 10.8 s

################################################################################
                      Learning iteration 968/50000                      

                       Computation: 135720 steps/s (collection: 0.598s, learning 0.126s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0083
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0031
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0103
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0085
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.72s
                        Total time: 762.58s
                               ETA: 643 mins 6.8 s

################################################################################
                      Learning iteration 969/50000                      

                       Computation: 120654 steps/s (collection: 0.687s, learning 0.127s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0054
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.81s
                        Total time: 763.39s
                               ETA: 643 mins 7.5 s

################################################################################
                      Learning iteration 970/50000                      

                       Computation: 129069 steps/s (collection: 0.610s, learning 0.152s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0084
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0223
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.76s
                        Total time: 764.15s
                               ETA: 643 mins 5.4 s

################################################################################
                      Learning iteration 971/50000                      

                       Computation: 120050 steps/s (collection: 0.686s, learning 0.133s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.82s
                        Total time: 764.97s
                               ETA: 643 mins 6.2 s

################################################################################
                      Learning iteration 972/50000                      

                       Computation: 117528 steps/s (collection: 0.700s, learning 0.137s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0181
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0228
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.84s
                        Total time: 765.81s
                               ETA: 643 mins 7.9 s

################################################################################
                      Learning iteration 973/50000                      

                       Computation: 130270 steps/s (collection: 0.630s, learning 0.124s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0086
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.75s
                        Total time: 766.56s
                               ETA: 643 mins 5.5 s

################################################################################
                      Learning iteration 974/50000                      

                       Computation: 132307 steps/s (collection: 0.613s, learning 0.130s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.74s
                        Total time: 767.31s
                               ETA: 643 mins 2.5 s

################################################################################
                      Learning iteration 975/50000                      

                       Computation: 127897 steps/s (collection: 0.631s, learning 0.137s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0223
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0117
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.77s
                        Total time: 768.07s
                               ETA: 643 mins 0.8 s

################################################################################
                      Learning iteration 976/50000                      

                       Computation: 121546 steps/s (collection: 0.670s, learning 0.139s)
               Value function loss: 0.0674
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.32
                Mean reward (task): -5.32
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.81s
                        Total time: 768.88s
                               ETA: 643 mins 1.1 s

################################################################################
                      Learning iteration 977/50000                      

                       Computation: 125172 steps/s (collection: 0.644s, learning 0.141s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.79s
                        Total time: 769.67s
                               ETA: 643 mins 0.2 s

################################################################################
                      Learning iteration 978/50000                      

                       Computation: 129372 steps/s (collection: 0.626s, learning 0.134s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0055
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0087
 Mean episode rew_tracking_lin_vel: 0.0116
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.76s
                        Total time: 770.43s
                               ETA: 642 mins 58.1 s

################################################################################
                      Learning iteration 979/50000                      

                       Computation: 119093 steps/s (collection: 0.665s, learning 0.161s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0223
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.83s
                        Total time: 771.25s
                               ETA: 642 mins 59.2 s

################################################################################
                      Learning iteration 980/50000                      

                       Computation: 119199 steps/s (collection: 0.682s, learning 0.143s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.82s
                        Total time: 772.08s
                               ETA: 643 mins 0.3 s

################################################################################
                      Learning iteration 981/50000                      

                       Computation: 115700 steps/s (collection: 0.707s, learning 0.143s)
               Value function loss: 0.0716
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.85s
                        Total time: 772.93s
                               ETA: 643 mins 2.7 s

################################################################################
                      Learning iteration 982/50000                      

                       Computation: 132324 steps/s (collection: 0.600s, learning 0.143s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.74s
                        Total time: 773.67s
                               ETA: 642 mins 59.7 s

################################################################################
                      Learning iteration 983/50000                      

                       Computation: 121614 steps/s (collection: 0.669s, learning 0.140s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.81s
                        Total time: 774.48s
                               ETA: 642 mins 59.9 s

################################################################################
                      Learning iteration 984/50000                      

                       Computation: 120192 steps/s (collection: 0.683s, learning 0.135s)
               Value function loss: 0.0797
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.82s
                        Total time: 775.30s
                               ETA: 643 mins 0.7 s

################################################################################
                      Learning iteration 985/50000                      

                       Computation: 122374 steps/s (collection: 0.680s, learning 0.123s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.80s
                        Total time: 776.10s
                               ETA: 643 mins 0.7 s

################################################################################
                      Learning iteration 986/50000                      

                       Computation: 118564 steps/s (collection: 0.706s, learning 0.123s)
               Value function loss: 0.0667
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0090
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.83s
                        Total time: 776.93s
                               ETA: 643 mins 2.0 s

################################################################################
                      Learning iteration 987/50000                      

                       Computation: 130750 steps/s (collection: 0.613s, learning 0.139s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.75s
                        Total time: 777.68s
                               ETA: 642 mins 59.5 s

################################################################################
                      Learning iteration 988/50000                      

                       Computation: 125389 steps/s (collection: 0.637s, learning 0.147s)
               Value function loss: 0.0780
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0089
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.78s
                        Total time: 778.47s
                               ETA: 642 mins 58.5 s

################################################################################
                      Learning iteration 989/50000                      

                       Computation: 118105 steps/s (collection: 0.708s, learning 0.124s)
               Value function loss: 0.0758
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.83s
                        Total time: 779.30s
                               ETA: 642 mins 60.0 s

################################################################################
                      Learning iteration 990/50000                      

                       Computation: 134674 steps/s (collection: 0.604s, learning 0.126s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.73s
                        Total time: 780.03s
                               ETA: 642 mins 56.4 s

################################################################################
                      Learning iteration 991/50000                      

                       Computation: 129273 steps/s (collection: 0.636s, learning 0.124s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.76s
                        Total time: 780.79s
                               ETA: 642 mins 54.2 s

################################################################################
                      Learning iteration 992/50000                      

                       Computation: 127514 steps/s (collection: 0.648s, learning 0.123s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0227
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0088
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.77s
                        Total time: 781.56s
                               ETA: 642 mins 52.7 s

################################################################################
                      Learning iteration 993/50000                      

                       Computation: 128664 steps/s (collection: 0.610s, learning 0.154s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.76s
                        Total time: 782.32s
                               ETA: 642 mins 50.7 s

################################################################################
                      Learning iteration 994/50000                      

                       Computation: 121556 steps/s (collection: 0.677s, learning 0.131s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.81s
                        Total time: 783.13s
                               ETA: 642 mins 51.0 s

################################################################################
                      Learning iteration 995/50000                      

                       Computation: 129357 steps/s (collection: 0.635s, learning 0.125s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.76s
                        Total time: 783.89s
                               ETA: 642 mins 48.9 s

################################################################################
                      Learning iteration 996/50000                      

                       Computation: 124981 steps/s (collection: 0.662s, learning 0.125s)
               Value function loss: 0.0766
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.79s
                        Total time: 784.68s
                               ETA: 642 mins 48.1 s

################################################################################
                      Learning iteration 997/50000                      

                       Computation: 130044 steps/s (collection: 0.633s, learning 0.123s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.76s
                        Total time: 785.43s
                               ETA: 642 mins 45.8 s

################################################################################
                      Learning iteration 998/50000                      

                       Computation: 115228 steps/s (collection: 0.707s, learning 0.146s)
               Value function loss: 0.0644
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.85s
                        Total time: 786.29s
                               ETA: 642 mins 48.2 s

################################################################################
                      Learning iteration 999/50000                      

                       Computation: 108472 steps/s (collection: 0.757s, learning 0.150s)
               Value function loss: 0.0892
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0130
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.91s
                        Total time: 787.19s
                               ETA: 642 mins 53.3 s

################################################################################
                     Learning iteration 1000/50000                      

                       Computation: 125387 steps/s (collection: 0.652s, learning 0.132s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 0.78s
                        Total time: 787.98s
                               ETA: 642 mins 52.3 s

################################################################################
                     Learning iteration 1001/50000                      

                       Computation: 115621 steps/s (collection: 0.720s, learning 0.130s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.28
                Mean reward (task): -5.28
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 0.85s
                        Total time: 788.83s
                               ETA: 642 mins 54.6 s

################################################################################
                     Learning iteration 1002/50000                      

                       Computation: 113279 steps/s (collection: 0.724s, learning 0.144s)
               Value function loss: 0.0770
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0119
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 0.87s
                        Total time: 789.70s
                               ETA: 642 mins 57.8 s

################################################################################
                     Learning iteration 1003/50000                      

                       Computation: 105700 steps/s (collection: 0.778s, learning 0.152s)
               Value function loss: 0.0954
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 0.93s
                        Total time: 790.63s
                               ETA: 643 mins 4.0 s

################################################################################
                     Learning iteration 1004/50000                      

                       Computation: 113830 steps/s (collection: 0.701s, learning 0.162s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0121
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 0.86s
                        Total time: 791.49s
                               ETA: 643 mins 6.9 s

################################################################################
                     Learning iteration 1005/50000                      

                       Computation: 111842 steps/s (collection: 0.720s, learning 0.159s)
               Value function loss: 0.0674
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 0.88s
                        Total time: 792.37s
                               ETA: 643 mins 10.5 s

################################################################################
                     Learning iteration 1006/50000                      

                       Computation: 106548 steps/s (collection: 0.765s, learning 0.158s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0086
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.22
                Mean reward (task): -5.22
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 0.92s
                        Total time: 793.29s
                               ETA: 643 mins 16.3 s

################################################################################
                     Learning iteration 1007/50000                      

                       Computation: 104930 steps/s (collection: 0.770s, learning 0.167s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 0.94s
                        Total time: 794.23s
                               ETA: 643 mins 22.8 s

################################################################################
                     Learning iteration 1008/50000                      

                       Computation: 118737 steps/s (collection: 0.685s, learning 0.143s)
               Value function loss: 0.0880
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0180
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0015
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0122
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 0.83s
                        Total time: 795.06s
                               ETA: 643 mins 23.9 s

################################################################################
                     Learning iteration 1009/50000                      

                       Computation: 109535 steps/s (collection: 0.741s, learning 0.156s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.90s
                        Total time: 795.95s
                               ETA: 643 mins 28.5 s

################################################################################
                     Learning iteration 1010/50000                      

                       Computation: 112432 steps/s (collection: 0.730s, learning 0.145s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0089
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.49
                Mean reward (task): -5.49
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0118
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.87s
                        Total time: 796.83s
                               ETA: 643 mins 31.9 s

################################################################################
                     Learning iteration 1011/50000                      

                       Computation: 110363 steps/s (collection: 0.725s, learning 0.166s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0225
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.89s
                        Total time: 797.72s
                               ETA: 643 mins 36.0 s

################################################################################
                     Learning iteration 1012/50000                      

                       Computation: 105650 steps/s (collection: 0.773s, learning 0.158s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.31
                Mean reward (task): -5.31
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0056
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0120
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.93s
                        Total time: 798.65s
                               ETA: 643 mins 42.1 s

################################################################################
                     Learning iteration 1013/50000                      

                       Computation: 109193 steps/s (collection: 0.723s, learning 0.177s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0224
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.90s
                        Total time: 799.55s
                               ETA: 643 mins 46.7 s

################################################################################
                     Learning iteration 1014/50000                      

                       Computation: 114196 steps/s (collection: 0.711s, learning 0.149s)
               Value function loss: 0.0712
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 29.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0123
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.86s
                        Total time: 800.41s
                               ETA: 643 mins 49.4 s

################################################################################
                     Learning iteration 1015/50000                      

                       Computation: 105532 steps/s (collection: 0.774s, learning 0.157s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0178
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0226
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0130
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.93s
                        Total time: 801.34s
                               ETA: 643 mins 55.5 s

################################################################################
                     Learning iteration 1016/50000                      

                       Computation: 114760 steps/s (collection: 0.725s, learning 0.131s)
               Value function loss: 0.0768
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0222
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.86s
                        Total time: 802.20s
                               ETA: 643 mins 58.0 s

################################################################################
                     Learning iteration 1017/50000                      

                       Computation: 112139 steps/s (collection: 0.728s, learning 0.148s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0221
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0131
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 0.88s
                        Total time: 803.07s
                               ETA: 644 mins 1.5 s

################################################################################
                     Learning iteration 1018/50000                      

                       Computation: 128225 steps/s (collection: 0.643s, learning 0.123s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.77s
                        Total time: 803.84s
                               ETA: 643 mins 59.6 s

################################################################################
                     Learning iteration 1019/50000                      

                       Computation: 108500 steps/s (collection: 0.748s, learning 0.158s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0086
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.91s
                        Total time: 804.75s
                               ETA: 644 mins 4.4 s

################################################################################
                     Learning iteration 1020/50000                      

                       Computation: 132937 steps/s (collection: 0.600s, learning 0.140s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.23
                Mean reward (task): -5.23
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0057
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.74s
                        Total time: 805.49s
                               ETA: 644 mins 1.3 s

################################################################################
                     Learning iteration 1021/50000                      

                       Computation: 124610 steps/s (collection: 0.657s, learning 0.132s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0174
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.79s
                        Total time: 806.28s
                               ETA: 644 mins 0.5 s

################################################################################
                     Learning iteration 1022/50000                      

                       Computation: 129488 steps/s (collection: 0.631s, learning 0.128s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0218
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0128
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.76s
                        Total time: 807.03s
                               ETA: 643 mins 58.3 s

################################################################################
                     Learning iteration 1023/50000                      

                       Computation: 122507 steps/s (collection: 0.675s, learning 0.127s)
               Value function loss: 0.0761
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.80s
                        Total time: 807.84s
                               ETA: 643 mins 58.1 s

################################################################################
                     Learning iteration 1024/50000                      

                       Computation: 123000 steps/s (collection: 0.669s, learning 0.130s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.37
                Mean reward (task): -5.37
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.80s
                        Total time: 808.64s
                               ETA: 643 mins 57.8 s

################################################################################
                     Learning iteration 1025/50000                      

                       Computation: 127106 steps/s (collection: 0.643s, learning 0.130s)
               Value function loss: 0.0819
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0129
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.77s
                        Total time: 809.41s
                               ETA: 643 mins 56.3 s

################################################################################
                     Learning iteration 1026/50000                      

                       Computation: 118454 steps/s (collection: 0.695s, learning 0.135s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.24
                Mean reward (task): -5.24
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0177
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.83s
                        Total time: 810.24s
                               ETA: 643 mins 57.5 s

################################################################################
                     Learning iteration 1027/50000                      

                       Computation: 113861 steps/s (collection: 0.732s, learning 0.132s)
               Value function loss: 0.0678
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0131
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.86s
                        Total time: 811.10s
                               ETA: 644 mins 0.2 s

################################################################################
                     Learning iteration 1028/50000                      

                       Computation: 119811 steps/s (collection: 0.691s, learning 0.129s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0058
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0220
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0091
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.82s
                        Total time: 811.92s
                               ETA: 644 mins 0.9 s

################################################################################
                     Learning iteration 1029/50000                      

                       Computation: 125010 steps/s (collection: 0.664s, learning 0.123s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0173
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0124
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.79s
                        Total time: 812.71s
                               ETA: 644 mins 0.0 s

################################################################################
                     Learning iteration 1030/50000                      

                       Computation: 18938 steps/s (collection: 5.047s, learning 0.144s)
               Value function loss: 0.0778
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0176
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0217
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0092
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 5.19s
                        Total time: 817.90s
                               ETA: 647 mins 28.3 s

################################################################################
                     Learning iteration 1031/50000                      

                       Computation: 80928 steps/s (collection: 1.075s, learning 0.139s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.21s
                        Total time: 819.12s
                               ETA: 647 mins 47.5 s

################################################################################
                     Learning iteration 1032/50000                      

                       Computation: 72349 steps/s (collection: 1.197s, learning 0.162s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0131
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 1.36s
                        Total time: 820.47s
                               ETA: 648 mins 13.5 s

################################################################################
                     Learning iteration 1033/50000                      

                       Computation: 82582 steps/s (collection: 1.051s, learning 0.139s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0032
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0126
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 1.19s
                        Total time: 821.66s
                               ETA: 648 mins 31.5 s

################################################################################
                     Learning iteration 1034/50000                      

                       Computation: 80908 steps/s (collection: 1.072s, learning 0.143s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0210
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0125
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 1.22s
                        Total time: 822.88s
                               ETA: 648 mins 50.5 s

################################################################################
                     Learning iteration 1035/50000                      

                       Computation: 81910 steps/s (collection: 1.059s, learning 0.142s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0130
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 1.20s
                        Total time: 824.08s
                               ETA: 649 mins 8.9 s

################################################################################
                     Learning iteration 1036/50000                      

                       Computation: 75995 steps/s (collection: 1.124s, learning 0.170s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0214
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0128
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 1.29s
                        Total time: 825.37s
                               ETA: 649 mins 31.6 s

################################################################################
                     Learning iteration 1037/50000                      

                       Computation: 77395 steps/s (collection: 1.117s, learning 0.153s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 1.27s
                        Total time: 826.64s
                               ETA: 649 mins 53.2 s

################################################################################
                     Learning iteration 1038/50000                      

                       Computation: 83609 steps/s (collection: 1.034s, learning 0.142s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.39
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0093
 Mean episode rew_tracking_lin_vel: 0.0130
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 1.18s
                        Total time: 827.82s
                               ETA: 650 mins 10.3 s

################################################################################
                     Learning iteration 1039/50000                      

                       Computation: 95207 steps/s (collection: 0.897s, learning 0.136s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0127
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 1.03s
                        Total time: 828.85s
                               ETA: 650 mins 20.6 s

################################################################################
                     Learning iteration 1040/50000                      

                       Computation: 129642 steps/s (collection: 0.635s, learning 0.124s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.76s
                        Total time: 829.61s
                               ETA: 650 mins 18.0 s

################################################################################
                     Learning iteration 1041/50000                      

                       Computation: 121955 steps/s (collection: 0.681s, learning 0.125s)
               Value function loss: 0.0958
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.70
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0213
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0133
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.81s
                        Total time: 830.42s
                               ETA: 650 mins 17.6 s

################################################################################
                     Learning iteration 1042/50000                      

                       Computation: 118493 steps/s (collection: 0.663s, learning 0.167s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.06
                Mean reward (task): -5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0059
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0215
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0131
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.83s
                        Total time: 831.25s
                               ETA: 650 mins 18.3 s

################################################################################
                     Learning iteration 1043/50000                      

                       Computation: 115484 steps/s (collection: 0.711s, learning 0.140s)
               Value function loss: 0.0630
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0210
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0094
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.85s
                        Total time: 832.10s
                               ETA: 650 mins 20.1 s

################################################################################
                     Learning iteration 1044/50000                      

                       Computation: 130435 steps/s (collection: 0.627s, learning 0.127s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.75s
                        Total time: 832.85s
                               ETA: 650 mins 17.2 s

################################################################################
                     Learning iteration 1045/50000                      

                       Computation: 135079 steps/s (collection: 0.605s, learning 0.122s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0136
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.73s
                        Total time: 833.58s
                               ETA: 650 mins 13.2 s

################################################################################
                     Learning iteration 1046/50000                      

                       Computation: 112867 steps/s (collection: 0.747s, learning 0.124s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0205
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0096
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.87s
                        Total time: 834.45s
                               ETA: 650 mins 15.9 s

################################################################################
                     Learning iteration 1047/50000                      

                       Computation: 129037 steps/s (collection: 0.624s, learning 0.138s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0129
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.76s
                        Total time: 835.21s
                               ETA: 650 mins 13.4 s

################################################################################
                     Learning iteration 1048/50000                      

                       Computation: 132617 steps/s (collection: 0.608s, learning 0.133s)
               Value function loss: 0.0764
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0095
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.74s
                        Total time: 835.95s
                               ETA: 650 mins 10.0 s

################################################################################
                     Learning iteration 1049/50000                      

                       Computation: 135475 steps/s (collection: 0.602s, learning 0.124s)
               Value function loss: 0.0669
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0209
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.73s
                        Total time: 836.68s
                               ETA: 650 mins 5.9 s

################################################################################
                     Learning iteration 1050/50000                      

                       Computation: 125536 steps/s (collection: 0.654s, learning 0.129s)
               Value function loss: 0.0775
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0138
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.78s
                        Total time: 837.46s
                               ETA: 650 mins 4.5 s

################################################################################
                     Learning iteration 1051/50000                      

                       Computation: 121929 steps/s (collection: 0.682s, learning 0.124s)
               Value function loss: 0.0654
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0206
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.81s
                        Total time: 838.27s
                               ETA: 650 mins 4.1 s

################################################################################
                     Learning iteration 1052/50000                      

                       Computation: 114871 steps/s (collection: 0.716s, learning 0.140s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0175
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0219
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.86s
                        Total time: 839.12s
                               ETA: 650 mins 6.1 s

################################################################################
                     Learning iteration 1053/50000                      

                       Computation: 129350 steps/s (collection: 0.637s, learning 0.123s)
               Value function loss: 0.0824
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0209
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.76s
                        Total time: 839.88s
                               ETA: 650 mins 3.6 s

################################################################################
                     Learning iteration 1054/50000                      

                       Computation: 112770 steps/s (collection: 0.745s, learning 0.127s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0088
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0210
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.87s
                        Total time: 840.75s
                               ETA: 650 mins 6.2 s

################################################################################
                     Learning iteration 1055/50000                      

                       Computation: 127823 steps/s (collection: 0.619s, learning 0.150s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0212
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.77s
                        Total time: 841.52s
                               ETA: 650 mins 4.1 s

################################################################################
                     Learning iteration 1056/50000                      

                       Computation: 137007 steps/s (collection: 0.593s, learning 0.125s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.40
                Mean reward (task): -5.40
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 30.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0033
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0132
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.72s
                        Total time: 842.24s
                               ETA: 649 mins 59.7 s

################################################################################
                     Learning iteration 1057/50000                      

                       Computation: 137289 steps/s (collection: 0.578s, learning 0.138s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0206
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0134
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.72s
                        Total time: 842.96s
                               ETA: 649 mins 55.1 s

################################################################################
                     Learning iteration 1058/50000                      

                       Computation: 110410 steps/s (collection: 0.725s, learning 0.165s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0060
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0142
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.89s
                        Total time: 843.85s
                               ETA: 649 mins 58.7 s

################################################################################
                     Learning iteration 1059/50000                      

                       Computation: 127501 steps/s (collection: 0.645s, learning 0.126s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0216
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.77s
                        Total time: 844.62s
                               ETA: 649 mins 56.7 s

################################################################################
                     Learning iteration 1060/50000                      

                       Computation: 135976 steps/s (collection: 0.600s, learning 0.123s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.26
                Mean reward (task): -5.26
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.72s
                        Total time: 845.34s
                               ETA: 649 mins 52.5 s

################################################################################
                     Learning iteration 1061/50000                      

                       Computation: 124967 steps/s (collection: 0.647s, learning 0.140s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0171
   Mean episode rew_dof_pos_limits: -0.0061
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.79s
                        Total time: 846.13s
                               ETA: 649 mins 51.2 s

################################################################################
                     Learning iteration 1062/50000                      

                       Computation: 132178 steps/s (collection: 0.621s, learning 0.123s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0212
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.74s
                        Total time: 846.87s
                               ETA: 649 mins 48.0 s

################################################################################
                     Learning iteration 1063/50000                      

                       Computation: 127111 steps/s (collection: 0.631s, learning 0.142s)
               Value function loss: 0.0644
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0203
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.77s
                        Total time: 847.65s
                               ETA: 649 mins 46.1 s

################################################################################
                     Learning iteration 1064/50000                      

                       Computation: 130274 steps/s (collection: 0.628s, learning 0.127s)
               Value function loss: 0.0738
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.75s
                        Total time: 848.40s
                               ETA: 649 mins 43.4 s

################################################################################
                     Learning iteration 1065/50000                      

                       Computation: 128710 steps/s (collection: 0.637s, learning 0.126s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0135
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.76s
                        Total time: 849.16s
                               ETA: 649 mins 41.1 s

################################################################################
                     Learning iteration 1066/50000                      

                       Computation: 117133 steps/s (collection: 0.702s, learning 0.138s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0207
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.84s
                        Total time: 850.00s
                               ETA: 649 mins 42.2 s

################################################################################
                     Learning iteration 1067/50000                      

                       Computation: 125176 steps/s (collection: 0.660s, learning 0.125s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0167
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0205
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0097
 Mean episode rew_tracking_lin_vel: 0.0137
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.79s
                        Total time: 850.79s
                               ETA: 649 mins 40.9 s

################################################################################
                     Learning iteration 1068/50000                      

                       Computation: 122606 steps/s (collection: 0.654s, learning 0.148s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0170
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0206
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0140
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.80s
                        Total time: 851.59s
                               ETA: 649 mins 40.4 s

################################################################################
                     Learning iteration 1069/50000                      

                       Computation: 127820 steps/s (collection: 0.630s, learning 0.140s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.77s
                        Total time: 852.36s
                               ETA: 649 mins 38.3 s

################################################################################
                     Learning iteration 1070/50000                      

                       Computation: 127669 steps/s (collection: 0.646s, learning 0.124s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0203
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.77s
                        Total time: 853.13s
                               ETA: 649 mins 36.3 s

################################################################################
                     Learning iteration 1071/50000                      

                       Computation: 127185 steps/s (collection: 0.651s, learning 0.122s)
               Value function loss: 0.0651
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0169
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0208
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.77s
                        Total time: 853.90s
                               ETA: 649 mins 34.4 s

################################################################################
                     Learning iteration 1072/50000                      

                       Computation: 129054 steps/s (collection: 0.621s, learning 0.141s)
               Value function loss: 0.0670
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.76s
                        Total time: 854.66s
                               ETA: 649 mins 32.0 s

################################################################################
                     Learning iteration 1073/50000                      

                       Computation: 118179 steps/s (collection: 0.694s, learning 0.138s)
               Value function loss: 0.0634
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 31.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0062
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0098
 Mean episode rew_tracking_lin_vel: 0.0141
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.83s
                        Total time: 855.50s
                               ETA: 649 mins 32.8 s

################################################################################
                     Learning iteration 1074/50000                      

                       Computation: 131109 steps/s (collection: 0.627s, learning 0.123s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0020
Mean episode rew_power_distribution: -0.0203
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0099
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.75s
                        Total time: 856.25s
                               ETA: 649 mins 29.9 s

################################################################################
                     Learning iteration 1075/50000                      

                       Computation: 119954 steps/s (collection: 0.696s, learning 0.123s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0139
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.82s
                        Total time: 857.06s
                               ETA: 649 mins 30.2 s

################################################################################
                     Learning iteration 1076/50000                      

                       Computation: 134702 steps/s (collection: 0.605s, learning 0.125s)
               Value function loss: 0.0577
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0020
Mean episode rew_power_distribution: -0.0199
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0151
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.73s
                        Total time: 857.79s
                               ETA: 649 mins 26.3 s

################################################################################
                     Learning iteration 1077/50000                      

                       Computation: 114433 steps/s (collection: 0.721s, learning 0.138s)
               Value function loss: 0.0530
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.86s
                        Total time: 858.65s
                               ETA: 649 mins 28.4 s

################################################################################
                     Learning iteration 1078/50000                      

                       Computation: 113382 steps/s (collection: 0.720s, learning 0.147s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0143
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.87s
                        Total time: 859.52s
                               ETA: 649 mins 30.8 s

################################################################################
                     Learning iteration 1079/50000                      

                       Computation: 131101 steps/s (collection: 0.626s, learning 0.124s)
               Value function loss: 0.0648
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0203
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0149
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.75s
                        Total time: 860.27s
                               ETA: 649 mins 27.9 s

################################################################################
                     Learning iteration 1080/50000                      

                       Computation: 127191 steps/s (collection: 0.650s, learning 0.123s)
               Value function loss: 0.0643
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.77s
                        Total time: 861.04s
                               ETA: 649 mins 26.0 s

################################################################################
                     Learning iteration 1081/50000                      

                       Computation: 122663 steps/s (collection: 0.677s, learning 0.124s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.80s
                        Total time: 861.84s
                               ETA: 649 mins 25.4 s

################################################################################
                     Learning iteration 1082/50000                      

                       Computation: 123433 steps/s (collection: 0.669s, learning 0.127s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.80s
                        Total time: 862.64s
                               ETA: 649 mins 24.6 s

################################################################################
                     Learning iteration 1083/50000                      

                       Computation: 140887 steps/s (collection: 0.574s, learning 0.124s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0172
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0211
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0144
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.70s
                        Total time: 863.34s
                               ETA: 649 mins 19.4 s

################################################################################
                     Learning iteration 1084/50000                      

                       Computation: 115355 steps/s (collection: 0.703s, learning 0.149s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0148
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.85s
                        Total time: 864.19s
                               ETA: 649 mins 21.1 s

################################################################################
                     Learning iteration 1085/50000                      

                       Computation: 120302 steps/s (collection: 0.673s, learning 0.144s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0202
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0100
 Mean episode rew_tracking_lin_vel: 0.0148
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.82s
                        Total time: 865.01s
                               ETA: 649 mins 21.2 s

################################################################################
                     Learning iteration 1086/50000                      

                       Computation: 124439 steps/s (collection: 0.627s, learning 0.163s)
               Value function loss: 0.0601
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0148
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.79s
                        Total time: 865.80s
                               ETA: 649 mins 20.1 s

################################################################################
                     Learning iteration 1087/50000                      

                       Computation: 134285 steps/s (collection: 0.607s, learning 0.125s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0199
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.73s
                        Total time: 866.53s
                               ETA: 649 mins 16.4 s

################################################################################
                     Learning iteration 1088/50000                      

                       Computation: 129717 steps/s (collection: 0.634s, learning 0.124s)
               Value function loss: 0.0729
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0168
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0202
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0147
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.76s
                        Total time: 867.29s
                               ETA: 649 mins 13.9 s

################################################################################
                     Learning iteration 1089/50000                      

                       Computation: 130047 steps/s (collection: 0.626s, learning 0.130s)
               Value function loss: 0.0601
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0196
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0148
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.76s
                        Total time: 868.04s
                               ETA: 649 mins 11.3 s

################################################################################
                     Learning iteration 1090/50000                      

                       Computation: 130373 steps/s (collection: 0.630s, learning 0.124s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.48
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0201
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0102
 Mean episode rew_tracking_lin_vel: 0.0150
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.75s
                        Total time: 868.80s
                               ETA: 649 mins 8.6 s

################################################################################
                     Learning iteration 1091/50000                      

                       Computation: 129705 steps/s (collection: 0.621s, learning 0.136s)
               Value function loss: 0.0604
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0194
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.76s
                        Total time: 869.56s
                               ETA: 649 mins 6.1 s

################################################################################
                     Learning iteration 1092/50000                      

                       Computation: 113754 steps/s (collection: 0.733s, learning 0.131s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0195
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.86s
                        Total time: 870.42s
                               ETA: 649 mins 8.3 s

################################################################################
                     Learning iteration 1093/50000                      

                       Computation: 135051 steps/s (collection: 0.591s, learning 0.137s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.20
                Mean reward (task): -5.20
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.06
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0166
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0204
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0145
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.73s
                        Total time: 871.15s
                               ETA: 649 mins 4.5 s

################################################################################
                     Learning iteration 1094/50000                      

                       Computation: 131508 steps/s (collection: 0.623s, learning 0.124s)
               Value function loss: 0.0672
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0193
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.75s
                        Total time: 871.90s
                               ETA: 649 mins 1.5 s

################################################################################
                     Learning iteration 1095/50000                      

                       Computation: 121676 steps/s (collection: 0.665s, learning 0.143s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.69
               Mean reward (total): -5.39
                Mean reward (task): -5.39
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0165
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.81s
                        Total time: 872.70s
                               ETA: 649 mins 1.2 s

################################################################################
                     Learning iteration 1096/50000                      

                       Computation: 117434 steps/s (collection: 0.695s, learning 0.143s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.06
                Mean reward (task): -5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.84s
                        Total time: 873.54s
                               ETA: 649 mins 2.2 s

################################################################################
                     Learning iteration 1097/50000                      

                       Computation: 130114 steps/s (collection: 0.615s, learning 0.140s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0063
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0199
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0146
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.76s
                        Total time: 874.30s
                               ETA: 648 mins 59.6 s

################################################################################
                     Learning iteration 1098/50000                      

                       Computation: 122004 steps/s (collection: 0.646s, learning 0.160s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0195
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0151
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.81s
                        Total time: 875.10s
                               ETA: 648 mins 59.2 s

################################################################################
                     Learning iteration 1099/50000                      

                       Computation: 115672 steps/s (collection: 0.667s, learning 0.183s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0197
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0104
 Mean episode rew_tracking_lin_vel: 0.0150
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.85s
                        Total time: 875.95s
                               ETA: 649 mins 0.8 s

################################################################################
                     Learning iteration 1100/50000                      

                       Computation: 133223 steps/s (collection: 0.600s, learning 0.138s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0064
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0199
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0016
 Mean episode rew_tracking_ang_vel: 0.0103
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.74s
                        Total time: 876.69s
                               ETA: 648 mins 57.4 s

################################################################################
                     Learning iteration 1101/50000                      

                       Computation: 117286 steps/s (collection: 0.704s, learning 0.134s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.25
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0192
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0157
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.84s
                        Total time: 877.53s
                               ETA: 648 mins 58.5 s

################################################################################
                     Learning iteration 1102/50000                      

                       Computation: 121063 steps/s (collection: 0.672s, learning 0.140s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0196
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0105
 Mean episode rew_tracking_lin_vel: 0.0152
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.81s
                        Total time: 878.34s
                               ETA: 648 mins 58.4 s

################################################################################
                     Learning iteration 1103/50000                      

                       Computation: 114067 steps/s (collection: 0.732s, learning 0.129s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0193
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.86s
                        Total time: 879.20s
                               ETA: 649 mins 0.5 s

################################################################################
                     Learning iteration 1104/50000                      

                       Computation: 125219 steps/s (collection: 0.656s, learning 0.129s)
               Value function loss: 0.0617
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0164
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0195
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0156
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.79s
                        Total time: 879.99s
                               ETA: 648 mins 59.2 s

################################################################################
                     Learning iteration 1105/50000                      

                       Computation: 125048 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0034
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0193
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0149
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.79s
                        Total time: 880.77s
                               ETA: 648 mins 58.0 s

################################################################################
                     Learning iteration 1106/50000                      

                       Computation: 133027 steps/s (collection: 0.612s, learning 0.127s)
               Value function loss: 0.0625
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0197
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.74s
                        Total time: 881.51s
                               ETA: 648 mins 54.6 s

################################################################################
                     Learning iteration 1107/50000                      

                       Computation: 130255 steps/s (collection: 0.623s, learning 0.132s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0196
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0106
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.75s
                        Total time: 882.27s
                               ETA: 648 mins 52.0 s

################################################################################
                     Learning iteration 1108/50000                      

                       Computation: 114897 steps/s (collection: 0.716s, learning 0.140s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0163
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0199
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.86s
                        Total time: 883.12s
                               ETA: 648 mins 53.8 s

################################################################################
                     Learning iteration 1109/50000                      

                       Computation: 133522 steps/s (collection: 0.601s, learning 0.136s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0194
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.74s
                        Total time: 883.86s
                               ETA: 648 mins 50.4 s

################################################################################
                     Learning iteration 1110/50000                      

                       Computation: 123353 steps/s (collection: 0.668s, learning 0.129s)
               Value function loss: 0.0622
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.06
                Mean reward (task): -5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0199
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0154
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.80s
                        Total time: 884.66s
                               ETA: 648 mins 49.6 s

################################################################################
                     Learning iteration 1111/50000                      

                       Computation: 127225 steps/s (collection: 0.634s, learning 0.139s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0065
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0200
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0150
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.77s
                        Total time: 885.43s
                               ETA: 648 mins 47.8 s

################################################################################
                     Learning iteration 1112/50000                      

                       Computation: 121431 steps/s (collection: 0.686s, learning 0.124s)
               Value function loss: 0.0560
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0191
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0156
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.81s
                        Total time: 886.24s
                               ETA: 648 mins 47.6 s

################################################################################
                     Learning iteration 1113/50000                      

                       Computation: 129139 steps/s (collection: 0.636s, learning 0.125s)
               Value function loss: 0.0644
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0199
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.76s
                        Total time: 887.00s
                               ETA: 648 mins 45.2 s

################################################################################
                     Learning iteration 1114/50000                      

                       Computation: 120342 steps/s (collection: 0.664s, learning 0.153s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0192
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.82s
                        Total time: 887.82s
                               ETA: 648 mins 45.3 s

################################################################################
                     Learning iteration 1115/50000                      

                       Computation: 118173 steps/s (collection: 0.710s, learning 0.122s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0193
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0107
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.83s
                        Total time: 888.65s
                               ETA: 648 mins 46.1 s

################################################################################
                     Learning iteration 1116/50000                      

                       Computation: 129068 steps/s (collection: 0.619s, learning 0.143s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.60
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0194
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.76s
                        Total time: 889.41s
                               ETA: 648 mins 43.8 s

################################################################################
                     Learning iteration 1117/50000                      

                       Computation: 129922 steps/s (collection: 0.632s, learning 0.124s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0196
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.76s
                        Total time: 890.17s
                               ETA: 648 mins 41.3 s

################################################################################
                     Learning iteration 1118/50000                      

                       Computation: 128500 steps/s (collection: 0.643s, learning 0.122s)
               Value function loss: 0.0621
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0191
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0155
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.77s
                        Total time: 890.93s
                               ETA: 648 mins 39.1 s

################################################################################
                     Learning iteration 1119/50000                      

                       Computation: 138171 steps/s (collection: 0.587s, learning 0.124s)
               Value function loss: 0.0617
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0194
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.71s
                        Total time: 891.64s
                               ETA: 648 mins 34.6 s

################################################################################
                     Learning iteration 1120/50000                      

                       Computation: 118749 steps/s (collection: 0.680s, learning 0.148s)
               Value function loss: 0.0607
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0157
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.83s
                        Total time: 892.47s
                               ETA: 648 mins 35.2 s

################################################################################
                     Learning iteration 1121/50000                      

                       Computation: 118197 steps/s (collection: 0.694s, learning 0.137s)
               Value function loss: 0.0590
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0191
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.83s
                        Total time: 893.30s
                               ETA: 648 mins 35.9 s

################################################################################
                     Learning iteration 1122/50000                      

                       Computation: 122370 steps/s (collection: 0.660s, learning 0.144s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0087
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0191
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0156
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.80s
                        Total time: 894.10s
                               ETA: 648 mins 35.5 s

################################################################################
                     Learning iteration 1123/50000                      

                       Computation: 131075 steps/s (collection: 0.613s, learning 0.137s)
               Value function loss: 0.0586
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0066
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0195
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0108
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.75s
                        Total time: 894.85s
                               ETA: 648 mins 32.6 s

################################################################################
                     Learning iteration 1124/50000                      

                       Computation: 131878 steps/s (collection: 0.622s, learning 0.124s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.94
                Mean reward (task): -4.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0035
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0190
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.75s
                        Total time: 895.60s
                               ETA: 648 mins 29.6 s

################################################################################
                     Learning iteration 1125/50000                      

                       Computation: 121841 steps/s (collection: 0.675s, learning 0.132s)
               Value function loss: 0.0619
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0188
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.81s
                        Total time: 896.41s
                               ETA: 648 mins 29.3 s

################################################################################
                     Learning iteration 1126/50000                      

                       Computation: 132318 steps/s (collection: 0.619s, learning 0.124s)
               Value function loss: 0.0617
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0191
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0153
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.74s
                        Total time: 897.15s
                               ETA: 648 mins 26.2 s

################################################################################
                     Learning iteration 1127/50000                      

                       Computation: 132951 steps/s (collection: 0.617s, learning 0.122s)
               Value function loss: 0.0641
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.74s
                        Total time: 897.89s
                               ETA: 648 mins 23.0 s

################################################################################
                     Learning iteration 1128/50000                      

                       Computation: 136172 steps/s (collection: 0.586s, learning 0.135s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0192
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0159
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.72s
                        Total time: 898.61s
                               ETA: 648 mins 19.0 s

################################################################################
                     Learning iteration 1129/50000                      

                       Computation: 118384 steps/s (collection: 0.698s, learning 0.132s)
               Value function loss: 0.0657
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0067
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0188
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0109
 Mean episode rew_tracking_lin_vel: 0.0157
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.83s
                        Total time: 899.44s
                               ETA: 648 mins 19.7 s

################################################################################
                     Learning iteration 1130/50000                      

                       Computation: 125034 steps/s (collection: 0.663s, learning 0.123s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0162
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.79s
                        Total time: 900.23s
                               ETA: 648 mins 18.4 s

################################################################################
                     Learning iteration 1131/50000                      

                       Computation: 126062 steps/s (collection: 0.637s, learning 0.143s)
               Value function loss: 0.0575
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0194
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0163
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.78s
                        Total time: 901.01s
                               ETA: 648 mins 16.9 s

################################################################################
                     Learning iteration 1132/50000                      

                       Computation: 124338 steps/s (collection: 0.634s, learning 0.157s)
               Value function loss: 0.0644
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0088
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0190
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.79s
                        Total time: 901.80s
                               ETA: 648 mins 15.9 s

################################################################################
                     Learning iteration 1133/50000                      

                       Computation: 133363 steps/s (collection: 0.609s, learning 0.128s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0068
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0188
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0158
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.74s
                        Total time: 902.54s
                               ETA: 648 mins 12.6 s

################################################################################
                     Learning iteration 1134/50000                      

                       Computation: 131449 steps/s (collection: 0.624s, learning 0.123s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 32.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.75s
                        Total time: 903.28s
                               ETA: 648 mins 9.7 s

################################################################################
                     Learning iteration 1135/50000                      

                       Computation: 122642 steps/s (collection: 0.678s, learning 0.123s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0192
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0110
 Mean episode rew_tracking_lin_vel: 0.0164
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.80s
                        Total time: 904.08s
                               ETA: 648 mins 9.2 s

################################################################################
                     Learning iteration 1136/50000                      

                       Computation: 127459 steps/s (collection: 0.634s, learning 0.137s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.77s
                        Total time: 904.86s
                               ETA: 648 mins 7.3 s

################################################################################
                     Learning iteration 1137/50000                      

                       Computation: 112746 steps/s (collection: 0.705s, learning 0.167s)
               Value function loss: 0.1017
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.24
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.87s
                        Total time: 905.73s
                               ETA: 648 mins 9.8 s

################################################################################
                     Learning iteration 1138/50000                      

                       Computation: 122632 steps/s (collection: 0.666s, learning 0.135s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.80s
                        Total time: 906.53s
                               ETA: 648 mins 9.2 s

################################################################################
                     Learning iteration 1139/50000                      

                       Computation: 134637 steps/s (collection: 0.606s, learning 0.124s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0104
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.73s
                        Total time: 907.26s
                               ETA: 648 mins 5.6 s

################################################################################
                     Learning iteration 1140/50000                      

                       Computation: 113451 steps/s (collection: 0.730s, learning 0.137s)
               Value function loss: 0.0678
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.68
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0169
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.87s
                        Total time: 908.13s
                               ETA: 648 mins 7.9 s

################################################################################
                     Learning iteration 1141/50000                      

                       Computation: 128439 steps/s (collection: 0.636s, learning 0.130s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.87
                Mean reward (task): -4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.74
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0089
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.77s
                        Total time: 908.89s
                               ETA: 648 mins 5.8 s

################################################################################
                     Learning iteration 1142/50000                      

                       Computation: 129186 steps/s (collection: 0.636s, learning 0.125s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.76s
                        Total time: 909.65s
                               ETA: 648 mins 3.5 s

################################################################################
                     Learning iteration 1143/50000                      

                       Computation: 132709 steps/s (collection: 0.601s, learning 0.140s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0040
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0089
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.74s
                        Total time: 910.39s
                               ETA: 648 mins 0.3 s

################################################################################
                     Learning iteration 1144/50000                      

                       Computation: 131615 steps/s (collection: 0.622s, learning 0.125s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.94
                Mean reward (task): -4.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0069
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0111
 Mean episode rew_tracking_lin_vel: 0.0160
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.75s
                        Total time: 911.14s
                               ETA: 647 mins 57.4 s

################################################################################
                     Learning iteration 1145/50000                      

                       Computation: 112191 steps/s (collection: 0.754s, learning 0.122s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0089
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0165
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.88s
                        Total time: 912.02s
                               ETA: 648 mins 0.1 s

################################################################################
                     Learning iteration 1146/50000                      

                       Computation: 132469 steps/s (collection: 0.619s, learning 0.123s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.94
                Mean reward (task): -4.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0149
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.74s
                        Total time: 912.76s
                               ETA: 647 mins 57.0 s

################################################################################
                     Learning iteration 1147/50000                      

                       Computation: 129046 steps/s (collection: 0.638s, learning 0.124s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.47
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0192
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.76s
                        Total time: 913.52s
                               ETA: 647 mins 54.7 s

################################################################################
                     Learning iteration 1148/50000                      

                       Computation: 124799 steps/s (collection: 0.662s, learning 0.126s)
               Value function loss: 0.0608
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.42
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.79s
                        Total time: 914.31s
                               ETA: 647 mins 53.6 s

################################################################################
                     Learning iteration 1149/50000                      

                       Computation: 118892 steps/s (collection: 0.698s, learning 0.129s)
               Value function loss: 0.0523
                    Surrogate loss: -0.0030
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 0.83s
                        Total time: 915.13s
                               ETA: 647 mins 54.1 s

################################################################################
                     Learning iteration 1150/50000                      

                       Computation: 126764 steps/s (collection: 0.653s, learning 0.122s)
               Value function loss: 0.0572
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0089
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.78s
                        Total time: 915.91s
                               ETA: 647 mins 52.5 s

################################################################################
                     Learning iteration 1151/50000                      

                       Computation: 136433 steps/s (collection: 0.595s, learning 0.125s)
               Value function loss: 0.0554
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.84
                Mean reward (task): -4.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0169
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.72s
                        Total time: 916.63s
                               ETA: 647 mins 48.5 s

################################################################################
                     Learning iteration 1152/50000                      

                       Computation: 121050 steps/s (collection: 0.683s, learning 0.129s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.81s
                        Total time: 917.44s
                               ETA: 647 mins 48.4 s

################################################################################
                     Learning iteration 1153/50000                      

                       Computation: 124579 steps/s (collection: 0.663s, learning 0.126s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.79s
                        Total time: 918.23s
                               ETA: 647 mins 47.3 s

################################################################################
                     Learning iteration 1154/50000                      

                       Computation: 127729 steps/s (collection: 0.634s, learning 0.135s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.77s
                        Total time: 919.00s
                               ETA: 647 mins 45.4 s

################################################################################
                     Learning iteration 1155/50000                      

                       Computation: 129779 steps/s (collection: 0.620s, learning 0.137s)
               Value function loss: 0.0555
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0171
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.76s
                        Total time: 919.76s
                               ETA: 647 mins 43.0 s

################################################################################
                     Learning iteration 1156/50000                      

                       Computation: 134007 steps/s (collection: 0.607s, learning 0.127s)
               Value function loss: 0.0667
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0070
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0112
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.73s
                        Total time: 920.49s
                               ETA: 647 mins 39.6 s

################################################################################
                     Learning iteration 1157/50000                      

                       Computation: 137281 steps/s (collection: 0.590s, learning 0.126s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0167
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.72s
                        Total time: 921.21s
                               ETA: 647 mins 35.4 s

################################################################################
                     Learning iteration 1158/50000                      

                       Computation: 123959 steps/s (collection: 0.670s, learning 0.123s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.79s
                        Total time: 922.00s
                               ETA: 647 mins 34.5 s

################################################################################
                     Learning iteration 1159/50000                      

                       Computation: 121904 steps/s (collection: 0.682s, learning 0.125s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0190
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0166
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.81s
                        Total time: 922.81s
                               ETA: 647 mins 34.2 s

################################################################################
                     Learning iteration 1160/50000                      

                       Computation: 135117 steps/s (collection: 0.599s, learning 0.129s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.73s
                        Total time: 923.54s
                               ETA: 647 mins 30.5 s

################################################################################
                     Learning iteration 1161/50000                      

                       Computation: 134891 steps/s (collection: 0.606s, learning 0.123s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.50
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.73s
                        Total time: 924.26s
                               ETA: 647 mins 26.9 s

################################################################################
                     Learning iteration 1162/50000                      

                       Computation: 139001 steps/s (collection: 0.585s, learning 0.122s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.72
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.71s
                        Total time: 924.97s
                               ETA: 647 mins 22.4 s

################################################################################
                     Learning iteration 1163/50000                      

                       Computation: 132531 steps/s (collection: 0.616s, learning 0.126s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.74s
                        Total time: 925.71s
                               ETA: 647 mins 19.4 s

################################################################################
                     Learning iteration 1164/50000                      

                       Computation: 115283 steps/s (collection: 0.699s, learning 0.153s)
               Value function loss: 0.0588
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0113
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.85s
                        Total time: 926.57s
                               ETA: 647 mins 21.0 s

################################################################################
                     Learning iteration 1165/50000                      

                       Computation: 128550 steps/s (collection: 0.640s, learning 0.124s)
               Value function loss: 0.0591
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.76s
                        Total time: 927.33s
                               ETA: 647 mins 18.9 s

################################################################################
                     Learning iteration 1166/50000                      

                       Computation: 138354 steps/s (collection: 0.587s, learning 0.124s)
               Value function loss: 0.0653
                    Surrogate loss: -0.0082
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0170
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.71s
                        Total time: 928.04s
                               ETA: 647 mins 14.6 s

################################################################################
                     Learning iteration 1167/50000                      

                       Computation: 134545 steps/s (collection: 0.597s, learning 0.134s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.78
                Mean reward (task): -4.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0173
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.73s
                        Total time: 928.77s
                               ETA: 647 mins 11.1 s

################################################################################
                     Learning iteration 1168/50000                      

                       Computation: 119330 steps/s (collection: 0.651s, learning 0.173s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0028
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0177
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.82s
                        Total time: 929.60s
                               ETA: 647 mins 11.5 s

################################################################################
                     Learning iteration 1169/50000                      

                       Computation: 123912 steps/s (collection: 0.653s, learning 0.140s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.79s
                        Total time: 930.39s
                               ETA: 647 mins 10.6 s

################################################################################
                     Learning iteration 1170/50000                      

                       Computation: 118780 steps/s (collection: 0.701s, learning 0.126s)
               Value function loss: 0.0607
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 0.83s
                        Total time: 931.22s
                               ETA: 647 mins 11.2 s

################################################################################
                     Learning iteration 1171/50000                      

                       Computation: 125811 steps/s (collection: 0.639s, learning 0.143s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0191
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0168
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 0.78s
                        Total time: 932.00s
                               ETA: 647 mins 9.8 s

################################################################################
                     Learning iteration 1172/50000                      

                       Computation: 126297 steps/s (collection: 0.655s, learning 0.123s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.78s
                        Total time: 932.78s
                               ETA: 647 mins 8.3 s

################################################################################
                     Learning iteration 1173/50000                      

                       Computation: 120997 steps/s (collection: 0.688s, learning 0.124s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.29
                Mean reward (task): -5.29
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0036
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.81s
                        Total time: 933.59s
                               ETA: 647 mins 8.2 s

################################################################################
                     Learning iteration 1174/50000                      

                       Computation: 130228 steps/s (collection: 0.632s, learning 0.123s)
               Value function loss: 0.0573
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.87
                Mean reward (task): -4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0149
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0073
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.75s
                        Total time: 934.34s
                               ETA: 647 mins 5.8 s

################################################################################
                     Learning iteration 1175/50000                      

                       Computation: 132410 steps/s (collection: 0.599s, learning 0.144s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.74s
                        Total time: 935.09s
                               ETA: 647 mins 2.8 s

################################################################################
                     Learning iteration 1176/50000                      

                       Computation: 124355 steps/s (collection: 0.654s, learning 0.136s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0114
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.79s
                        Total time: 935.88s
                               ETA: 647 mins 1.8 s

################################################################################
                     Learning iteration 1177/50000                      

                       Computation: 126373 steps/s (collection: 0.640s, learning 0.138s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0188
       Mean episode rew_smoothness: -0.0105
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.78s
                        Total time: 936.65s
                               ETA: 647 mins 0.3 s

################################################################################
                     Learning iteration 1178/50000                      

                       Computation: 129796 steps/s (collection: 0.629s, learning 0.129s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0149
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.76s
                        Total time: 937.41s
                               ETA: 646 mins 57.9 s

################################################################################
                     Learning iteration 1179/50000                      

                       Computation: 120551 steps/s (collection: 0.691s, learning 0.124s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.11
                Mean reward (task): -5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.04
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0089
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.82s
                        Total time: 938.23s
                               ETA: 646 mins 58.0 s

################################################################################
                     Learning iteration 1180/50000                      

                       Computation: 122252 steps/s (collection: 0.671s, learning 0.133s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0115
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.80s
                        Total time: 939.03s
                               ETA: 646 mins 57.5 s

################################################################################
                     Learning iteration 1181/50000                      

                       Computation: 120370 steps/s (collection: 0.689s, learning 0.128s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.82s
                        Total time: 939.85s
                               ETA: 646 mins 57.6 s

################################################################################
                     Learning iteration 1182/50000                      

                       Computation: 136521 steps/s (collection: 0.596s, learning 0.124s)
               Value function loss: 0.0527
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0149
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.72s
                        Total time: 940.57s
                               ETA: 646 mins 53.7 s

################################################################################
                     Learning iteration 1183/50000                      

                       Computation: 115482 steps/s (collection: 0.706s, learning 0.145s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0089
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0074
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.85s
                        Total time: 941.42s
                               ETA: 646 mins 55.3 s

################################################################################
                     Learning iteration 1184/50000                      

                       Computation: 121482 steps/s (collection: 0.676s, learning 0.133s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0071
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.81s
                        Total time: 942.23s
                               ETA: 646 mins 55.0 s

################################################################################
                     Learning iteration 1185/50000                      

                       Computation: 123003 steps/s (collection: 0.672s, learning 0.127s)
               Value function loss: 0.0569
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.81
                Mean reward (task): -4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.80s
                        Total time: 943.03s
                               ETA: 646 mins 54.4 s

################################################################################
                     Learning iteration 1186/50000                      

                       Computation: 113717 steps/s (collection: 0.717s, learning 0.147s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0083
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.69
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.86s
                        Total time: 943.89s
                               ETA: 646 mins 56.5 s

################################################################################
                     Learning iteration 1187/50000                      

                       Computation: 118356 steps/s (collection: 0.687s, learning 0.144s)
               Value function loss: 0.0590
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0193
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.83s
                        Total time: 944.72s
                               ETA: 646 mins 57.1 s

################################################################################
                     Learning iteration 1188/50000                      

                       Computation: 108984 steps/s (collection: 0.750s, learning 0.152s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.90s
                        Total time: 945.62s
                               ETA: 647 mins 0.7 s

################################################################################
                     Learning iteration 1189/50000                      

                       Computation: 127009 steps/s (collection: 0.642s, learning 0.132s)
               Value function loss: 0.0715
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.77s
                        Total time: 946.40s
                               ETA: 646 mins 59.1 s

################################################################################
                     Learning iteration 1190/50000                      

                       Computation: 130803 steps/s (collection: 0.612s, learning 0.139s)
               Value function loss: 0.0570
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0072
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.75s
                        Total time: 947.15s
                               ETA: 646 mins 56.5 s

################################################################################
                     Learning iteration 1191/50000                      

                       Computation: 122998 steps/s (collection: 0.656s, learning 0.144s)
               Value function loss: 0.0559
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.80s
                        Total time: 947.95s
                               ETA: 646 mins 55.8 s

################################################################################
                     Learning iteration 1192/50000                      

                       Computation: 117789 steps/s (collection: 0.674s, learning 0.161s)
               Value function loss: 0.0608
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.83s
                        Total time: 948.78s
                               ETA: 646 mins 56.6 s

################################################################################
                     Learning iteration 1193/50000                      

                       Computation: 114716 steps/s (collection: 0.727s, learning 0.130s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.06
                Mean reward (task): -5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.86s
                        Total time: 949.64s
                               ETA: 646 mins 58.4 s

################################################################################
                     Learning iteration 1194/50000                      

                       Computation: 126686 steps/s (collection: 0.650s, learning 0.126s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0037
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.78s
                        Total time: 950.42s
                               ETA: 646 mins 56.8 s

################################################################################
                     Learning iteration 1195/50000                      

                       Computation: 118981 steps/s (collection: 0.687s, learning 0.139s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.87
                Mean reward (task): -4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.83s
                        Total time: 951.24s
                               ETA: 646 mins 57.2 s

################################################################################
                     Learning iteration 1196/50000                      

                       Computation: 122156 steps/s (collection: 0.664s, learning 0.141s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.78
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0190
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.80s
                        Total time: 952.05s
                               ETA: 646 mins 56.8 s

################################################################################
                     Learning iteration 1197/50000                      

                       Computation: 121826 steps/s (collection: 0.680s, learning 0.127s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0116
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.81s
                        Total time: 952.85s
                               ETA: 646 mins 56.5 s

################################################################################
                     Learning iteration 1198/50000                      

                       Computation: 124476 steps/s (collection: 0.665s, learning 0.125s)
               Value function loss: 0.0570
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.79s
                        Total time: 953.64s
                               ETA: 646 mins 55.5 s

################################################################################
                     Learning iteration 1199/50000                      

                       Computation: 126714 steps/s (collection: 0.653s, learning 0.123s)
               Value function loss: 0.0615
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.78s
                        Total time: 954.42s
                               ETA: 646 mins 53.9 s

################################################################################
                     Learning iteration 1200/50000                      

                       Computation: 127559 steps/s (collection: 0.646s, learning 0.125s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.78
                Mean reward (task): -4.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.77s
                        Total time: 955.19s
                               ETA: 646 mins 52.1 s

################################################################################
                     Learning iteration 1201/50000                      

                       Computation: 129083 steps/s (collection: 0.632s, learning 0.129s)
               Value function loss: 0.0674
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0090
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0172
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.76s
                        Total time: 955.95s
                               ETA: 646 mins 49.9 s

################################################################################
                     Learning iteration 1202/50000                      

                       Computation: 114735 steps/s (collection: 0.711s, learning 0.146s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0188
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0177
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.86s
                        Total time: 956.81s
                               ETA: 646 mins 51.6 s

################################################################################
                     Learning iteration 1203/50000                      

                       Computation: 115810 steps/s (collection: 0.709s, learning 0.140s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.85s
                        Total time: 957.66s
                               ETA: 646 mins 53.0 s

################################################################################
                     Learning iteration 1204/50000                      

                       Computation: 127111 steps/s (collection: 0.627s, learning 0.146s)
               Value function loss: 0.0602
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.77s
                        Total time: 958.43s
                               ETA: 646 mins 51.3 s

################################################################################
                     Learning iteration 1205/50000                      

                       Computation: 125685 steps/s (collection: 0.658s, learning 0.125s)
               Value function loss: 0.0670
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.78s
                        Total time: 959.21s
                               ETA: 646 mins 50.0 s

################################################################################
                     Learning iteration 1206/50000                      

                       Computation: 118055 steps/s (collection: 0.693s, learning 0.140s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.11
                Mean reward (task): -5.11
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0073
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0190
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.83s
                        Total time: 960.05s
                               ETA: 646 mins 50.7 s

################################################################################
                     Learning iteration 1207/50000                      

                       Computation: 128118 steps/s (collection: 0.619s, learning 0.148s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0190
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.77s
                        Total time: 960.81s
                               ETA: 646 mins 48.8 s

################################################################################
                     Learning iteration 1208/50000                      

                       Computation: 120233 steps/s (collection: 0.661s, learning 0.156s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0019
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.82s
                        Total time: 961.63s
                               ETA: 646 mins 48.9 s

################################################################################
                     Learning iteration 1209/50000                      

                       Computation: 112330 steps/s (collection: 0.726s, learning 0.149s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.88s
                        Total time: 962.51s
                               ETA: 646 mins 51.3 s

################################################################################
                     Learning iteration 1210/50000                      

                       Computation: 123057 steps/s (collection: 0.658s, learning 0.141s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0076
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0174
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.80s
                        Total time: 963.31s
                               ETA: 646 mins 50.6 s

################################################################################
                     Learning iteration 1211/50000                      

                       Computation: 115608 steps/s (collection: 0.715s, learning 0.136s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.80
                Mean reward (task): -4.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0078
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.85s
                        Total time: 964.16s
                               ETA: 646 mins 52.0 s

################################################################################
                     Learning iteration 1212/50000                      

                       Computation: 115658 steps/s (collection: 0.706s, learning 0.144s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.84
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0074
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0075
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.85s
                        Total time: 965.01s
                               ETA: 646 mins 53.4 s

################################################################################
                     Learning iteration 1213/50000                      

                       Computation: 118466 steps/s (collection: 0.686s, learning 0.144s)
               Value function loss: 0.0557
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.87
                Mean reward (task): -4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.83s
                        Total time: 965.84s
                               ETA: 646 mins 54.0 s

################################################################################
                     Learning iteration 1214/50000                      

                       Computation: 113803 steps/s (collection: 0.714s, learning 0.150s)
               Value function loss: 0.0576
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.83
                Mean reward (task): -4.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0076
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.86s
                        Total time: 966.70s
                               ETA: 646 mins 56.0 s

################################################################################
                     Learning iteration 1215/50000                      

                       Computation: 138103 steps/s (collection: 0.590s, learning 0.122s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.94
                Mean reward (task): -4.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.71s
                        Total time: 967.41s
                               ETA: 646 mins 51.8 s

################################################################################
                     Learning iteration 1216/50000                      

                       Computation: 135173 steps/s (collection: 0.596s, learning 0.131s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.73s
                        Total time: 968.14s
                               ETA: 646 mins 48.3 s

################################################################################
                     Learning iteration 1217/50000                      

                       Computation: 130143 steps/s (collection: 0.626s, learning 0.130s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.18
                Mean reward (task): -5.18
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.76s
                        Total time: 968.89s
                               ETA: 646 mins 45.9 s

################################################################################
                     Learning iteration 1218/50000                      

                       Computation: 128762 steps/s (collection: 0.640s, learning 0.123s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.06
                Mean reward (task): -5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.76s
                        Total time: 969.66s
                               ETA: 646 mins 43.8 s

################################################################################
                     Learning iteration 1219/50000                      

                       Computation: 136656 steps/s (collection: 0.596s, learning 0.123s)
               Value function loss: 0.0614
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.72s
                        Total time: 970.38s
                               ETA: 646 mins 39.9 s

################################################################################
                     Learning iteration 1220/50000                      

                       Computation: 133135 steps/s (collection: 0.616s, learning 0.123s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.13
                Mean reward (task): -5.13
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0176
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.74s
                        Total time: 971.11s
                               ETA: 646 mins 36.9 s

################################################################################
                     Learning iteration 1221/50000                      

                       Computation: 123362 steps/s (collection: 0.648s, learning 0.149s)
               Value function loss: 0.0608
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.80s
                        Total time: 971.91s
                               ETA: 646 mins 36.1 s

################################################################################
                     Learning iteration 1222/50000                      

                       Computation: 125681 steps/s (collection: 0.659s, learning 0.123s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.37
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0077
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0179
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.78s
                        Total time: 972.69s
                               ETA: 646 mins 34.8 s

################################################################################
                     Learning iteration 1223/50000                      

                       Computation: 128273 steps/s (collection: 0.642s, learning 0.124s)
               Value function loss: 0.0698
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.96
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.77s
                        Total time: 973.46s
                               ETA: 646 mins 32.9 s

################################################################################
                     Learning iteration 1224/50000                      

                       Computation: 123623 steps/s (collection: 0.668s, learning 0.127s)
               Value function loss: 0.0586
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.80s
                        Total time: 974.26s
                               ETA: 646 mins 32.1 s

################################################################################
                     Learning iteration 1225/50000                      

                       Computation: 110551 steps/s (collection: 0.746s, learning 0.143s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.06
                Mean reward (task): -5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.89s
                        Total time: 975.14s
                               ETA: 646 mins 35.0 s

################################################################################
                     Learning iteration 1226/50000                      

                       Computation: 121379 steps/s (collection: 0.661s, learning 0.148s)
               Value function loss: 0.0698
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.51
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0091
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0119
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.81s
                        Total time: 975.95s
                               ETA: 646 mins 34.8 s

################################################################################
                     Learning iteration 1227/50000                      

                       Computation: 136077 steps/s (collection: 0.597s, learning 0.125s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0084
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0117
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.72s
                        Total time: 976.68s
                               ETA: 646 mins 31.1 s

################################################################################
                     Learning iteration 1228/50000                      

                       Computation: 116085 steps/s (collection: 0.720s, learning 0.127s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0118
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.85s
                        Total time: 977.52s
                               ETA: 646 mins 32.3 s

################################################################################
                     Learning iteration 1229/50000                      

                       Computation: 111789 steps/s (collection: 0.738s, learning 0.141s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.91
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0120
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.88s
                        Total time: 978.40s
                               ETA: 646 mins 34.9 s

################################################################################
                     Learning iteration 1230/50000                      

                       Computation: 115851 steps/s (collection: 0.715s, learning 0.133s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.80
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0020
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.85s
                        Total time: 979.25s
                               ETA: 646 mins 36.2 s

################################################################################
                     Learning iteration 1231/50000                      

                       Computation: 116726 steps/s (collection: 0.713s, learning 0.130s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.84s
                        Total time: 980.09s
                               ETA: 646 mins 37.2 s

################################################################################
                     Learning iteration 1232/50000                      

                       Computation: 109522 steps/s (collection: 0.735s, learning 0.163s)
               Value function loss: 0.0577
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.90s
                        Total time: 980.99s
                               ETA: 646 mins 40.5 s

################################################################################
                     Learning iteration 1233/50000                      

                       Computation: 114874 steps/s (collection: 0.707s, learning 0.149s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0075
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0188
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.86s
                        Total time: 981.85s
                               ETA: 646 mins 42.1 s

################################################################################
                     Learning iteration 1234/50000                      

                       Computation: 123610 steps/s (collection: 0.650s, learning 0.145s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0121
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.80s
                        Total time: 982.64s
                               ETA: 646 mins 41.2 s

################################################################################
                     Learning iteration 1235/50000                      

                       Computation: 119259 steps/s (collection: 0.690s, learning 0.134s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0079
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.82s
                        Total time: 983.47s
                               ETA: 646 mins 41.6 s

################################################################################
                     Learning iteration 1236/50000                      

                       Computation: 118571 steps/s (collection: 0.659s, learning 0.170s)
               Value function loss: 0.0608
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.62
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.83s
                        Total time: 984.30s
                               ETA: 646 mins 42.1 s

################################################################################
                     Learning iteration 1237/50000                      

                       Computation: 113561 steps/s (collection: 0.721s, learning 0.145s)
               Value function loss: 0.0555
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.28
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0188
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.87s
                        Total time: 985.16s
                               ETA: 646 mins 44.1 s

################################################################################
                     Learning iteration 1238/50000                      

                       Computation: 116548 steps/s (collection: 0.685s, learning 0.158s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0071
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.21
                Mean reward (task): -5.21
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.84s
                        Total time: 986.00s
                               ETA: 646 mins 45.1 s

################################################################################
                     Learning iteration 1239/50000                      

                       Computation: 121704 steps/s (collection: 0.682s, learning 0.125s)
               Value function loss: 0.0564
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0175
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.81s
                        Total time: 986.81s
                               ETA: 646 mins 44.8 s

################################################################################
                     Learning iteration 1240/50000                      

                       Computation: 119157 steps/s (collection: 0.699s, learning 0.126s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0182
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.82s
                        Total time: 987.64s
                               ETA: 646 mins 45.2 s

################################################################################
                     Learning iteration 1241/50000                      

                       Computation: 129336 steps/s (collection: 0.636s, learning 0.124s)
               Value function loss: 0.0667
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.76s
                        Total time: 988.40s
                               ETA: 646 mins 43.0 s

################################################################################
                     Learning iteration 1242/50000                      

                       Computation: 126978 steps/s (collection: 0.649s, learning 0.125s)
               Value function loss: 0.0659
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 33.21
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0092
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0190
       Mean episode rew_smoothness: -0.0106
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.77s
                        Total time: 989.17s
                               ETA: 646 mins 41.3 s

################################################################################
                     Learning iteration 1243/50000                      

                       Computation: 139817 steps/s (collection: 0.580s, learning 0.123s)
               Value function loss: 0.0670
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.15
                Mean reward (task): -5.15
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.70s
                        Total time: 989.87s
                               ETA: 646 mins 36.9 s

################################################################################
                     Learning iteration 1244/50000                      

                       Computation: 126521 steps/s (collection: 0.652s, learning 0.125s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0027
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.78s
                        Total time: 990.65s
                               ETA: 646 mins 35.4 s

################################################################################
                     Learning iteration 1245/50000                      

                       Computation: 135130 steps/s (collection: 0.603s, learning 0.124s)
               Value function loss: 0.0655
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.73s
                        Total time: 991.38s
                               ETA: 646 mins 31.9 s

################################################################################
                     Learning iteration 1246/50000                      

                       Computation: 131153 steps/s (collection: 0.626s, learning 0.123s)
               Value function loss: 0.1049
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.87
                Mean reward (task): -4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0107
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0181
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.75s
                        Total time: 992.13s
                               ETA: 646 mins 29.3 s

################################################################################
                     Learning iteration 1247/50000                      

                       Computation: 115580 steps/s (collection: 0.726s, learning 0.124s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.81
                Mean reward (task): -4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.85s
                        Total time: 992.98s
                               ETA: 646 mins 30.6 s

################################################################################
                     Learning iteration 1248/50000                      

                       Computation: 123303 steps/s (collection: 0.674s, learning 0.123s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.80s
                        Total time: 993.78s
                               ETA: 646 mins 29.9 s

################################################################################
                     Learning iteration 1249/50000                      

                       Computation: 125014 steps/s (collection: 0.662s, learning 0.124s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.85
                Mean reward (task): -4.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.27
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0090
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.79s
                        Total time: 994.56s
                               ETA: 646 mins 28.8 s

################################################################################
                     Learning iteration 1250/50000                      

                       Computation: 118317 steps/s (collection: 0.707s, learning 0.124s)
               Value function loss: 0.0610
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.83s
                        Total time: 995.39s
                               ETA: 646 mins 29.3 s

################################################################################
                     Learning iteration 1251/50000                      

                       Computation: 116933 steps/s (collection: 0.694s, learning 0.147s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0178
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.84s
                        Total time: 996.23s
                               ETA: 646 mins 30.3 s

################################################################################
                     Learning iteration 1252/50000                      

                       Computation: 125979 steps/s (collection: 0.655s, learning 0.125s)
               Value function loss: 0.0604
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.78s
                        Total time: 997.01s
                               ETA: 646 mins 28.9 s

################################################################################
                     Learning iteration 1253/50000                      

                       Computation: 111341 steps/s (collection: 0.748s, learning 0.135s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0008
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.88s
                        Total time: 997.90s
                               ETA: 646 mins 31.5 s

################################################################################
                     Learning iteration 1254/50000                      

                       Computation: 129420 steps/s (collection: 0.616s, learning 0.144s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.67
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0093
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.76s
                        Total time: 998.66s
                               ETA: 646 mins 29.3 s

################################################################################
                     Learning iteration 1255/50000                      

                       Computation: 121864 steps/s (collection: 0.677s, learning 0.129s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.81s
                        Total time: 999.46s
                               ETA: 646 mins 28.9 s

################################################################################
                     Learning iteration 1256/50000                      

                       Computation: 133890 steps/s (collection: 0.612s, learning 0.122s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.77
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.73s
                        Total time: 1000.20s
                               ETA: 646 mins 25.7 s

################################################################################
                     Learning iteration 1257/50000                      

                       Computation: 125683 steps/s (collection: 0.657s, learning 0.125s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0122
 Mean episode rew_tracking_lin_vel: 0.0180
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.78s
                        Total time: 1000.98s
                               ETA: 646 mins 24.4 s

################################################################################
                     Learning iteration 1258/50000                      

                       Computation: 126346 steps/s (collection: 0.638s, learning 0.140s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.78s
                        Total time: 1001.76s
                               ETA: 646 mins 22.9 s

################################################################################
                     Learning iteration 1259/50000                      

                       Computation: 117902 steps/s (collection: 0.696s, learning 0.138s)
               Value function loss: 0.0590
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.17
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.83s
                        Total time: 1002.59s
                               ETA: 646 mins 23.6 s

################################################################################
                     Learning iteration 1260/50000                      

                       Computation: 133520 steps/s (collection: 0.613s, learning 0.123s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.74s
                        Total time: 1003.33s
                               ETA: 646 mins 20.5 s

################################################################################
                     Learning iteration 1261/50000                      

                       Computation: 122180 steps/s (collection: 0.673s, learning 0.132s)
               Value function loss: 0.0626
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0037
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.80s
                        Total time: 1004.13s
                               ETA: 646 mins 20.1 s

################################################################################
                     Learning iteration 1262/50000                      

                       Computation: 118470 steps/s (collection: 0.673s, learning 0.156s)
               Value function loss: 0.0591
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.08
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0017
 Mean episode rew_tracking_ang_vel: 0.0123
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.83s
                        Total time: 1004.96s
                               ETA: 646 mins 20.6 s

################################################################################
                     Learning iteration 1263/50000                      

                       Computation: 109413 steps/s (collection: 0.748s, learning 0.151s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.90s
                        Total time: 1005.86s
                               ETA: 646 mins 23.8 s

################################################################################
                     Learning iteration 1264/50000                      

                       Computation: 112304 steps/s (collection: 0.735s, learning 0.140s)
               Value function loss: 0.0639
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.88s
                        Total time: 1006.74s
                               ETA: 646 mins 26.0 s

################################################################################
                     Learning iteration 1265/50000                      

                       Computation: 108205 steps/s (collection: 0.757s, learning 0.152s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.94
                Mean reward (task): -4.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.13
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.91s
                        Total time: 1007.65s
                               ETA: 646 mins 29.6 s

################################################################################
                     Learning iteration 1266/50000                      

                       Computation: 116296 steps/s (collection: 0.693s, learning 0.152s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.84
                Mean reward (task): -4.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.85s
                        Total time: 1008.49s
                               ETA: 646 mins 30.7 s

################################################################################
                     Learning iteration 1267/50000                      

                       Computation: 114938 steps/s (collection: 0.688s, learning 0.167s)
               Value function loss: 0.0663
                    Surrogate loss: -0.0074
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.86s
                        Total time: 1009.35s
                               ETA: 646 mins 32.1 s

################################################################################
                     Learning iteration 1268/50000                      

                       Computation: 118136 steps/s (collection: 0.702s, learning 0.130s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0177
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0184
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.83s
                        Total time: 1010.18s
                               ETA: 646 mins 32.7 s

################################################################################
                     Learning iteration 1269/50000                      

                       Computation: 112914 steps/s (collection: 0.730s, learning 0.141s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0038
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.87s
                        Total time: 1011.05s
                               ETA: 646 mins 34.8 s

################################################################################
                     Learning iteration 1270/50000                      

                       Computation: 120490 steps/s (collection: 0.684s, learning 0.132s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.08
                Mean reward (task): -5.08
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0161
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0191
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0124
 Mean episode rew_tracking_lin_vel: 0.0183
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.82s
                        Total time: 1011.86s
                               ETA: 646 mins 34.8 s

################################################################################
                     Learning iteration 1271/50000                      

                       Computation: 117448 steps/s (collection: 0.692s, learning 0.145s)
               Value function loss: 0.0580
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.98
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.84s
                        Total time: 1012.70s
                               ETA: 646 mins 35.5 s

################################################################################
                     Learning iteration 1272/50000                      

                       Computation: 134387 steps/s (collection: 0.592s, learning 0.140s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.07
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.73s
                        Total time: 1013.43s
                               ETA: 646 mins 32.3 s

################################################################################
                     Learning iteration 1273/50000                      

                       Computation: 120290 steps/s (collection: 0.675s, learning 0.142s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0186
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.82s
                        Total time: 1014.25s
                               ETA: 646 mins 32.3 s

################################################################################
                     Learning iteration 1274/50000                      

                       Computation: 114227 steps/s (collection: 0.734s, learning 0.127s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.86s
                        Total time: 1015.11s
                               ETA: 646 mins 33.9 s

################################################################################
                     Learning iteration 1275/50000                      

                       Computation: 116657 steps/s (collection: 0.715s, learning 0.128s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0080
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.84s
                        Total time: 1015.95s
                               ETA: 646 mins 34.9 s

################################################################################
                     Learning iteration 1276/50000                      

                       Computation: 109829 steps/s (collection: 0.765s, learning 0.130s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.90s
                        Total time: 1016.85s
                               ETA: 646 mins 37.9 s

################################################################################
                     Learning iteration 1277/50000                      

                       Computation: 115576 steps/s (collection: 0.711s, learning 0.140s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.84
                Mean reward (task): -4.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.85s
                        Total time: 1017.70s
                               ETA: 646 mins 39.2 s

################################################################################
                     Learning iteration 1278/50000                      

                       Computation: 119844 steps/s (collection: 0.673s, learning 0.147s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.82s
                        Total time: 1018.52s
                               ETA: 646 mins 39.3 s

################################################################################
                     Learning iteration 1279/50000                      

                       Computation: 121161 steps/s (collection: 0.670s, learning 0.141s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.81s
                        Total time: 1019.33s
                               ETA: 646 mins 39.1 s

################################################################################
                     Learning iteration 1280/50000                      

                       Computation: 116813 steps/s (collection: 0.698s, learning 0.143s)
               Value function loss: 0.0554
                    Surrogate loss: -0.0042
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.84s
                        Total time: 1020.17s
                               ETA: 646 mins 40.0 s

################################################################################
                     Learning iteration 1281/50000                      

                       Computation: 112051 steps/s (collection: 0.726s, learning 0.151s)
               Value function loss: 0.0669
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.70
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0125
 Mean episode rew_tracking_lin_vel: 0.0187
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.88s
                        Total time: 1021.05s
                               ETA: 646 mins 42.3 s

################################################################################
                     Learning iteration 1282/50000                      

                       Computation: 128984 steps/s (collection: 0.597s, learning 0.165s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.76s
                        Total time: 1021.81s
                               ETA: 646 mins 40.2 s

################################################################################
                     Learning iteration 1283/50000                      

                       Computation: 120195 steps/s (collection: 0.672s, learning 0.145s)
               Value function loss: 0.0662
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.46
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.82s
                        Total time: 1022.63s
                               ETA: 646 mins 40.2 s

################################################################################
                     Learning iteration 1284/50000                      

                       Computation: 130899 steps/s (collection: 0.618s, learning 0.133s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.17
                Mean reward (task): -5.17
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0162
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0191
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0189
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.75s
                        Total time: 1023.38s
                               ETA: 646 mins 37.7 s

################################################################################
                     Learning iteration 1285/50000                      

                       Computation: 122186 steps/s (collection: 0.670s, learning 0.135s)
               Value function loss: 0.0549
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.56
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0077
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.80s
                        Total time: 1024.18s
                               ETA: 646 mins 37.2 s

################################################################################
                     Learning iteration 1286/50000                      

                       Computation: 129490 steps/s (collection: 0.616s, learning 0.143s)
               Value function loss: 0.0595
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.80
                Mean reward (task): -4.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.76s
                        Total time: 1024.94s
                               ETA: 646 mins 35.0 s

################################################################################
                     Learning iteration 1287/50000                      

                       Computation: 102841 steps/s (collection: 0.774s, learning 0.182s)
               Value function loss: 0.0569
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.96s
                        Total time: 1025.90s
                               ETA: 646 mins 40.2 s

################################################################################
                     Learning iteration 1288/50000                      

                       Computation: 108112 steps/s (collection: 0.762s, learning 0.147s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0067
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0076
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0185
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.91s
                        Total time: 1026.81s
                               ETA: 646 mins 43.7 s

################################################################################
                     Learning iteration 1289/50000                      

                       Computation: 114220 steps/s (collection: 0.684s, learning 0.177s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0034
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.89
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0094
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.86s
                        Total time: 1027.67s
                               ETA: 646 mins 45.3 s

################################################################################
                     Learning iteration 1290/50000                      

                       Computation: 110253 steps/s (collection: 0.722s, learning 0.169s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0077
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.89s
                        Total time: 1028.56s
                               ETA: 646 mins 48.1 s

################################################################################
                     Learning iteration 1291/50000                      

                       Computation: 111902 steps/s (collection: 0.706s, learning 0.173s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.38
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.88s
                        Total time: 1029.44s
                               ETA: 646 mins 50.4 s

################################################################################
                     Learning iteration 1292/50000                      

                       Computation: 105220 steps/s (collection: 0.766s, learning 0.168s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.06
                Mean reward (task): -5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.02
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.93s
                        Total time: 1030.37s
                               ETA: 646 mins 54.7 s

################################################################################
                     Learning iteration 1293/50000                      

                       Computation: 113375 steps/s (collection: 0.696s, learning 0.171s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.75
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0078
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.87s
                        Total time: 1031.24s
                               ETA: 646 mins 56.6 s

################################################################################
                     Learning iteration 1294/50000                      

                       Computation: 113415 steps/s (collection: 0.726s, learning 0.141s)
               Value function loss: 0.0721
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0108
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.87s
                        Total time: 1032.11s
                               ETA: 646 mins 58.4 s

################################################################################
                     Learning iteration 1295/50000                      

                       Computation: 130576 steps/s (collection: 0.610s, learning 0.142s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.32
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.75s
                        Total time: 1032.86s
                               ETA: 646 mins 56.0 s

################################################################################
                     Learning iteration 1296/50000                      

                       Computation: 111134 steps/s (collection: 0.735s, learning 0.149s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.88s
                        Total time: 1033.75s
                               ETA: 646 mins 58.5 s

################################################################################
                     Learning iteration 1297/50000                      

                       Computation: 115050 steps/s (collection: 0.686s, learning 0.168s)
               Value function loss: 0.0588
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.91
                Mean reward (task): -4.91
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.85s
                        Total time: 1034.60s
                               ETA: 646 mins 59.8 s

################################################################################
                     Learning iteration 1298/50000                      

                       Computation: 116514 steps/s (collection: 0.700s, learning 0.143s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0079
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.78
                Mean reward (task): -4.78
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.86
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.84s
                        Total time: 1035.44s
                               ETA: 647 mins 0.8 s

################################################################################
                     Learning iteration 1299/50000                      

                       Computation: 107449 steps/s (collection: 0.745s, learning 0.170s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.53
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0188
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.91s
                        Total time: 1036.36s
                               ETA: 647 mins 4.4 s

################################################################################
                     Learning iteration 1300/50000                      

                       Computation: 113973 steps/s (collection: 0.718s, learning 0.145s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 34.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.86s
                        Total time: 1037.22s
                               ETA: 647 mins 6.0 s

################################################################################
                     Learning iteration 1301/50000                      

                       Computation: 113324 steps/s (collection: 0.716s, learning 0.151s)
               Value function loss: 0.0504
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0081
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.87s
                        Total time: 1038.09s
                               ETA: 647 mins 7.9 s

################################################################################
                     Learning iteration 1302/50000                      

                       Computation: 116854 steps/s (collection: 0.718s, learning 0.123s)
               Value function loss: 0.0648
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0095
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0193
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.84s
                        Total time: 1038.93s
                               ETA: 647 mins 8.7 s

################################################################################
                     Learning iteration 1303/50000                      

                       Computation: 127757 steps/s (collection: 0.634s, learning 0.135s)
               Value function loss: 0.0656
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.61
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.77s
                        Total time: 1039.70s
                               ETA: 647 mins 6.9 s

################################################################################
                     Learning iteration 1304/50000                      

                       Computation: 110963 steps/s (collection: 0.735s, learning 0.151s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0031
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.09
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0109
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.89s
                        Total time: 1040.59s
                               ETA: 647 mins 9.4 s

################################################################################
                     Learning iteration 1305/50000                      

                       Computation: 115306 steps/s (collection: 0.716s, learning 0.137s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0078
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.54
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0079
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0177
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0192
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.85s
                        Total time: 1041.44s
                               ETA: 647 mins 10.6 s

################################################################################
                     Learning iteration 1306/50000                      

                       Computation: 120688 steps/s (collection: 0.666s, learning 0.149s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.10
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.81s
                        Total time: 1042.25s
                               ETA: 647 mins 10.5 s

################################################################################
                     Learning iteration 1307/50000                      

                       Computation: 133044 steps/s (collection: 0.610s, learning 0.129s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.87
                Mean reward (task): -4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.71
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0038
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0195
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.74s
                        Total time: 1042.99s
                               ETA: 647 mins 7.5 s

################################################################################
                     Learning iteration 1308/50000                      

                       Computation: 122547 steps/s (collection: 0.670s, learning 0.133s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.97
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0176
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0199
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.80s
                        Total time: 1043.79s
                               ETA: 647 mins 6.9 s

################################################################################
                     Learning iteration 1309/50000                      

                       Computation: 121468 steps/s (collection: 0.683s, learning 0.127s)
               Value function loss: 0.0645
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.81
                Mean reward (task): -4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0194
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.81s
                        Total time: 1044.60s
                               ETA: 647 mins 6.5 s

################################################################################
                     Learning iteration 1310/50000                      

                       Computation: 116770 steps/s (collection: 0.708s, learning 0.134s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 35.95
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0080
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0126
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.84s
                        Total time: 1045.44s
                               ETA: 647 mins 7.4 s

################################################################################
                     Learning iteration 1311/50000                      

                       Computation: 131185 steps/s (collection: 0.618s, learning 0.131s)
               Value function loss: 0.0575
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.87
                Mean reward (task): -4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0200
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.75s
                        Total time: 1046.19s
                               ETA: 647 mins 4.8 s

################################################################################
                     Learning iteration 1312/50000                      

                       Computation: 120575 steps/s (collection: 0.647s, learning 0.168s)
               Value function loss: 0.0604
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.72
                Mean reward (task): -4.72
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.55
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0176
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.82s
                        Total time: 1047.01s
                               ETA: 647 mins 4.7 s

################################################################################
                     Learning iteration 1313/50000                      

                       Computation: 122801 steps/s (collection: 0.648s, learning 0.153s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.80s
                        Total time: 1047.81s
                               ETA: 647 mins 4.0 s

################################################################################
                     Learning iteration 1314/50000                      

                       Computation: 120202 steps/s (collection: 0.669s, learning 0.149s)
               Value function loss: 0.0634
                    Surrogate loss: -0.0039
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.57
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.82s
                        Total time: 1048.63s
                               ETA: 647 mins 3.9 s

################################################################################
                     Learning iteration 1315/50000                      

                       Computation: 120806 steps/s (collection: 0.680s, learning 0.134s)
               Value function loss: 0.0549
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0177
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.81s
                        Total time: 1049.44s
                               ETA: 647 mins 3.7 s

################################################################################
                     Learning iteration 1316/50000                      

                       Computation: 116343 steps/s (collection: 0.703s, learning 0.142s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0198
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.84s
                        Total time: 1050.29s
                               ETA: 647 mins 4.7 s

################################################################################
                     Learning iteration 1317/50000                      

                       Computation: 114085 steps/s (collection: 0.706s, learning 0.155s)
               Value function loss: 0.0708
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.81
                Mean reward (task): -4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0081
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0110
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0190
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.86s
                        Total time: 1051.15s
                               ETA: 647 mins 6.3 s

################################################################################
                     Learning iteration 1318/50000                      

                       Computation: 122157 steps/s (collection: 0.660s, learning 0.145s)
               Value function loss: 0.0621
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0191
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.80s
                        Total time: 1051.95s
                               ETA: 647 mins 5.7 s

################################################################################
                     Learning iteration 1319/50000                      

                       Computation: 118679 steps/s (collection: 0.702s, learning 0.127s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0211
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.83s
                        Total time: 1052.78s
                               ETA: 647 mins 6.1 s

################################################################################
                     Learning iteration 1320/50000                      

                       Computation: 124115 steps/s (collection: 0.645s, learning 0.147s)
               Value function loss: 0.0630
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0127
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.79s
                        Total time: 1053.57s
                               ETA: 647 mins 5.1 s

################################################################################
                     Learning iteration 1321/50000                      

                       Computation: 108265 steps/s (collection: 0.753s, learning 0.155s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0081
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.12
                Mean reward (task): -5.12
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.63
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0200
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.91s
                        Total time: 1054.48s
                               ETA: 647 mins 8.3 s

################################################################################
                     Learning iteration 1322/50000                      

                       Computation: 117844 steps/s (collection: 0.694s, learning 0.140s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.23
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.83s
                        Total time: 1055.31s
                               ETA: 647 mins 8.9 s

################################################################################
                     Learning iteration 1323/50000                      

                       Computation: 112139 steps/s (collection: 0.745s, learning 0.131s)
               Value function loss: 0.0564
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.83
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0201
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.88s
                        Total time: 1056.19s
                               ETA: 647 mins 11.0 s

################################################################################
                     Learning iteration 1324/50000                      

                       Computation: 123455 steps/s (collection: 0.672s, learning 0.125s)
               Value function loss: 0.0626
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.03
                Mean reward (task): -5.03
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.80s
                        Total time: 1056.99s
                               ETA: 647 mins 10.1 s

################################################################################
                     Learning iteration 1325/50000                      

                       Computation: 120498 steps/s (collection: 0.691s, learning 0.125s)
               Value function loss: 0.0581
                    Surrogate loss: -0.0053
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.82s
                        Total time: 1057.80s
                               ETA: 647 mins 10.0 s

################################################################################
                     Learning iteration 1326/50000                      

                       Computation: 115533 steps/s (collection: 0.722s, learning 0.129s)
               Value function loss: 0.0674
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.85s
                        Total time: 1058.65s
                               ETA: 647 mins 11.2 s

################################################################################
                     Learning iteration 1327/50000                      

                       Computation: 125678 steps/s (collection: 0.654s, learning 0.128s)
               Value function loss: 0.0621
                    Surrogate loss: -0.0036
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.80
                Mean reward (task): -4.80
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.01
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.78s
                        Total time: 1059.44s
                               ETA: 647 mins 9.8 s

################################################################################
                     Learning iteration 1328/50000                      

                       Computation: 121215 steps/s (collection: 0.677s, learning 0.134s)
               Value function loss: 0.0654
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.81s
                        Total time: 1060.25s
                               ETA: 647 mins 9.5 s

################################################################################
                     Learning iteration 1329/50000                      

                       Computation: 106501 steps/s (collection: 0.774s, learning 0.149s)
               Value function loss: 0.1224
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.41
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0199
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.92s
                        Total time: 1061.17s
                               ETA: 647 mins 13.3 s

################################################################################
                     Learning iteration 1330/50000                      

                       Computation: 107089 steps/s (collection: 0.767s, learning 0.151s)
               Value function loss: 0.0607
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.19
                Mean reward (task): -5.19
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.29
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.92s
                        Total time: 1062.09s
                               ETA: 647 mins 16.9 s

################################################################################
                     Learning iteration 1331/50000                      

                       Computation: 114347 steps/s (collection: 0.732s, learning 0.127s)
               Value function loss: 0.0638
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0096
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.86s
                        Total time: 1062.95s
                               ETA: 647 mins 18.3 s

################################################################################
                     Learning iteration 1332/50000                      

                       Computation: 108265 steps/s (collection: 0.762s, learning 0.146s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0033
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.91s
                        Total time: 1063.86s
                               ETA: 647 mins 21.5 s

################################################################################
                     Learning iteration 1333/50000                      

                       Computation: 104096 steps/s (collection: 0.780s, learning 0.164s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.85
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 0.94s
                        Total time: 1064.80s
                               ETA: 647 mins 26.1 s

################################################################################
                     Learning iteration 1334/50000                      

                       Computation: 106853 steps/s (collection: 0.739s, learning 0.181s)
               Value function loss: 0.0629
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 0.92s
                        Total time: 1065.72s
                               ETA: 647 mins 29.7 s

################################################################################
                     Learning iteration 1335/50000                      

                       Computation: 119270 steps/s (collection: 0.681s, learning 0.143s)
               Value function loss: 0.0564
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 0.82s
                        Total time: 1066.54s
                               ETA: 647 mins 29.9 s

################################################################################
                     Learning iteration 1336/50000                      

                       Computation: 114552 steps/s (collection: 0.716s, learning 0.142s)
               Value function loss: 0.0657
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 0.86s
                        Total time: 1067.40s
                               ETA: 647 mins 31.2 s

################################################################################
                     Learning iteration 1337/50000                      

                       Computation: 120267 steps/s (collection: 0.675s, learning 0.143s)
               Value function loss: 0.0617
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.14
                Mean reward (task): -5.14
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.44
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0186
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0197
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 0.82s
                        Total time: 1068.22s
                               ETA: 647 mins 31.1 s

################################################################################
                     Learning iteration 1338/50000                      

                       Computation: 115648 steps/s (collection: 0.681s, learning 0.169s)
               Value function loss: 0.0622
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0177
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 0.85s
                        Total time: 1069.07s
                               ETA: 647 mins 32.2 s

################################################################################
                     Learning iteration 1339/50000                      

                       Computation: 110354 steps/s (collection: 0.746s, learning 0.145s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0158
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 0.89s
                        Total time: 1069.96s
                               ETA: 647 mins 34.8 s

################################################################################
                     Learning iteration 1340/50000                      

                       Computation: 112291 steps/s (collection: 0.726s, learning 0.149s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 0.88s
                        Total time: 1070.84s
                               ETA: 647 mins 36.8 s

################################################################################
                     Learning iteration 1341/50000                      

                       Computation: 119938 steps/s (collection: 0.667s, learning 0.153s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0050
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.81
                Mean reward (task): -4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.31
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 0.82s
                        Total time: 1071.66s
                               ETA: 647 mins 36.7 s

################################################################################
                     Learning iteration 1342/50000                      

                       Computation: 112981 steps/s (collection: 0.729s, learning 0.141s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0093
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0128
 Mean episode rew_tracking_lin_vel: 0.0196
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.87s
                        Total time: 1072.53s
                               ETA: 647 mins 38.5 s

################################################################################
                     Learning iteration 1343/50000                      

                       Computation: 115208 steps/s (collection: 0.704s, learning 0.149s)
               Value function loss: 0.0697
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0156
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0187
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.85s
                        Total time: 1073.38s
                               ETA: 647 mins 39.7 s

################################################################################
                     Learning iteration 1344/50000                      

                       Computation: 121611 steps/s (collection: 0.655s, learning 0.154s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.94
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0159
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.81s
                        Total time: 1074.19s
                               ETA: 647 mins 39.3 s

################################################################################
                     Learning iteration 1345/50000                      

                       Computation: 119526 steps/s (collection: 0.678s, learning 0.144s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0055
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0199
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.82s
                        Total time: 1075.01s
                               ETA: 647 mins 39.3 s

################################################################################
                     Learning iteration 1346/50000                      

                       Computation: 114953 steps/s (collection: 0.693s, learning 0.163s)
               Value function loss: 0.0566
                    Surrogate loss: -0.0073
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0175
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.86s
                        Total time: 1075.87s
                               ETA: 647 mins 40.6 s

################################################################################
                     Learning iteration 1347/50000                      

                       Computation: 105503 steps/s (collection: 0.777s, learning 0.155s)
               Value function loss: 0.0614
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.77
                Mean reward (task): -4.77
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0184
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.93s
                        Total time: 1076.80s
                               ETA: 647 mins 44.6 s

################################################################################
                     Learning iteration 1348/50000                      

                       Computation: 120609 steps/s (collection: 0.671s, learning 0.144s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0052
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.14
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0183
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.82s
                        Total time: 1077.61s
                               ETA: 647 mins 44.3 s

################################################################################
                     Learning iteration 1349/50000                      

                       Computation: 116969 steps/s (collection: 0.706s, learning 0.134s)
               Value function loss: 0.0670
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0181
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.84s
                        Total time: 1078.45s
                               ETA: 647 mins 45.0 s

################################################################################
                     Learning iteration 1350/50000                      

                       Computation: 116267 steps/s (collection: 0.702s, learning 0.144s)
               Value function loss: 0.0609
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.82
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0176
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.85s
                        Total time: 1079.30s
                               ETA: 647 mins 45.9 s

################################################################################
                     Learning iteration 1351/50000                      

                       Computation: 113559 steps/s (collection: 0.738s, learning 0.128s)
               Value function loss: 0.0638
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.87s
                        Total time: 1080.16s
                               ETA: 647 mins 47.5 s

################################################################################
                     Learning iteration 1352/50000                      

                       Computation: 128426 steps/s (collection: 0.617s, learning 0.148s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0058
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -4.93
                Mean reward (task): -4.93
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.43
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0160
   Mean episode rew_dof_pos_limits: -0.0082
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0189
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0202
        Mean episode terrain_level: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.77s
                        Total time: 1080.93s
                               ETA: 647 mins 45.5 s

################################################################################
                     Learning iteration 1353/50000                      

                       Computation: 115154 steps/s (collection: 0.696s, learning 0.158s)
               Value function loss: 0.0654
                    Surrogate loss: -0.0026
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.67
               Mean reward (total): -5.04
                Mean reward (task): -5.04
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0203
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.85s
                        Total time: 1081.78s
                               ETA: 647 mins 46.7 s

################################################################################
                     Learning iteration 1354/50000                      

                       Computation: 112247 steps/s (collection: 0.724s, learning 0.152s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.99
                Mean reward (task): -4.99
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.16
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0175
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0206
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.88s
                        Total time: 1082.66s
                               ETA: 647 mins 48.7 s

################################################################################
                     Learning iteration 1355/50000                      

                       Computation: 114495 steps/s (collection: 0.724s, learning 0.134s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.36
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.86s
                        Total time: 1083.52s
                               ETA: 647 mins 50.0 s

################################################################################
                     Learning iteration 1356/50000                      

                       Computation: 115553 steps/s (collection: 0.717s, learning 0.134s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0057
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.84
                Mean reward (task): -4.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.90
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0174
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.85s
                        Total time: 1084.37s
                               ETA: 647 mins 51.0 s

################################################################################
                     Learning iteration 1357/50000                      

                       Computation: 104279 steps/s (collection: 0.775s, learning 0.167s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0010
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.16
                Mean reward (task): -5.16
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.05
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0176
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0207
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.94s
                        Total time: 1085.31s
                               ETA: 647 mins 55.4 s

################################################################################
                     Learning iteration 1358/50000                      

                       Computation: 113872 steps/s (collection: 0.722s, learning 0.141s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0069
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.87
                Mean reward (task): -4.87
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0200
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.86s
                        Total time: 1086.17s
                               ETA: 647 mins 56.9 s

################################################################################
                     Learning iteration 1359/50000                      

                       Computation: 108704 steps/s (collection: 0.760s, learning 0.144s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.83
                Mean reward (task): -4.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.59
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.90s
                        Total time: 1087.08s
                               ETA: 647 mins 59.8 s

################################################################################
                     Learning iteration 1360/50000                      

                       Computation: 112828 steps/s (collection: 0.717s, learning 0.154s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.82
                Mean reward (task): -4.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0039
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0185
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.87s
                        Total time: 1087.95s
                               ETA: 648 mins 1.6 s

################################################################################
                     Learning iteration 1361/50000                      

                       Computation: 111084 steps/s (collection: 0.737s, learning 0.148s)
               Value function loss: 0.0578
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.12
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0155
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0180
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.88s
                        Total time: 1088.83s
                               ETA: 648 mins 3.9 s

################################################################################
                     Learning iteration 1362/50000                      

                       Computation: 117306 steps/s (collection: 0.686s, learning 0.152s)
               Value function loss: 0.0595
                    Surrogate loss: -0.0072
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.94
                Mean reward (task): -4.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.73
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0157
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0088
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0182
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.84s
                        Total time: 1089.67s
                               ETA: 648 mins 4.4 s

################################################################################
                     Learning iteration 1363/50000                      

                       Computation: 108543 steps/s (collection: 0.764s, learning 0.142s)
               Value function loss: 0.0618
                    Surrogate loss: -0.0048
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.94
                Mean reward (task): -4.94
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0154
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.91s
                        Total time: 1090.58s
                               ETA: 648 mins 7.4 s

################################################################################
                     Learning iteration 1364/50000                      

                       Computation: 106455 steps/s (collection: 0.768s, learning 0.156s)
               Value function loss: 0.0639
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.00
                Mean reward (task): -5.00
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.19
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0152
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0179
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.92s
                        Total time: 1091.50s
                               ETA: 648 mins 11.1 s

################################################################################
                     Learning iteration 1365/50000                      

                       Computation: 124729 steps/s (collection: 0.657s, learning 0.132s)
               Value function loss: 0.0633
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.98
                Mean reward (task): -4.98
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0174
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.79s
                        Total time: 1092.29s
                               ETA: 648 mins 9.8 s

################################################################################
                     Learning iteration 1366/50000                      

                       Computation: 113724 steps/s (collection: 0.732s, learning 0.133s)
               Value function loss: 0.0567
                    Surrogate loss: -0.0060
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.81
                Mean reward (task): -4.81
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.18
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0173
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.86s
                        Total time: 1093.15s
                               ETA: 648 mins 11.3 s

################################################################################
                     Learning iteration 1367/50000                      

                       Computation: 111513 steps/s (collection: 0.724s, learning 0.157s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0035
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.92
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0178
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0205
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.88s
                        Total time: 1094.04s
                               ETA: 648 mins 13.5 s

################################################################################
                     Learning iteration 1368/50000                      

                       Computation: 111136 steps/s (collection: 0.750s, learning 0.135s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.79
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0153
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0173
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0129
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.88s
                        Total time: 1094.92s
                               ETA: 648 mins 15.7 s

################################################################################
                     Learning iteration 1369/50000                      

                       Computation: 112342 steps/s (collection: 0.724s, learning 0.151s)
               Value function loss: 0.0549
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.82
                Mean reward (task): -4.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.30
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0172
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.88s
                        Total time: 1095.80s
                               ETA: 648 mins 17.5 s

################################################################################
                     Learning iteration 1370/50000                      

                       Computation: 118985 steps/s (collection: 0.691s, learning 0.135s)
               Value function loss: 0.0542
                    Surrogate loss: -0.0047
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.34
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0148
   Mean episode rew_dof_pos_limits: -0.0083
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0170
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0018
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.83s
                        Total time: 1096.62s
                               ETA: 648 mins 17.7 s

################################################################################
                     Learning iteration 1371/50000                      

                       Computation: 112104 steps/s (collection: 0.723s, learning 0.153s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0046
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.85
                Mean reward (task): -4.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.68
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0146
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0169
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.88s
                        Total time: 1097.50s
                               ETA: 648 mins 19.6 s

################################################################################
                     Learning iteration 1372/50000                      

                       Computation: 121431 steps/s (collection: 0.679s, learning 0.131s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0068
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0147
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0169
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0132
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.81s
                        Total time: 1098.31s
                               ETA: 648 mins 19.1 s

################################################################################
                     Learning iteration 1373/50000                      

                       Computation: 127698 steps/s (collection: 0.630s, learning 0.140s)
               Value function loss: 0.0570
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.01
                Mean reward (task): -5.01
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.87
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0147
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0170
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0130
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.77s
                        Total time: 1099.08s
                               ETA: 648 mins 17.3 s

################################################################################
                     Learning iteration 1374/50000                      

                       Computation: 115910 steps/s (collection: 0.698s, learning 0.150s)
               Value function loss: 0.0602
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.88
                Mean reward (task): -4.88
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0146
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0167
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0209
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.85s
                        Total time: 1099.93s
                               ETA: 648 mins 18.2 s

################################################################################
                     Learning iteration 1375/50000                      

                       Computation: 115179 steps/s (collection: 0.701s, learning 0.152s)
               Value function loss: 0.0579
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.05
                Mean reward (task): -5.05
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0096
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0151
   Mean episode rew_dof_pos_limits: -0.0084
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0080
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0172
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0131
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.85s
                        Total time: 1100.78s
                               ETA: 648 mins 19.3 s

################################################################################
                     Learning iteration 1376/50000                      

                       Computation: 125401 steps/s (collection: 0.655s, learning 0.129s)
               Value function loss: 0.0543
                    Surrogate loss: -0.0062
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0146
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0172
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.78s
                        Total time: 1101.56s
                               ETA: 648 mins 17.9 s

################################################################################
                     Learning iteration 1377/50000                      

                       Computation: 119849 steps/s (collection: 0.675s, learning 0.145s)
               Value function loss: 0.0573
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.83
                Mean reward (task): -4.83
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.33
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0149
   Mean episode rew_dof_pos_limits: -0.0085
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0172
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0204
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.82s
                        Total time: 1102.38s
                               ETA: 648 mins 17.8 s

################################################################################
                     Learning iteration 1378/50000                      

                       Computation: 106862 steps/s (collection: 0.786s, learning 0.134s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.81
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0097
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0142
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0164
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0208
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.92s
                        Total time: 1103.30s
                               ETA: 648 mins 21.3 s

################################################################################
                     Learning iteration 1379/50000                      

                       Computation: 116739 steps/s (collection: 0.713s, learning 0.129s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0056
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.93
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0145
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0167
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.84s
                        Total time: 1104.15s
                               ETA: 648 mins 21.9 s

################################################################################
                     Learning iteration 1380/50000                      

                       Computation: 113853 steps/s (collection: 0.731s, learning 0.132s)
               Value function loss: 0.0703
                    Surrogate loss: -0.0025
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.82
                Mean reward (task): -4.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.64
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0146
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0169
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0135
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.86s
                        Total time: 1105.01s
                               ETA: 648 mins 23.4 s

################################################################################
                     Learning iteration 1381/50000                      

                       Computation: 120708 steps/s (collection: 0.680s, learning 0.134s)
               Value function loss: 0.0657
                    Surrogate loss: -0.0065
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.96
                Mean reward (task): -4.96
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0147
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0168
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0210
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.81s
                        Total time: 1105.82s
                               ETA: 648 mins 23.1 s

################################################################################
                     Learning iteration 1382/50000                      

                       Computation: 126740 steps/s (collection: 0.646s, learning 0.130s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0064
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.86
                Mean reward (task): -4.86
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.35
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0091
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0150
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0172
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.78s
                        Total time: 1106.60s
                               ETA: 648 mins 21.4 s

################################################################################
                     Learning iteration 1383/50000                      

                       Computation: 121315 steps/s (collection: 0.676s, learning 0.135s)
               Value function loss: 0.0565
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.10
                Mean reward (task): -5.10
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.00
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0149
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0172
       Mean episode rew_smoothness: -0.0114
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.81s
                        Total time: 1107.41s
                               ETA: 648 mins 21.0 s

################################################################################
                     Learning iteration 1384/50000                      

                       Computation: 112197 steps/s (collection: 0.746s, learning 0.130s)
               Value function loss: 0.0563
                    Surrogate loss: -0.0075
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.11
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0145
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0169
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.88s
                        Total time: 1108.29s
                               ETA: 648 mins 22.8 s

################################################################################
                     Learning iteration 1385/50000                      

                       Computation: 109231 steps/s (collection: 0.766s, learning 0.134s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0070
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.84
                Mean reward (task): -4.84
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.45
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0144
   Mean episode rew_dof_pos_limits: -0.0086
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0082
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0167
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0214
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.90s
                        Total time: 1109.19s
                               ETA: 648 mins 25.5 s

################################################################################
                     Learning iteration 1386/50000                      

                       Computation: 117408 steps/s (collection: 0.700s, learning 0.137s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.95
                Mean reward (task): -4.95
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.15
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0146
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0167
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0216
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.84s
                        Total time: 1110.02s
                               ETA: 648 mins 26.0 s

################################################################################
                     Learning iteration 1387/50000                      

                       Computation: 116030 steps/s (collection: 0.707s, learning 0.140s)
               Value function loss: 0.0563
                    Surrogate loss: -0.0024
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.88
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0145
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0042
      Mean episode rew_orientation: -0.0019
Mean episode rew_power_distribution: -0.0165
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0137
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.85s
                        Total time: 1110.87s
                               ETA: 648 mins 26.9 s

################################################################################
                     Learning iteration 1388/50000                      

                       Computation: 105611 steps/s (collection: 0.755s, learning 0.176s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0066
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.02
                Mean reward (task): -5.02
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.99
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0147
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0169
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.93s
                        Total time: 1111.80s
                               ETA: 648 mins 30.6 s

################################################################################
                     Learning iteration 1389/50000                      

                       Computation: 105960 steps/s (collection: 0.776s, learning 0.152s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.49
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0145
   Mean episode rew_dof_pos_limits: -0.0087
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0169
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0221
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.93s
                        Total time: 1112.73s
                               ETA: 648 mins 34.3 s

################################################################################
                     Learning iteration 1390/50000                      

                       Computation: 118651 steps/s (collection: 0.682s, learning 0.147s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0054
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.85
                Mean reward (task): -4.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 36.65
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0144
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0083
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0165
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0212
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.83s
                        Total time: 1113.56s
                               ETA: 648 mins 34.5 s

################################################################################
                     Learning iteration 1391/50000                      

                       Computation: 114632 steps/s (collection: 0.709s, learning 0.149s)
               Value function loss: 0.0607
                    Surrogate loss: -0.0051
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.90
                Mean reward (task): -4.90
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 39.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0092
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0145
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0085
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0165
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.86s
                        Total time: 1114.41s
                               ETA: 648 mins 35.6 s

################################################################################
                     Learning iteration 1392/50000                      

                       Computation: 112237 steps/s (collection: 0.746s, learning 0.130s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0061
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.09
                Mean reward (task): -5.09
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0148
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0169
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0229
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.88s
                        Total time: 1115.29s
                               ETA: 648 mins 37.5 s

################################################################################
                     Learning iteration 1393/50000                      

                       Computation: 119418 steps/s (collection: 0.695s, learning 0.128s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.89
                Mean reward (task): -4.89
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.20
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0093
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0144
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0084
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0165
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0136
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.82s
                        Total time: 1116.11s
                               ETA: 648 mins 37.5 s

################################################################################
                     Learning iteration 1394/50000                      

                       Computation: 116830 steps/s (collection: 0.707s, learning 0.134s)
               Value function loss: 0.0567
                    Surrogate loss: -0.0063
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.85
                Mean reward (task): -4.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 41.58
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0100
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0141
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0086
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0162
       Mean episode rew_smoothness: -0.0113
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0213
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.84s
                        Total time: 1116.96s
                               ETA: 648 mins 38.1 s

################################################################################
                     Learning iteration 1395/50000                      

                       Computation: 123716 steps/s (collection: 0.651s, learning 0.144s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0043
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.82
                Mean reward (task): -4.82
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.26
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0144
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0166
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.79s
                        Total time: 1117.75s
                               ETA: 648 mins 37.1 s

################################################################################
                     Learning iteration 1396/50000                      

                       Computation: 115576 steps/s (collection: 0.719s, learning 0.131s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0045
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.06
                Mean reward (task): -5.06
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.40
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0147
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0168
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0217
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.85s
                        Total time: 1118.60s
                               ETA: 648 mins 38.0 s

################################################################################
                     Learning iteration 1397/50000                      

                       Computation: 119230 steps/s (collection: 0.698s, learning 0.127s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0044
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.97
                Mean reward (task): -4.97
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.22
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0094
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0149
   Mean episode rew_dof_pos_limits: -0.0088
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0040
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0168
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0220
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.82s
                        Total time: 1119.42s
                               ETA: 648 mins 38.0 s

################################################################################
                     Learning iteration 1398/50000                      

                       Computation: 124830 steps/s (collection: 0.663s, learning 0.124s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0059
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -5.07
                Mean reward (task): -5.07
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 38.52
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0142
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0087
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0162
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0009
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0218
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.79s
                        Total time: 1120.21s
                               ETA: 648 mins 36.8 s

################################################################################
                     Learning iteration 1399/50000                      

                       Computation: 112528 steps/s (collection: 0.728s, learning 0.145s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0041
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.85
                Mean reward (task): -4.85
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 40.03
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0099
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0143
   Mean episode rew_dof_pos_limits: -0.0090
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0089
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0165
       Mean episode rew_smoothness: -0.0112
      Mean episode rew_stand_still: -0.0011
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0134
 Mean episode rew_tracking_lin_vel: 0.0224
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.87s
                        Total time: 1121.09s
                               ETA: 648 mins 38.5 s

################################################################################
                     Learning iteration 1400/50000                      

                       Computation: 114721 steps/s (collection: 0.711s, learning 0.146s)
               Value function loss: 0.0551
                    Surrogate loss: -0.0049
                Discriminator loss: 0.0000
            Discriminator accuracy: 0.0000
             Mean action noise std: 0.66
               Mean reward (total): -4.92
                Mean reward (task): -4.92
         Mean reward (exploration): 0.00
             Mean reward (entropy): 0.00
               Mean episode length: 37.76
--------------------------------------------------------------------------------
      Mean episode rew_action_rate: -0.0098
       Mean episode rew_ang_vel_xy: -0.0095
      Mean episode rew_base_height: -0.0013
          Mean episode rew_dof_acc: -0.0144
   Mean episode rew_dof_pos_limits: -0.0089
      Mean episode rew_joint_power: -0.0006
        Mean episode rew_lin_vel_z: -0.0090
           Mean episode rew_no_fly: 0.0041
      Mean episode rew_orientation: -0.0018
Mean episode rew_power_distribution: -0.0162
       Mean episode rew_smoothness: -0.0111
      Mean episode rew_stand_still: -0.0010
      Mean episode rew_termination: -0.2000
          Mean episode rew_torques: -0.0019
 Mean episode rew_tracking_ang_vel: 0.0133
 Mean episode rew_tracking_lin_vel: 0.0215
        Mean episode terrain_level: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.86s
                        Total time: 1121.94s
                               ETA: 648 mins 39.6 s

swanlab:Error happened while training
swanlab:üåü Run `swanlab watch -l /home/aaron/Downloads/Locomotion_Baseline-main/legged_gym/legged_gym/scripts/swanlog` to view SwanLab Experiment Dashboard locally
swanlab:üè† View project at https://swanlab.cn/@Aaron/wow
swanlab:üöÄ View run at https://swanlab.cn/@Aaron/wow/runs/vsy9figgx3gqx0zq2s4ow
swanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading completeswanlab: / Waiting for uploading completeswanlab: - Waiting for uploading completeswanlab: \ Waiting for uploading completeswanlab: | Waiting for uploading complete                                                                                                    swanlab: \ Updating experiment status...                                                                                                    